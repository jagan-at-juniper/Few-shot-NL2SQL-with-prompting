{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Flattening\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.types import StringType, IntegerType, ArrayType, FloatType, MapType, LongType\n",
    "from pyspark.sql.functions import lit, udf, size, avg, min as min_, max as max_, sum as sum_, count, countDistinct, col, mean, stddev, struct, explode, explode_outer, unix_timestamp, sum as sum_\n",
    "from operator import itemgetter\n",
    "import json\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import collect_list, collect_set, row_number, dense_rank, lead, lag, rank\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import collections\n",
    "from collections import deque\n",
    "import redis\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/mistsys/ds_incubator/venv/lib/python3.7/site-packages (1.17.2)\r\n",
      "Requirement already satisfied: pandas in /opt/mistsys/ds_incubator/venv/lib/python3.7/site-packages (0.25.1)\r\n",
      "Requirement already satisfied: sklearn in /opt/mistsys/ds_incubator/venv/lib/python3.7/site-packages (0.0)\r\n",
      "Requirement already satisfied: redis in /opt/mistsys/ds_incubator/venv/lib/python3.7/site-packages (3.3.11)\r\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/mistsys/ds_incubator/venv/lib/python3.7/site-packages (from pandas) (2019.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /opt/mistsys/ds_incubator/venv/lib/python3.7/site-packages (from pandas) (2.8.0)\r\n",
      "Requirement already satisfied: scikit-learn in /opt/mistsys/ds_incubator/venv/lib/python3.7/site-packages (from sklearn) (0.21.3)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/mistsys/ds_incubator/venv/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/mistsys/ds_incubator/venv/lib/python3.7/site-packages (from scikit-learn->sklearn) (0.14.0)\r\n",
      "Requirement already satisfied: scipy>=0.17.0 in /opt/mistsys/ds_incubator/venv/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.3.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas sklearn redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndf = spark.read.parquet('s3://mist-secorapp-staging/oc-stats/oc-stats-staging/dt=2019-09-01/hr=01/*')\\nrdd = df.rdd\\nr = rdd.map(lambda x: [(x['id'], x['when'], t['name'], t['rx_bytes'],t['tx_bytes'],t['rx_bps'],t['tx_bps'],t['rx_packets'],t['tx_packets']) for t in x['device']['interfaces']])\\nr1 = r.flatMap(lambda x: [t for t in x])\\ndf1 = sqlContext.createDataFrame(r1)\\ndf1 = df1.withColumnRenamed('_1','id')\\ntsudf = udf(lambda x: x[0]*1000000000+x[1], LongType())\\ndf1 = df1.withColumn('ts', tsudf(struct(df1['_2']['seconds'],df1['_2']['nanos'])))\\ndf1 = df1.withColumnRenamed('_3','name')         .withColumnRenamed('_4','rx_bytes')         .withColumnRenamed('_5','tx_bytes')         .withColumnRenamed('_6','rx_bps')         .withColumnRenamed('_7','tx_bps')         .withColumnRenamed('_8','rx_pkts')         .withColumnRenamed('_9','tx_pkts')         .drop('_2')\\n\\nid_name_gp = df1.groupby('id','name').agg(sum_('rx_bytes'),sum_('tx_bytes'),sum_('rx_bps'),sum_('tx_bps'),sum_('rx_pkts'),sum_('tx_pkts'))\\nid_gp = df1.groupby('id').agg(sum_('rx_bytes'),sum_('tx_bytes'),sum_('rx_bps'),sum_('tx_bps'),sum_('rx_pkts'),sum_('tx_pkts'))\\n\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "df = spark.read.parquet('s3://mist-secorapp-staging/oc-stats/oc-stats-staging/dt=2019-09-01/hr=01/*')\n",
    "rdd = df.rdd\n",
    "r = rdd.map(lambda x: [(x['id'], x['when'], t['name'], t['rx_bytes'],t['tx_bytes'],t['rx_bps'],t['tx_bps'],t['rx_packets'],t['tx_packets']) for t in x['device']['interfaces']])\n",
    "r1 = r.flatMap(lambda x: [t for t in x])\n",
    "df1 = sqlContext.createDataFrame(r1)\n",
    "df1 = df1.withColumnRenamed('_1','id')\n",
    "tsudf = udf(lambda x: x[0]*1000000000+x[1], LongType())\n",
    "df1 = df1.withColumn('ts', tsudf(struct(df1['_2']['seconds'],df1['_2']['nanos'])))\n",
    "df1 = df1.withColumnRenamed('_3','name') \\\n",
    "        .withColumnRenamed('_4','rx_bytes') \\\n",
    "        .withColumnRenamed('_5','tx_bytes') \\\n",
    "        .withColumnRenamed('_6','rx_bps') \\\n",
    "        .withColumnRenamed('_7','tx_bps') \\\n",
    "        .withColumnRenamed('_8','rx_pkts') \\\n",
    "        .withColumnRenamed('_9','tx_pkts') \\\n",
    "        .drop('_2')\n",
    "\n",
    "id_name_gp = df1.groupby('id','name').agg(sum_('rx_bytes'),sum_('tx_bytes'),sum_('rx_bps'),sum_('tx_bps'),sum_('rx_pkts'),sum_('tx_pkts'))\n",
    "id_gp = df1.groupby('id').agg(sum_('rx_bytes'),sum_('tx_bytes'),sum_('rx_bps'),sum_('tx_bps'),sum_('rx_pkts'),sum_('tx_pkts'))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o173.parquet.\n: java.nio.file.AccessDeniedException: s3://mist-data-science-dev/jing/switch/interfaces2019-09-18/_temporary/0: innerMkdirs on s3://mist-data-science-dev/jing/switch/interfaces2019-09-18/_temporary/0: com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: AC3819AC759D2C1B), S3 Extended Request ID: aTpmWWZFqEldgOzeSUGlthJlZwW3LIhLeHqMvEee0HsNBx5OOhS445ZDhZLayj5sSzialklXPBo=\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:158)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:101)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.mkdirs(S3AFileSystem.java:1484)\n\tat org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1914)\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:343)\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:162)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:139)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: AC3819AC759D2C1B), S3 Extended Request ID: aTpmWWZFqEldgOzeSUGlthJlZwW3LIhLeHqMvEee0HsNBx5OOhS445ZDhZLayj5sSzialklXPBo=\n\tat com.amazonaws.http.AmazonHttpClient.handleErrorResponse(AmazonHttpClient.java:1182)\n\tat com.amazonaws.http.AmazonHttpClient.executeOneRequest(AmazonHttpClient.java:770)\n\tat com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:489)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:310)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3785)\n\tat com.amazonaws.services.s3.AmazonS3Client.putObject(AmazonS3Client.java:1472)\n\tat com.amazonaws.services.s3.transfer.internal.UploadCallable.uploadInOneChunk(UploadCallable.java:131)\n\tat com.amazonaws.services.s3.transfer.internal.UploadCallable.call(UploadCallable.java:123)\n\tat com.amazonaws.services.s3.transfer.internal.UploadMonitor.call(UploadMonitor.java:139)\n\tat com.amazonaws.services.s3.transfer.internal.UploadMonitor.call(UploadMonitor.java:47)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-b136e963cdb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0mdfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m \u001b[0mdfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoalesce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m's3://mist-data-science-dev/jing/switch/interfaces'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0mdfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpersist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/mistsys/ds_incubator/venv/spark-2.4.4-bin-without-hadoop/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    841\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/mistsys/ds_incubator/venv/spark-2.4.4-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/mistsys/ds_incubator/venv/spark-2.4.4-bin-without-hadoop/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/mistsys/ds_incubator/venv/spark-2.4.4-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o173.parquet.\n: java.nio.file.AccessDeniedException: s3://mist-data-science-dev/jing/switch/interfaces2019-09-18/_temporary/0: innerMkdirs on s3://mist-data-science-dev/jing/switch/interfaces2019-09-18/_temporary/0: com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: AC3819AC759D2C1B), S3 Extended Request ID: aTpmWWZFqEldgOzeSUGlthJlZwW3LIhLeHqMvEee0HsNBx5OOhS445ZDhZLayj5sSzialklXPBo=\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:158)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:101)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.mkdirs(S3AFileSystem.java:1484)\n\tat org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1914)\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:343)\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:162)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:139)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.amazonaws.services.s3.model.AmazonS3Exception: Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: AC3819AC759D2C1B), S3 Extended Request ID: aTpmWWZFqEldgOzeSUGlthJlZwW3LIhLeHqMvEee0HsNBx5OOhS445ZDhZLayj5sSzialklXPBo=\n\tat com.amazonaws.http.AmazonHttpClient.handleErrorResponse(AmazonHttpClient.java:1182)\n\tat com.amazonaws.http.AmazonHttpClient.executeOneRequest(AmazonHttpClient.java:770)\n\tat com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:489)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:310)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3785)\n\tat com.amazonaws.services.s3.AmazonS3Client.putObject(AmazonS3Client.java:1472)\n\tat com.amazonaws.services.s3.transfer.internal.UploadCallable.uploadInOneChunk(UploadCallable.java:131)\n\tat com.amazonaws.services.s3.transfer.internal.UploadCallable.call(UploadCallable.java:123)\n\tat com.amazonaws.services.s3.transfer.internal.UploadMonitor.call(UploadMonitor.java:139)\n\tat com.amazonaws.services.s3.transfer.internal.UploadMonitor.call(UploadMonitor.java:47)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "#flatten interface\n",
    "date = '2019-09-18'\n",
    "df = spark.read.parquet('s3://mist-secorapp-staging/oc-stats/oc-stats-staging/dt='+date+'/*')\n",
    "\n",
    "import uuid\n",
    "def bbfunc(x):\n",
    "    if len(x) == 0: return ''\n",
    "    else: return str(uuid.UUID(bytes=bytes(x)))\n",
    "bb_udf = udf(bbfunc, StringType())\n",
    "\n",
    "def macfunc(bb):\n",
    "    x = ''\n",
    "    for i in range(0,6):\n",
    "        x += '%0*X' % (2, bytes(bb)[i]) + '-'\n",
    "    return x[:-1]\n",
    "mac_udf = udf(macfunc, StringType())\n",
    "\n",
    "df = df.withColumn('id', mac_udf('id')) \\\n",
    "        .withColumn('mac', mac_udf('mac')) \\\n",
    "        .withColumn('org_id', bb_udf('org_id')) \\\n",
    "        .withColumn('site_id', bb_udf('site_id'))\n",
    "                    \n",
    "ts_udf = udf(lambda x: x[0]*1000000+int(x[1]/1000), LongType())\n",
    "df = df.withColumn('ts', ts_udf(struct('when.seconds', 'when.nanos'))).drop('when')\n",
    "\n",
    "dfin = df.select('id',\n",
    "                 'mac',\n",
    "                 'org_id',\n",
    "                 'site_id',\n",
    "                 'ts',\n",
    "                 'remote_addr',\n",
    "                 col('device.hostname').alias('hostname'),\n",
    "                 col('device.model').alias('model'),\n",
    "                 col('device.firmware_version').alias('version'),\n",
    "                 col('device.serial_number').alias('serial'),\n",
    "                 col('device.uptime').alias('uptime'),\n",
    "                 col('device.poe_controller.index').alias('pctrl_ix'),\n",
    "                 col('device.poe_controller.max_power').alias('pctrl_maxpwr'),\n",
    "                 col('device.poe_controller.consumption').alias('pctrl_consump'),\n",
    "                 col('device.poe_controller.guardband').alias('pctrl_gband'),\n",
    "                 explode_outer('device.interfaces').alias('interfaces'))\n",
    "\n",
    "dfin = dfin.select('id',\n",
    "                 'mac',\n",
    "                 'org_id',\n",
    "                 'site_id',\n",
    "                 'ts',\n",
    "                 'remote_addr',\n",
    "                  'hostname',\n",
    "                  'model',\n",
    "                  'version',\n",
    "                  'serial',\n",
    "                  'uptime',\n",
    "                   'pctrl_ix',\n",
    "                   'pctrl_maxpwr',\n",
    "                   'pctrl_consump',\n",
    "                   'pctrl_gband',\n",
    "                  col('interfaces.name').alias('name'),\n",
    "                   col('interfaces.link').alias('link'),\n",
    "                   col('interfaces.full_duplex').alias('full_duplex'),\n",
    "                   col('interfaces.mbps').alias('mbps'),\n",
    "                   col('interfaces.mtu').alias('mtu'),\n",
    "                   col('interfaces.address').alias('address'),\n",
    "                   col('interfaces.admin_status').alias('admin_status'),\n",
    "                   col('interfaces.last_flapped.seconds').alias('last_flapped'),\n",
    "                   col('interfaces.errors').alias('errors'),\n",
    "                   col('interfaces.poe.enabled').alias('poe_enabled'),\n",
    "                   col('interfaces.poe.status').alias('poe_status'),\n",
    "                   col('interfaces.poe.power_limit').alias('poe_pwrlimit'),\n",
    "                   col('interfaces.poe.power').alias('poe_power'),\n",
    "                   col('interfaces.poe.priority').alias('poe_priority'),\n",
    "                   col('interfaces.poe.class').alias('poe_class'),\n",
    "                   col('interfaces.poe.mode').alias('poe_mode'),\n",
    "                   col('interfaces.rx_bytes').alias('rx_bytes'),\n",
    "                   col('interfaces.tx_bytes').alias('tx_bytes'),\n",
    "                   col('interfaces.rx_packets').alias('rx_pkts'),\n",
    "                   col('interfaces.tx_packets').alias('tx_pkts'),\n",
    "                   col('interfaces.rx_bps').alias('rx_bps'),\n",
    "                   col('interfaces.tx_bps').alias('tx_bps'))\n",
    "\n",
    "dfin.persist().first()\n",
    "dfin.coalesce(100).write.parquet('s3://mist-data-science-dev/jing/switch/interfaces'+date)\n",
    "dfin.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten chassis: flatten all 4 arrays\n",
    "dfch = df.select('id',\n",
    "                 'mac',\n",
    "                 'org_id',\n",
    "                 'site_id',\n",
    "                 'ts',\n",
    "                 'remote_addr',\n",
    "                 col('device.hostname').alias('hostname'),\n",
    "                 col('device.model').alias('model'),\n",
    "                 col('device.firmware_version').alias('version'),\n",
    "                 col('device.serial_number').alias('serial'),\n",
    "                 col('device.uptime').alias('uptime'),\n",
    "                 col('device.poe_controller.index').alias('pctrl_ix'),\n",
    "                 col('device.poe_controller.max_power').alias('pctrl_maxpwr'),\n",
    "                 col('device.poe_controller.consumption').alias('pctrl_consump'),\n",
    "                 col('device.poe_controller.guardband').alias('pctrl_gband'),\n",
    "                 col('device.chassis.routing_engines').alias('routing_engines1'),\n",
    "                 col('device.chassis.fans').alias('fans1'),\n",
    "                 col('device.chassis.temperatures').alias('temperatures1'),\n",
    "                 col('device.chassis.psus').alias('psus1')\n",
    "                )\n",
    "\n",
    "dfch = dfch.select(col('*'),explode_outer('routing_engines1').alias('routing_engines')) \\\n",
    "            .select(col('*'),explode_outer('temperatures1').alias('temperatures')) \\\n",
    "            .select(col('*'),explode_outer('fans1').alias('fans')) \\\n",
    "            .select(col('*'),explode_outer('psus1').alias('psus')) \\\n",
    "            .drop('routing_engines1','temperatures1','fans1','psus1')\n",
    "\n",
    "dfch = dfch.select(col('*'),\n",
    "                  col('routing_engines.slot').alias('eng_slot'),\n",
    "                  col('routing_engines.is_master').alias('eng_ismaster'),\n",
    "                  col('routing_engines.status').alias('eng_status'),\n",
    "                  col('routing_engines.temp_celsius').alias('eng_tempc'),\n",
    "                  col('routing_engines.memory.total_mb').alias('eng_memoryttlmb'),\n",
    "                  col('routing_engines.memory.in_use_mb').alias('eng_memoryinusemb'),\n",
    "                  col('routing_engines.cpu.user').alias('eng_cpuuser'),\n",
    "                  col('routing_engines.cpu.background').alias('eng_cpubackground'),\n",
    "                  col('routing_engines.cpu.system').alias('eng_cpusystem'),\n",
    "                  col('routing_engines.cpu.interrupt').alias('eng_cpuinterrupt'),\n",
    "                  col('routing_engines.cpu.idle').alias('eng_cpuidle'),\n",
    "                  col('routing_engines.cputemp_celsius').alias('eng_cputempc'),\n",
    "                  col('routing_engines.model').alias('eng_model'),\n",
    "                  col('routing_engines.last_reboot_reason').alias('eng_lastrebootreason'),\n",
    "                  col('routing_engines.load_avg').alias('eng_loadavg'),\n",
    "                  col('fans.name').alias('fans_name'),\n",
    "                  col('fans.status').alias('fans_status'),\n",
    "                   col('temperatures.name').alias('temp_name'),\n",
    "                   col('temperatures.status').alias('temp_status'),\n",
    "                   col('temperatures.temp_celsius').alias('temp_tempc'),\n",
    "                    col('psus.name').alias('psus_name'),\n",
    "                   col('psus.status').alias('psus_status')) \\\n",
    "                    .drop('routing_engines','fans','temperatures','psus')\n",
    "\n",
    "dfch.persist().first()\n",
    "dfch.coalesce(100).write.parquet('s3://mist-data-science-dev/jing/switch/chassis'+date)\n",
    "dfch.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten config_history\n",
    "dfconf = df.select('id',\n",
    "                 'mac',\n",
    "                 'org_id',\n",
    "                 'site_id',\n",
    "                 'ts',\n",
    "                 'remote_addr',\n",
    "                 col('device.hostname').alias('hostname'),\n",
    "                 col('device.model').alias('model'),\n",
    "                 col('device.firmware_version').alias('version'),\n",
    "                 col('device.serial_number').alias('serial'),\n",
    "                 col('device.uptime').alias('uptime'),\n",
    "                 col('device.poe_controller.index').alias('pctrl_ix'),\n",
    "                 col('device.poe_controller.max_power').alias('pctrl_maxpwr'),\n",
    "                 col('device.poe_controller.consumption').alias('pctrl_consump'),\n",
    "                 col('device.poe_controller.guardband').alias('pctrl_gband'),\n",
    "                 col('device.config_history').alias('config_history1')\n",
    "                )\n",
    "\n",
    "dfconf = dfconf.select(col('*'),explode_outer('config_history1').alias('config_history')) \\\n",
    "            .drop('config_history1')\n",
    "\n",
    "dfconf = dfconf.select(col('*'),\n",
    "                  col('config_history.sequence_num').alias('confhist_seqnum'),\n",
    "                  col('config_history.user').alias('confhist_user'),\n",
    "                   col('config_history.client').alias('confhist_client'),\n",
    "                   col('config_history.diff').alias('confhist_diff'),\n",
    "                   col('config_history.timestamp').alias('confhist_timestamp')) \\\n",
    "                    .drop('config_history')\n",
    "\n",
    "dfconf.persist().first()\n",
    "dfconf.coalesce(100).write.parquet('s3://mist-data-science-dev/jing/switch/config_history'+date)\n",
    "dfconf.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten alarms\n",
    "dfalarm = df.select('id',\n",
    "                 'mac',\n",
    "                 'org_id',\n",
    "                 'site_id',\n",
    "                 'ts',\n",
    "                 'remote_addr',\n",
    "                 col('device.hostname').alias('hostname'),\n",
    "                 col('device.model').alias('model'),\n",
    "                 col('device.firmware_version').alias('version'),\n",
    "                 col('device.serial_number').alias('serial'),\n",
    "                 col('device.uptime').alias('uptime'),\n",
    "                 col('device.poe_controller.index').alias('pctrl_ix'),\n",
    "                 col('device.poe_controller.max_power').alias('pctrl_maxpwr'),\n",
    "                 col('device.poe_controller.consumption').alias('pctrl_consump'),\n",
    "                 col('device.poe_controller.guardband').alias('pctrl_gband'),\n",
    "                 col('device.alarms').alias('alarms1')\n",
    "                )\n",
    "\n",
    "dfalarm = dfalarm.select(col('*'),explode_outer('alarms1').alias('alarms')) \\\n",
    "            .drop('alarms1')\n",
    "\n",
    "dfalarm = dfalarm.select(col('*'),\n",
    "                  col('alarms.timestamp').alias('alarm_ts'),\n",
    "                  col('alarms.type').alias('alarm_type'),\n",
    "                   col('alarms.alarm_class').alias('alarm_class'),\n",
    "                   col('alarms.description').alias('alarm_desc'),\n",
    "                   col('alarms.short_description').alias('alarm_shortdesc')) \\\n",
    "                    .drop('alarms')\n",
    "\n",
    "dfalarm.persist().first()\n",
    "dfalarm.coalesce(100).write.parquet('s3://mist-data-science-dev/jing/switch/alarms'+date)\n",
    "dfalarm.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten ipv4_route\n",
    "dfipv = df.select('id',\n",
    "                 'mac',\n",
    "                 'org_id',\n",
    "                 'site_id',\n",
    "                 'ts',\n",
    "                 'remote_addr',\n",
    "                 col('device.hostname').alias('hostname'),\n",
    "                 col('device.model').alias('model'),\n",
    "                 col('device.firmware_version').alias('version'),\n",
    "                 col('device.serial_number').alias('serial'),\n",
    "                 col('device.uptime').alias('uptime'),\n",
    "                 col('device.poe_controller.index').alias('pctrl_ix'),\n",
    "                 col('device.poe_controller.max_power').alias('pctrl_maxpwr'),\n",
    "                 col('device.poe_controller.consumption').alias('pctrl_consump'),\n",
    "                 col('device.poe_controller.guardband').alias('pctrl_gband'),\n",
    "                 col('device.ipv4_route').alias('ipv1')\n",
    "                )\n",
    "\n",
    "dfipv = dfipv.select(col('*'),explode_outer('ipv1').alias('ipv')) \\\n",
    "            .drop('ipv1')\n",
    "\n",
    "dfipv = dfipv.select(col('*'),\n",
    "                  col('ipv.dst.mask').alias('ipv_dstmask'),\n",
    "                  col('ipv.dst.ip').alias('ipv_dstip'),\n",
    "                   col('ipv.gw').alias('ipv_gw')) \\\n",
    "                    .drop('ipv')\n",
    "\n",
    "dfipv.persist().first()\n",
    "dfipv.coalesce(100).write.parquet('s3://mist-data-science-dev/jing/switch/ipv'+date)\n",
    "dfipv.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten lldpneighbors\n",
    "dflldp = df.select('id',\n",
    "                 'mac',\n",
    "                 'org_id',\n",
    "                 'site_id',\n",
    "                 'ts',\n",
    "                 'remote_addr',\n",
    "                 col('device.hostname').alias('hostname'),\n",
    "                 col('device.model').alias('model'),\n",
    "                 col('device.firmware_version').alias('version'),\n",
    "                 col('device.serial_number').alias('serial'),\n",
    "                 col('device.uptime').alias('uptime'),\n",
    "                 col('device.poe_controller.index').alias('pctrl_ix'),\n",
    "                 col('device.poe_controller.max_power').alias('pctrl_maxpwr'),\n",
    "                 col('device.poe_controller.consumption').alias('pctrl_consump'),\n",
    "                 col('device.poe_controller.guardband').alias('pctrl_gband'),\n",
    "                 col('device.lldpneighbors').alias('lldp1')\n",
    "                )\n",
    "\n",
    "dflldp = dflldp.select(col('*'),explode_outer('lldp1').alias('lldp')) \\\n",
    "            .drop('lldp1')\n",
    "\n",
    "dflldp = dflldp.select(col('*'),\n",
    "                  col('lldp.local_port_id').alias('lldp_locportid'),\n",
    "                  col('lldp.local_parent_iface_name').alias('lldp_locifacename'),\n",
    "                   col('lldp.remote_chassis_id').alias('lldp_rmtchassisid'),\n",
    "                   col('lldp.remote_chassis_idsubtype').alias('lldp_rmtchassisidsub'),\n",
    "                      col('lldp.remote_port_desc').alias('lldp_rmtportdesc'),\n",
    "                      col('lldp.remote_system_name').alias('lldp_rmtsysname'),\n",
    "                      col('lldp.remote_mgmt_addr').alias('lldp_rmtmgmtaddr'),\n",
    "                      col('lldp.remote_system_desc').alias('lldp_rmtsysdesc')) \\\n",
    "                        .drop('lldp')\n",
    "\n",
    "dflldp.persist().first()\n",
    "dflldp.coalesce(100).write.parquet('s3://mist-data-science-dev/jing/switch/lldp'+date)\n",
    "dflldp.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten svistats\n",
    "dfsvi = df.select('id',\n",
    "                 'mac',\n",
    "                 'org_id',\n",
    "                 'site_id',\n",
    "                 'ts',\n",
    "                 'remote_addr',\n",
    "                 col('device.hostname').alias('hostname'),\n",
    "                 col('device.model').alias('model'),\n",
    "                 col('device.firmware_version').alias('version'),\n",
    "                 col('device.serial_number').alias('serial'),\n",
    "                 col('device.uptime').alias('uptime'),\n",
    "                 col('device.poe_controller.index').alias('pctrl_ix'),\n",
    "                 col('device.poe_controller.max_power').alias('pctrl_maxpwr'),\n",
    "                 col('device.poe_controller.consumption').alias('pctrl_consump'),\n",
    "                 col('device.poe_controller.guardband').alias('pctrl_gband'),\n",
    "                 col('device.svistats').alias('svi1')\n",
    "                )\n",
    "\n",
    "dfsvi = dfsvi.select(col('*'),explode_outer('svi1').alias('svi')) \\\n",
    "            .drop('svi1')\n",
    "\n",
    "dfsvi = dfsvi.select(col('*'),\n",
    "                  col('svi.dev').alias('svi_dev'),\n",
    "                  col('svi.vlan').alias('svi_vlan'),\n",
    "                   col('svi.ips').alias('svi_ips')) \\\n",
    "                    .drop('svi')\n",
    "\n",
    "dfsvi.persist().first()\n",
    "dfsvi.coalesce(100).write.parquet('s3://mist-data-science-dev/jing/switch/svi'+date)\n",
    "dfsvi.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data missing check\n",
    "dfch.groupby('id').agg(countDistinct('ts')).show() \n",
    "w0 = Window.partitionBy('id').orderBy('ts')                                                               \n",
    "df0 = df.select('id','ts').distinct()                                                                     \n",
    "df0 = df0.withColumn('ts1', lag('ts').over(w0))                                                           \n",
    "df0 = df0.na.fill(0)                                                                                      \n",
    "df0 = df0.withColumn('delta', df0.ts - df0.ts1)                                                            \n",
    "df0.filter(df0.delta>70000000).sort('delta').show(100)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "{'name': 'ge-0/0/0',\n",
    " 'link': False,\n",
    " 'full_duplex': False,\n",
    " 'mbps': 1000,\n",
    " 'mtu': 1514,\n",
    " 'address': bytearray(b'<\\x8c\\x93\\x94\\x95\\xaf'),\n",
    " 'admin_status': True,\n",
    " 'last_flapped': Row(seconds=260, nanos=0),\n",
    " 'errors': [],\n",
    " 'poe': Row(enabled=True, status=False, power_limit=15.399999618530273, power=0.0, priority=False, class='not-applicable', mode=''),\n",
    " 'rx_bytes': 0,\n",
    " 'rx_bps': 0,\n",
    " 'rx_packets': 0,\n",
    " 'tx_bytes': 0,\n",
    " 'tx_bps': 0,\n",
    " 'tx_packets': 0}\n",
    "\n",
    "\"\"\"\n",
    "def macfunc(bb):\n",
    "    x = ''\n",
    "    for i in range(0,6):\n",
    "        x += '%0*X' % (2, bytes(bb)[i]) + '-'\n",
    "    return x[:-1]\n",
    "\n",
    "r = rdd.map(lambda x: [(macfunc(x['id']), t['name'], x['when'], t['full_duplex'],t['last_flapped'])  for t in x['device']['interfaces'] if t['link']])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('s3://mist-secorapp-staging/oc-stats/oc-stats-staging/dt=2019-09-23/*')\n",
    "rdd = df.rdd\n",
    "\n",
    "r = rdd.map(lambda x: [(x['id'], x['when'], t['name'], t['rx_bytes'],t['tx_bytes'],t['rx_bps'],t['tx_bps'],t['rx_packets'],t['tx_packets'],t['mbps'], t['mtu'], t['errors'], t['poe']) for t in x['device']['interfaces']])\n",
    "r1 = r.flatMap(lambda x: [t for t in x])\n",
    "#excluding errors for now\n",
    "r2 = r1.map(lambda x: (x[0],x[1],x[2],x[3],x[4],x[5],x[6],x[7],x[8],x[9],x[10],x[12]['enabled'],x[12]['status'],x[12]['power_limit'],x[12]['power'],x[12]['priority'],x[12]['class'],x[12]['mode']))\n",
    "            \n",
    "df1 = sqlContext.createDataFrame(r2)\n",
    "tsudf = udf(lambda x: x[0]*1000000000+x[1], LongType())\n",
    "df1 = df1.withColumn('ts', tsudf(struct(df1['_2']['seconds'],df1['_2']['nanos']))).drop('_2')\n",
    "df1 = df1.withColumnRenamed('_1','id') \\\n",
    "        .withColumnRenamed('_3','name') \\\n",
    "        .withColumnRenamed('_4','rx_bytes') \\\n",
    "        .withColumnRenamed('_5','tx_bytes') \\\n",
    "        .withColumnRenamed('_6','rx_bps') \\\n",
    "        .withColumnRenamed('_7','tx_bps') \\\n",
    "        .withColumnRenamed('_8','rx_pkts') \\\n",
    "        .withColumnRenamed('_9','tx_pkts') \\\n",
    "        .withColumnRenamed('_10','mbps') \\\n",
    "        .withColumnRenamed('_11','mtu') \\\n",
    "        .withColumnRenamed('_12','poe_enabled') \\\n",
    "        .withColumnRenamed('_13','poe_status') \\\n",
    "        .withColumnRenamed('_14','poe_power_limit') \\\n",
    "        .withColumnRenamed('_15','poe_power') \\\n",
    "        .withColumnRenamed('_16','poe_priority') \\\n",
    "        .withColumnRenamed('_17','poe_class') \\\n",
    "        .withColumnRenamed('_18','poe_mode')\n",
    "#r1 = rdd.map(lambda x: [(x['id'], x['when'], t['poe_controller']['max_power'], t['']) for t in x['device']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1. each interface has a configued mbps (id, name), not each id (800-1000), should be max bps\n",
    "   446 unique interface (57/446 changed mbps in a day but in very small amount 10e-14 though)\n",
    "   same for mtu (1500 or so)\n",
    "2. pkts, bytes are incremental\n",
    "3. bytearray(b'<\\x8c\\x93\\x94\\x95\\xac') == [3C 8C 93 94 95 AC] ??? id\n",
    "4. dont know mbps for id\n",
    "5. mbps is mega bps configured for id (but under interfaces... should change to id level?)\n",
    "6. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = Window.partitionBy('id','name').orderBy('ts')\n",
    "df1 = df1.withColumn('prev_rxbytes', lag('rx_bytes').over(w)) \\\n",
    "        .withColumn('prev_txbytes', lag('tx_bytes').over(w)) \\\n",
    "        .withColumn('prev_rxpkts', lag('rx_pkts').over(w)) \\\n",
    "        .withColumn('prev_txpkts', lag('tx_pkts').over(w)) \n",
    "df1 = df1.filter(df1.prev_rxpkts.isNotNull())\n",
    "\n",
    "df1 = df1.withColumn('rxpkts', df1.rx_pkts - df1.prev_rxpkts) \\\n",
    "            .withColumn('txpkts', df1.tx_pkts - df1.prev_txpkts) \\\n",
    "            .withColumn('rxbytes', df1.rx_bytes - df1.prev_rxbytes) \\\n",
    "            .withColumn('txbytes', df1.tx_bytes - df1.prev_txbytes) \\\n",
    "            .drop('prev_rxpkts','prev_txpkts','prev_rxbytes','prev_txbytes','rx_pkts','tx_pkts','rx_bytes','tx_bytes')\n",
    "\n",
    "df1.write.parquet('s3://mist-data-science-dev/jing/switch_flat_20190910a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = [-float(\"inf\"), 0.0, 10,100,1000,10000,float(\"inf\")]\n",
    "bucketizer = Bucketizer(splits=splits, inputCol=\"rxpkts\", outputCol=\"bucket\")\n",
    "rxpkts = bucketizer.transform(df1)\n",
    "rxpkts.sort('bucket').groupby('bucket').count().show()\n",
    "+------+------+                                                                 \n",
    "|bucket| count|\n",
    "+------+------+\n",
    "|   1.0|545160|\n",
    "|   2.0| 13594|\n",
    "|   3.0|   913|\n",
    "|   4.0| 37937|\n",
    "|   5.0|  1878|\n",
    "+------+------+\n",
    "splits = [-float(\"inf\"), 0.0, 10,100,1000,10000,float(\"inf\")]\n",
    "bucketizer = Bucketizer(splits=splits, inputCol=\"txpkts\", outputCol=\"bucket\")\n",
    "rxpkts = bucketizer.transform(df1)\n",
    "rxpkts.sort('bucket').groupby('bucket').count().show()\n",
    "+------+------+                                                                 \n",
    "|bucket| count|\n",
    "+------+------+\n",
    "|   1.0|537890|\n",
    "|   2.0|  2938|\n",
    "|   3.0|  7942|\n",
    "|   4.0| 48980|\n",
    "|   5.0|  1732|\n",
    "+------+------+\n",
    "splits = [-float(\"inf\"), 0.0, 10,100,1000,10000,float(\"inf\")]\n",
    "bucketizer = Bucketizer(splits=splits, inputCol=\"rxbytes\", outputCol=\"bucket\")\n",
    "rxpkts = bucketizer.transform(df1)\n",
    "rxpkts.sort('bucket').groupby('bucket').count().show()\n",
    "+------+------+                                                                 \n",
    "|bucket| count|\n",
    "+------+------+\n",
    "|   1.0|542265|\n",
    "|   3.0|  1445|\n",
    "|   4.0|  6785|\n",
    "|   5.0| 48987|\n",
    "+------+------+\n",
    "splits = [-float(\"inf\"), 0.0, 10,100,1000,10000,float(\"inf\")]\n",
    "bucketizer = Bucketizer(splits=splits, inputCol=\"txbytes\", outputCol=\"bucket\")\n",
    "rxpkts = bucketizer.transform(df1)\n",
    "rxpkts.sort('bucket').groupby('bucket').count().show()\n",
    "+------+------+                                                                 \n",
    "|bucket| count|\n",
    "+------+------+\n",
    "|   1.0|533451|\n",
    "|   4.0|  6592|\n",
    "|   5.0| 59439|\n",
    "+------+------+\n",
    "splits = [-float(\"inf\"), 0.0, 10,100,1000,10000,100000,1000000,10000000,100000000,float(\"inf\")]\n",
    "bucketizer = Bucketizer(splits=splits, inputCol=\"rx_bps\", outputCol=\"bucket\")\n",
    "rxpkts = bucketizer.transform(df1)\n",
    "rxpkts.sort('bucket').groupby('bucket').count().show()\n",
    "+------+------+ \n",
    "|bucket| count|\n",
    "+------+------+\n",
    "|   1.0|572735|0-10\n",
    "|   2.0|   810|\n",
    "|   3.0|  5103|\n",
    "|   4.0|  8512|\n",
    "|   5.0|  8599|\n",
    "|   6.0|  3225|\n",
    "|   7.0|   447|\n",
    "|   8.0|    51|10-100M\n",
    "+------+------+\n",
    "splits = [-float(\"inf\"), 0.0, 10,100,1000,10000,100000,1000000,10000000,100000000,float(\"inf\")]\n",
    "bucketizer = Bucketizer(splits=splits, inputCol=\"tx_bps\", outputCol=\"bucket\")\n",
    "rxpkts = bucketizer.transform(df1)\n",
    "rxpkts.sort('bucket').groupby('bucket').count().show()\n",
    "+------+------+                                                                 \n",
    "|bucket| count|\n",
    "+------+------+\n",
    "|   1.0|570109|0-10\n",
    "|   2.0|    85|\n",
    "|   3.0|  3709|\n",
    "|   4.0|  4836|\n",
    "|   5.0| 15945|\n",
    "|   6.0|  4313|\n",
    "|   7.0|   435|\n",
    "|   8.0|    50|10-100M\n",
    "+------+------+\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.groupby(col('id'),col('ts')).agg(sum_('rx_bps').alias('sum_rxbps'),sum_('tx_bps').alias('sum_txbp'),avg('mbps')).show()\n",
    "\n",
    "max of sum_rxbps: 88M\n",
    "max of sum_txbps: 91M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dfin.groupby('id','when').agg(sum_('rx_bps'),sum_('tx_bps')) \n",
    "splits = [-float(\"inf\"), 0.0, 10,100,1000,10000,100000,1000000,10000000,100000000,float(\"inf\")]\n",
    "bucketizer = Bucketizer(splits=splits, inputCol=\"sum(rx_bps)\", outputCol=\"bucket\")\n",
    "rxpkts = bucketizer.transform(d)\n",
    "rxpkts.sort('bucket').groupby('bucket').count().show()\n",
    "\n",
    "+------+-----+                                                                  \n",
    "|bucket|count|\n",
    "+------+-----+\n",
    "|   1.0| 4303| \n",
    "|   4.0|  278|\n",
    "|   5.0| 5629|\n",
    "|   6.0| 2382|\n",
    "|   7.0|  472|\n",
    "|   8.0|   60| 10-100M\n",
    "+------+-----+\n",
    "\n",
    "splits = [-float(\"inf\"), 0.0, 10,100,1000,10000,100000,1000000,10000000,100000000,float(\"inf\")]\n",
    "bucketizer = Bucketizer(splits=splits, inputCol=\"sum(tx_bps)\", outputCol=\"bucket\")\n",
    "rxpkts = bucketizer.transform(d)\n",
    "rxpkts.sort('bucket').groupby('bucket').count().show()\n",
    "\n",
    "+------+-----+                                                                  \n",
    "|bucket|count|\n",
    "+------+-----+\n",
    "|   1.0| 4303|\n",
    "|   4.0|  145|\n",
    "|   5.0| 4555|\n",
    "|   6.0| 3528|\n",
    "|   7.0|  535|\n",
    "|   8.0|   58| 10-100M\n",
    "+------+-----+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
