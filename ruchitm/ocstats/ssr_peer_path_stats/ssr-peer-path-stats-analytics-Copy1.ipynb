{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T17:19:32.223828Z",
     "start_time": "2021-10-06T17:19:26.974823Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-06_2021-10-05"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "from functools import partial\n",
    "import math\n",
    "\n",
    "\n",
    "def round_datetime(timestamp, min_interval=0, sec_interval=0):\n",
    "    if timestamp : \n",
    "        timestamp = int(timestamp)\n",
    "    else :\n",
    "        timestamp=0\n",
    "    ## scaling timestamp to seconds resolution\n",
    "    scale=1\n",
    "    resolution = int(math.log10(timestamp))\n",
    "    if resolution==12:\n",
    "        scale=1E3\n",
    "    elif resolution==15:\n",
    "        scale=1E6\n",
    "    elif resolution==18:\n",
    "        scale=1E9\n",
    "    timestamp = timestamp//scale\n",
    "    tm = datetime.fromtimestamp(timestamp)\n",
    "    if min_interval > 0 and sec_interval > 0:\n",
    "        tm = tm - timedelta(minutes=tm.minute % min_interval, seconds=tm.second % sec_interval)\n",
    "    elif min_interval > 0 :\n",
    "        tm = tm - timedelta(minutes=tm.minute % min_interval, seconds=tm.second)\n",
    "    elif sec_interval > 0 :\n",
    "        tm = tm - timedelta(minutes=tm.minute, seconds=tm.second % sec_interval)\n",
    "    return tm\n",
    "\n",
    "def date_list(endDate, delta=14):\n",
    "    temp=[endDate]\n",
    "    for i in range(1,delta+1):\n",
    "        temp.append(endDate - timedelta(days=i))\n",
    "    return '{' + ','.join([str(d.date()) for d in temp]) + '}'\n",
    "\n",
    "@F.udf(IntegerType())\n",
    "def ts_resolution(timestamp):\n",
    "    return int(math.log10(timestamp))+1\n",
    "\n",
    "\n",
    "udf_rounddate = F.udf(round_datetime, TimestampType())\n",
    "udf_round_1hour_floor = F.udf(partial(round_datetime, min_interval=60), TimestampType())\n",
    "udf_round_10min_floor = F.udf(partial(round_datetime, min_interval=10), TimestampType())\n",
    "udf_round_1min_floor = F.udf(partial(round_datetime, min_interval=1), TimestampType())\n",
    "udf_round_30sec_floor = F.udf(partial(round_datetime, min_interval=1, sec_interval=30), TimestampType())\n",
    "\n",
    "@F.udf(StringType())\n",
    "def is_privateIP(ip):\n",
    "    try :\n",
    "        ip_obj = ipaddress.ip_address(ip)\n",
    "        return \"private_IP\" if ip_obj.is_private else \"public_IP\"\n",
    "    except ValueError:\n",
    "        return \"other\"\n",
    "\n",
    "def magic_median(varname):\n",
    "    return F.expr(f'percentile_approx({varname}, 0.5)')\n",
    "\n",
    "def magic_iqr(varname):\n",
    "    return F.expr(f'percentile_approx({varname}, 0.75)-percentile_approx({varname}, 0.25)')\n",
    "\n",
    "def getDateString(dates):\n",
    "    if ',' not in dates:\n",
    "        return dates.split('{')[1].split('}')[0]\n",
    "    else:\n",
    "        return dates.split('{')[1].split(',')[0] + \"_\" + dates.split('{')[1].split(',')[-1].split('}')[0]\n",
    "\n",
    "def get_agg_funcs(data_cols):\n",
    "    avg_funcs = [F.avg(col).alias(f'avg_{col}') for col in data_cols]\n",
    "    std_funcs = [F.stddev(col).alias(f'std_{col}') for col in data_cols]\n",
    "    median_funcs = [magic_median(col).alias(f'median_{col}') for col in data_cols]\n",
    "    iqr_funcs = [magic_iqr(col).alias(f'iqr_{col}') for col in data_cols]\n",
    "    return avg_funcs + std_funcs + median_funcs + iqr_funcs\n",
    "############################################################\n",
    "\n",
    "endDate = datetime.today()\n",
    "interval = 1\n",
    "dates=date_list(endDate, delta=interval)\n",
    "date_string = getDateString(dates)\n",
    "print(date_string)\n",
    "hour = '{*}'\n",
    "storage = 's3'\n",
    "env = 'production'\n",
    "#env = 'staging'\n",
    "local = False\n",
    "path = 'oc-stats-analytics'\n",
    "user = 'test' ## change to personal username\n",
    "cols = [\"site_id\",\"mac\",\"t128.router_name\",\"interfaces\",\"org_id\",\"model\",\"ssr_peer_path_stats\"]\n",
    "\n",
    "df = spark.read.parquet(f\"{storage}://mist-secorapp-{env}/{path}{'-local' if local else ''}/{path}-{env}/dt={dates}/hr={hour}\")\n",
    "\n",
    "df2 = df.select(cols).filter((F.col('model')=='128T-Router')|(F.col('model')=='SSR'))\n",
    "\n",
    "##### #####\n",
    "time_agg_col = 'When_30sec'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process peer-path data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-06T17:19:38.055540Z",
     "start_time": "2021-10-06T17:19:36.821089Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bfd_group_cols = [time_agg_col,'site_id','peer_path_named','mac','device_interface']\n",
    "fpm_group_cols = [time_agg_col,'site_id','peer_path_named','mac','device_interface','fpm_traffic_class','fpm_protocol']\n",
    "bfd_cols = [f'bfd_{metric}' for metric in ['latency','jitter','loss']]                  \n",
    "fpm_cols = [f'fpm_{metric}' for metric in ['latency','jitter','loss']]                  \n",
    "\n",
    "\n",
    "df_peer_path = df2.select('site_id','mac','router_name','org_id',\n",
    "                          F.explode('ssr_peer_path_stats').alias('ssr_peer_path_stats'))\n",
    "\n",
    "df_peer_path = df_peer_path.select('site_id','mac','router_name','org_id','ssr_peer_path_stats.*')\n",
    "\n",
    "df_peer_path = df_peer_path.select('site_id','mac','router_name','org_id',\n",
    "                                    'peer_name','adjacent_address', 'device_interface',\n",
    "                                    'network_interface','vlan_id','uptime',\n",
    "                                    'peer_site_id', 'peer_mac', 'peer_network',\n",
    "                                    F.col('t128explicit_interface_type').alias('explicit_interface_type'),\n",
    "                                     F.size('samples').alias('n_samples'),\n",
    "                                     F.explode(F.col('samples')))\n",
    "\n",
    "\n",
    "df_peer_path = df_peer_path.select('site_id','mac','router_name','org_id',\n",
    "                                    'peer_name','adjacent_address', 'device_interface',\n",
    "                                    'network_interface','vlan_id','uptime',\n",
    "                                   'peer_site_id', 'peer_mac', 'peer_network','explicit_interface_type',\n",
    "                                    F.col('col.*'),'n_samples') \\\n",
    "                           .withColumn('When', udf_rounddate(F.col('timestamp'))) \\\n",
    "                           .withColumn('When_60sec', udf_round_1min_floor(F.col('timestamp'))) \\\n",
    "                           .withColumn('When_30sec', udf_round_30sec_floor(F.col('timestamp'))) \\\n",
    "                           .withColumnRenamed('latency','bfd_latency') \\\n",
    "                           .withColumnRenamed('jitter','bfd_jitter')\\\n",
    "                           .withColumnRenamed('loss','bfd_loss')\n",
    "\n",
    "df_peer_path = df_peer_path.filter(F.col('is_active')==True)\n",
    "df_peer_path = df_peer_path.filter(F.col('is_up')==True)\n",
    "\n",
    "df_peer_path = df_peer_path.withColumn('peer_path', F.concat_ws(\"__\", 'mac', 'peer_name',\n",
    "                                                                'network_interface', 'adjacent_address'))\n",
    "df_peer_path = df_peer_path.withColumn('peer_path_named', F.concat_ws(\"__\", 'router_name', 'peer_name',\n",
    "                                                                'network_interface', 'adjacent_address'))\n",
    "\n",
    "\n",
    "df_bfd_agg = df_peer_path.groupby(bfd_group_cols).agg(*get_agg_funcs(bfd_cols))\n",
    "\n",
    "other_cols = [col for col in df_peer_path.columns if col!='fpmstats']\n",
    "df_peer_path = df_peer_path.select(*other_cols, F.explode('fpmstats').alias('fpmstats'))\n",
    "df_peer_path = df_peer_path.select(*other_cols, 'fpmstats.*') \\\n",
    "                .withColumnRenamed('latency','fpm_latency') \\\n",
    "                .withColumnRenamed('jitter','fpm_jitter')\\\n",
    "                .withColumnRenamed('loss','fpm_loss')\\\n",
    "                .withColumnRenamed('traffic_class','fpm_traffic_class')\\\n",
    "                .withColumnRenamed('protocol','fpm_protocol')\n",
    "\n",
    "df_fpm_agg = df_peer_path.groupby(fpm_group_cols).agg(*get_agg_funcs(fpm_cols))\n",
    "\n",
    "\n",
    "# df_site = spark.read.parquet(\"s3://mist-secorapp-staging/dimension/site\") \\\n",
    "#                 .withColumnRenamed('id','site_id') \\\n",
    "#                 .withColumnRenamed('name','site_name') \\\n",
    "#                 .select('site_id','address','country_code','lat','lng')\n",
    "\n",
    "# df_site_peer = df_site\n",
    "# for col in df_site.columns:\n",
    "#     df_site_peer = df_site_peer.withColumnRenamed(col, 'peer_'+col)\n",
    "# #df_site.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T19:00:08.274785Z",
     "start_time": "2021-10-05T19:00:08.257333Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#df_bfd_agg.select('router_name','peer_path').distinct().show()\n",
    "#df_fpm_agg.select('router_name','peer_path').distinct().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-05T19:01:59.778660Z",
     "start_time": "2021-10-05T19:00:08.276117Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bfd_outpath = f'{storage}://mist-data-science-dev/{user}/ssr_peer_path/data/bfd_stats_dt={date_string}/'\n",
    "fpm_outpath = f'{storage}://mist-data-science-dev/{user}/ssr_peer_path/data/fpm_stats_dt={date_string}/'\n",
    "\n",
    "df_bfd_agg.write.csv(bfd_outpath, mode=\"overwrite\", header=True)\n",
    "df_fpm_agg.write.csv(fpm_outpath, mode=\"overwrite\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
