{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\" Client edges \"\"\"\n",
    "root\n",
    " |-- siteId: string (nullable = true)\n",
    " |-- orgId: string (nullable = true)\n",
    " |-- wcid: string (nullable = true)\n",
    " |-- clientWlanId: string (nullable = true)\n",
    " |-- ipAddress: string (nullable = true)\n",
    " |-- deviceMac: string (nullable = true)\n",
    " |-- deviceIp: string (nullable = true)\n",
    " |-- band: string (nullable = true)\n",
    " |-- id: string (nullable = true)\n",
    " |-- from: string (nullable = true)\n",
    " |-- to: string (nullable = true)\n",
    " |-- createdAt: long (nullable = true)\n",
    " |-- DFTemporalDedupeTransformer_max_temporal_interval: integer (nullable = true)\n",
    " |-- lastSeenAt: long (nullable = true)\n",
    " |-- expiredAt: integer (nullable = true)\n",
    " |-- lastModifiedAt: integer (nullable = true)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from datetime import datetime, timedelta\n",
    "import timeit\n",
    "import boto3\n",
    "import json\n",
    "import copy\n",
    "import pickle\n",
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Row, Window\n",
    "from pyspark.sql.types import *\n",
    "from graphframes import *\n",
    "\n",
    "\"\"\"\n",
    "Utility functions \n",
    "\"\"\"\n",
    "def date_list(endDate, delta=14):\n",
    "    temp = [endDate]\n",
    "    for i in range(1, delta + 1):\n",
    "        temp.append(endDate - timedelta(days=i))\n",
    "    return '{' + ','.join([str(d.date()) for d in temp]) + '}'\n",
    "\n",
    "\n",
    "def readParquet(prefix, dates=\"{*}\", hour=\"{*}\", fields=None):\n",
    "    path = prefix + \"/dt=\" + dates + \"/hr=\" + hour + \"/\"\n",
    "    return spark.read.parquet(path)\n",
    "\n",
    "def readCSV(prefix, dates=\"{*}\", hour=\"{*}\", fields=None):\n",
    "    path = prefix + \"/dt=\" + dates + \"/hr=\" + hour + \"/\"\n",
    "    return spark.read.csv(path)\n",
    "\n",
    "def readSeq(prefix, date, hour=\"*\", fields=None, toPandas=False):\n",
    "    path = prefix + \"/dt=\" + date + \"/hr=\" + hour + \"/\"\n",
    "    temp = sc.sequenceFile(path).values() \\\n",
    "        .map(bytearray.decode).map(json.loads)\n",
    "    if isinstance(fields, list):\n",
    "        temp = temp.flatMap(lambda x: Row([x[field] for field in fields]))\n",
    "    temp = spark.createDataFrame(temp)\n",
    "    if isinstance(fields, list):\n",
    "        for idx, field in enumerate(fields):\n",
    "            temp = temp.withColumnRenamed(\"_\" + str(idx + 1), field)\n",
    "    _schema = copy.deepcopy(temp.schema)\n",
    "    if toPandas:\n",
    "        return temp.toPandas(), _schema\n",
    "    return temp, _schema\n",
    "\n",
    "\n",
    "def roundDatetime(timestamp, interval=0):\n",
    "    tm = datetime.fromtimestamp(timestamp)\n",
    "    if interval > 0:\n",
    "        tm = tm - timedelta(minutes=tm.minute % interval, seconds=tm.second)\n",
    "    return tm\n",
    "\n",
    "\n",
    "@F.pandas_udf(\"int\", F.PandasUDFType.GROUPED_AGG)\n",
    "def median_udf(v):\n",
    "    return v.median()\n",
    "\n",
    "\n",
    "@F.pandas_udf(\"int\", F.PandasUDFType.GROUPED_AGG)\n",
    "def iqr_udf(v):\n",
    "    iqr = v.quantile(0.75) - v.quantile(0.25)\n",
    "    return iqr\n",
    "\n",
    "def gaussian_smooth(df, groups, window=5, std=2):\n",
    "    return df.set_index('time_of_day').groupby(groups)[['upper','lower']] \\\n",
    "                      .rolling(window, win_type='gaussian', min_periods=1, std=std) \\\n",
    "                      .mean().reset_index()\n",
    "\n",
    "def df_to_s3(df, path, filename):\n",
    "    s3 = boto3.resource('s3')\n",
    "    file_type = filename.split(\".\")[-1]\n",
    "    with io.StringIO() as outputBuffer:\n",
    "        if file_type == \"pickle\":\n",
    "            pickle.dump(df, outputBuffer)\n",
    "        elif file_type == \"json\":\n",
    "            df.to_json(outputBuffer, orient='index')\n",
    "        elif file_type == \"csv\":\n",
    "            df.to_csv(outputBuffer)\n",
    "            #json.dump(df, buffer)\n",
    "        print(outputBuffer.closed)\n",
    "        outputBuffer.seek(0)\n",
    "        obj = s3.Object('mist-data-science-dev', f'{path}/{filename}')\n",
    "        obj.put(Body=outputBuffer.getvalue())\n",
    "    print(outputBuffer.closed)\n",
    "\n",
    "\"\"\"\n",
    "Spark UDFs\n",
    "\"\"\"\n",
    "curried_roundDatetime = partial(roundDatetime, interval=0)\n",
    "udf_roundDate = F.udf(curried_roundDatetime, TimestampType())\n",
    "\n",
    "udf_scaleToSeconds = F.udf(lambda tm : int(float(tm)/1E6), LongType())\n",
    "udf_numElem = F.udf(lambda x : len(x), IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pure python function for cycle detection\n",
    "* Stops after first detected cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_site_graph(df_site, site_id=None):\n",
    "    if site_id:\n",
    "        df_site = df_site.filter(F.col('siteId')==site_id)\n",
    "\n",
    "    df_site_adj_list = df_site.select('siteId','src','dst')\\\n",
    "                            .groupby('siteId','src')\\\n",
    "                            .agg(F.collect_set(F.col('dst')).alias('dst'))\n",
    "    df_site_adj_list = df_site_adj_list.groupby(\"siteId\")\\\n",
    "             .agg(F.map_from_arrays(F.collect_list(\"src\"),F.collect_list(\"dst\")).alias(\"graph\"))\n",
    "    \n",
    "    return df_site_adj_list\n",
    "\n",
    "def _isCycle(graph, v, done, stack): \n",
    "\n",
    "    done[v] = True\n",
    "    stack[v] = True\n",
    "\n",
    "    for neighbour in graph[v] : \n",
    "        if neighbour in graph :\n",
    "            if done[neighbour] == False : \n",
    "                if _isCycle(graph, neighbour, done, stack) == True: \n",
    "                    print(f'Part of cycle: {neighbour}')\n",
    "                    return True\n",
    "            elif stack[neighbour] == True : \n",
    "                print(f'Last neighbour found on stack: {neighbour}')\n",
    "                return True\n",
    "\n",
    "    stack[v] = False\n",
    "    return False\n",
    "\n",
    "\n",
    "def isCycle(graph): \n",
    "    src_vertices = graph.keys()\n",
    "    done = {k:False for k in src_vertices} \n",
    "    stack = {k:False for k in src_vertices} \n",
    "\n",
    "    for node in src_vertices: \n",
    "        if done[node] == False: \n",
    "            if _isCycle(graph, node, done, stack) == True: \n",
    "                return True\n",
    "    return False\n",
    "\n",
    "udf_isCycle = F.udf(isCycle, BooleanType())\n",
    "\n",
    "def iscycle_unittest():\n",
    "    graph1 = {'a':['b','c','d'], 'b':['c','e'], 'd':['e','f','g'], 'e':['a','c']}\n",
    "    graph2 = {'a':['b','c','d'], 'b':['c'], 'd':['e','f','g'], 'e':['a','c']}\n",
    "    graph3 = {'a':['b','c','e'], 'b':['c'], 'd':['e','f','g'], 'e':['a','c']}\n",
    "    graph4 = {'a':['b','c','f'], 'b':['c'], 'd':['e','f','g'], 'e':['a','c']}\n",
    "\n",
    "\n",
    "    df_test = pd.DataFrame([['test1',graph1],\n",
    "                            ['test2',graph2],\n",
    "                            ['test3',graph3],\n",
    "                            ['test4',graph4]],\n",
    "                           columns=['siteId','graph'])\n",
    "    \n",
    "    df_test = spark.createDataFrame(df_test)\n",
    "    df_test_cycle = df_test.withColumn('cycle_detected', udf_isCycle(F.col('graph')))\n",
    "    \n",
    "    print('##### Check python ####')\n",
    "    print('Graph1')\n",
    "    print(f'Is cycle={isCycle(graph1)}')\n",
    "    print('Graph2')\n",
    "    print(f'Is cycle={isCycle(graph2)}')\n",
    "    print('Graph3')\n",
    "    print(f'Is cycle={isCycle(graph3)}')\n",
    "    print('Graph4')\n",
    "    print(f'Is cycle={isCycle(graph4)}')\n",
    "    print('\\n')\n",
    "\n",
    "    print('##### Check pyspark ####')\n",
    "\n",
    "    return df_test_cycle.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+--------------------------------+--------------------------------+\n",
      "|siteId                              |src                             |dst                             |\n",
      "+------------------------------------+--------------------------------+--------------------------------+\n",
      "|0e62b411-43d6-4228-9d45-f9c17f611013|71baced3326fe53538c6dd39e95cff0f|bd275b21c9bca779998bad68c1ce6f62|\n",
      "|22316bd0-0a39-4366-b02d-2389d0f6c6be|35a7ba2c5eb362598ba3e5f92e2ec009|f4b16a655ec2849203034256fa12be22|\n",
      "|fa8c8345-fe0c-4aa5-9e16-34ed779ec315|c68b58028e8b5861589ca833529f4083|88b5c561652666376d47dbfba042fa6f|\n",
      "|39ce773f-ca15-4018-a8fc-f6d9a82d82f6|7e437d5c9783c56796b6e86e0737cba0|287791e27e1aa35588e087a5d23c9d51|\n",
      "|a6c4ed56-a300-4eb1-8dc3-c6ba0481d9ff|88558945e7e46a0403df537e1240e439|76798ecfadf760ed2f24f9e036c5cedc|\n",
      "|de09a449-34b5-4f2d-8c56-e9bb5f62f9e1|adbe77d9ecb200b846133350b797b3de|1acfcca028293d7351f3df5785235dbd|\n",
      "|675bca94-3b45-4b43-bf09-437129d9763d|42e44488f58098030ff44ffc329d2526|2f81f63a7a6263a7abd15af70293bf34|\n",
      "|935140fc-b5b0-47b7-88a3-d5994903195e|90384e7cf435ef7dcadb3e09b7ea0dea|f5586d4fd1b6006c9c372c1e63753341|\n",
      "|2cae8a24-34db-4a95-932a-14f5dbbe7a0f|8d1936f37c3a3fa3b7f5350369f210b2|78bc8c3adcf443ce58451e602c851a41|\n",
      "|9b02df4f-8bb2-44e0-8a00-eb7f6918985a|89258939f26b057e25a7b768ab23e4ad|973b53bb0a6e2e9d8b0e01284213ca5f|\n",
      "|943f7061-72f8-4aea-a9b0-27ae94d793b0|d2290539cace1037296bd07c9f8ca25c|fb054b738de4d8cec214e1ed4742be43|\n",
      "|9c103626-af9d-41f4-ac8c-254cb61863f6|cc4845675abcce7edf481c38b6446b96|b5a3a47796f7c4ee9f25b2fbc6d02ccc|\n",
      "|48140061-d927-42c7-807a-8dc4827da13c|ab99862c90afc1d5514d377ce4f5de71|34b0ac3a12313a8e9c6587262f4343a3|\n",
      "|2a64668b-1deb-4d2e-9a71-08cc88ab46f2|24b18d39926ac5b959f8b5a7ce406aa3|4d7ff7e248667f2f9054d0ff096a1d99|\n",
      "|69618725-0f5d-4552-847e-ad9168196cc7|3c6aad59c435af39d560cec5d04b213b|a0f0efcdbefab6a4bd5e48d68248228f|\n",
      "|1ed4ba15-1628-4750-ab17-4ad9cb17d89a|9d8a5bb76f79bb0685b14239d23789e9|2ca513d7843ec605e70ad09cbfbe2a1f|\n",
      "|a5092570-7713-4dd6-aace-2d5fb92e3193|7fe5cb0084b3cb84d6eb435d775b9501|e8b26d043a96512c2dd17158ef2e5044|\n",
      "|7c82d0d4-7f6c-4a10-8a1d-9f183a90f983|c8a1079f141faad54128a553d5643c7b|dd24593a10b966a13d355bfaa223d873|\n",
      "|c265ebaa-5e6f-4a04-a47c-8fe802a9e1ff|08e8b66eed0e4edba938cc86ea3ffe84|31c35d54aa0ba26702659d7bf26ec98d|\n",
      "|07ce9346-ca0b-4294-81f7-97672fe8ca12|15cf6de4b8e6b2aaf76a052b4537710d|3a26002b32bf127f0108303a6066d64d|\n",
      "+------------------------------------+--------------------------------+--------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Variables\n",
    "\"\"\"\n",
    "ENV = 'production'\n",
    "component = 'device'\n",
    "prefix_edges = f\"s3://mist-aggregated-stats-{ENV}/aggregated-stats/graph/snapshots/{component}-edges/\"\n",
    "prefix_nodes = f\"s3://mist-aggregated-stats-{ENV}/aggregated-stats/graph/snapshots/{component}-nodes/\"\n",
    "\n",
    "END_DATE = datetime.today()\n",
    "#END_DATE = datetime.strptime('2021-03-03', '%Y-%m-%d')\n",
    "LAG = 0\n",
    "\n",
    "hour = '11'\n",
    "\n",
    "\"\"\"\"\"\"\n",
    "\n",
    "dates = date_list(END_DATE, delta=LAG)\n",
    "df_edges = readParquet(prefix_edges, dates, hour=hour)\n",
    "df_edges = df_edges.withColumn('isExpired', F.when(F.isnull(F.col('expiredAt')),'False').otherwise('True'))\n",
    "df_active_edges = df_edges.filter(F.col('isExpired')==False)\n",
    "df_active_edges = df_active_edges.withColumnRenamed(\"from\",\"src\").withColumnRenamed(\"to\",\"dst\")\n",
    "\n",
    "df_nodes = readParquet(prefix_nodes, dates)\n",
    "df_nodes = df_nodes.withColumn('isExpired', F.when(F.isnull(F.col('expiredAt')),'False').otherwise('True'))\n",
    "df_active_nodes = df_nodes.filter(F.col('isExpired')==False)\n",
    "\n",
    "df_active_edges.select('siteId','src','dst').show(5, truncate=False)\n",
    "#df_active_nodes.select('siteId','id').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+\n",
      "|siteId                              |\n",
      "+------------------------------------+\n",
      "|0049c3ef-3406-4dae-88f8-37783fb4b702|\n",
      "|1ef1d02d-c049-4f13-8d44-71b3654f75fd|\n",
      "|4599cb58-e197-4bc1-b8da-c90481b95305|\n",
      "|742bf504-6760-4007-8035-a47a934836a1|\n",
      "|8ac6670a-0e85-4939-84f9-a37d779a45a3|\n",
      "|ad8b6eef-de92-4a84-a262-8230e40fd893|\n",
      "|ec62f79d-d08a-44bf-884b-0b58ca101051|\n",
      "|f25d8fd8-a695-4421-8b84-036d2164c572|\n",
      "|0027af11-eb72-425f-a1b2-e8f3b4ce8d10|\n",
      "|0d2c68a0-96e9-4125-a53c-325f8471724e|\n",
      "|192fab31-1014-4f3d-aa7d-9454a37babe4|\n",
      "|341880de-2d4d-430f-9b5e-928577cd6c84|\n",
      "|6f643507-d233-4fb1-98bd-e4445446270e|\n",
      "|856606a9-176d-47d1-b3b6-d9750ab56b2e|\n",
      "|898b809b-2b46-443d-babb-c374ce638b94|\n",
      "|9291ba26-6e1e-11e5-9cdd-02e208b2d34f|\n",
      "|94650a18-f0a9-4852-a709-d12a2ce23eea|\n",
      "|aac40d89-9c00-4c7e-8d7c-ff11d4535f9c|\n",
      "|aba6715c-92f9-4828-9080-22dfbc2c2053|\n",
      "|b3610203-d3ca-4511-803c-dca6f17c470c|\n",
      "+------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Sites with cycles in device collection = 227\n"
     ]
    }
   ],
   "source": [
    "#site_id = '03e668d2-7c72-4451-ae99-2e32b2b97b71'\n",
    "site_id = None\n",
    "df_site_graph = get_site_graph(df_active_edges, site_id=site_id)\n",
    "df_site_graph_cycle = df_site_graph.withColumn('cycle_detected', udf_isCycle(F.col('graph')))\n",
    "\n",
    "df_site_graph_cycle_true = df_site_graph_cycle.filter(F.col('cycle_detected')==True)\n",
    "df_site_graph_cycle_true.select('siteId').show(5, truncate=False)\n",
    "cycle_detect = df_site_graph_cycle_true.count()\n",
    "print(f'Sites with cycles in {component} collection = {cycle_detect}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp = df_site_graph_cycle_true.select('siteId').toPandas()\n",
    "#df_to_s3(temp, f\"spark_jobs/ruchit-dev/site_graphdb_cycle_true_{dates}_hr_{hour}\",\n",
    "#         f\"sites.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print simple cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cycles:\n",
    "    def __init__(self, graph):\n",
    "        self._graph = graph\n",
    "        self.nodes = self.getNodes()\n",
    "        self.nodeMap = {node : i for i, node in enumerate(self.nodes)}\n",
    "        self.visited = ['NOT_VISITED' for _ in range(len(self.nodes))]  # Initialize all nodes unvisited\n",
    "        self.stack = []  # Stack to keep track of visited nodes\n",
    "        self.cycles = []\n",
    "\n",
    "    def getNodes(self):\n",
    "        _nodes=set([])\n",
    "        for k, v in self._graph.items():\n",
    "            items = [k] + v\n",
    "            for item in items:\n",
    "                _nodes.add(item)\n",
    "        return list(_nodes)\n",
    "\n",
    "    def printCycles(self):\n",
    "        return self.cycles\n",
    "    \n",
    "    def addCycle(self, v):\n",
    "        self.stack2 = []\n",
    "        self.stack2.append(self.stack[-1])\n",
    "        self.stack.pop()\n",
    "        cycle = []\n",
    "        #print(f'stack2={self.stack2}')\n",
    "        while self.stack2[-1] != v:\n",
    "            self.stack2.append(self.stack[-1])\n",
    "            self.stack.pop()\n",
    "        while len(self.stack2) > 0: \n",
    "            cycle.append(self.nodes[self.stack2[-1]])\n",
    "            self.stack.append(self.stack2[-1])\n",
    "            self.stack2.pop()\n",
    "            \n",
    "        if len(cycle)>1 : self.cycles.append(cycle)\n",
    "        \n",
    "    def dfs(self):\n",
    "        curr = self.stack[-1]\n",
    "        if self.nodes[curr] in self._graph:\n",
    "            for neighbour in self._graph[self.nodes[curr]]:\n",
    "                to = self.nodeMap[neighbour]\n",
    "                if self.visited[to] == 'ON_STACK':\n",
    "                    self.addCycle(to)\n",
    "                elif self.visited[to] == 'NOT_VISITED':\n",
    "                    self.stack.append(to)\n",
    "                    self.visited[to] = 'ON_STACK'\n",
    "                    self.dfs()\n",
    "                \n",
    "        self.visited[curr] = 'DONE'\n",
    "        self.stack.pop()\n",
    "\n",
    "    def findCycles(self):\n",
    "        for i, node in enumerate(self.nodes):\n",
    "            if self.visited[i] == 'NOT_VISITED':\n",
    "                self.stack = []\n",
    "                self.stack.append(i)\n",
    "                self.visited[i] = 'ON_STACK'\n",
    "                self.dfs()\n",
    "        \n",
    "        return self.printCycles()\n",
    "    \n",
    "def find_cycles(graph):\n",
    "    cycles = Cycles(graph)\n",
    "    return cycles.findCycles()\n",
    "\n",
    "udf_findCycles = F.udf(find_cycles, ArrayType(ArrayType(StringType())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_site_graph_cycles = df_site_graph_cycle_true.withColumn('cycles', udf_findCycles(F.col('graph')))\n",
    "#df_site_graph_cycles.select('siteId', 'cycles').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "temp_pd = df_site_graph_cycles.select('siteId', 'cycles').toPandas()\n",
    "df_to_s3(temp_pd, f\"spark_jobs/ruchit-dev/site_graphdb_cycles_{dates}_hr_{hour}\",\n",
    "         f\"sites_cycles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Site=0049c3ef-3406-4dae-88f8-37783fb4b702\n",
      "-->['79f275fc2c4b5fa8db87ea8d2928dd98', '112866ce3150072ce0ee0fb8be35f2a4']\n",
      "Site=192fab31-1014-4f3d-aa7d-9454a37babe4\n",
      "-->['c7e7042eb95d0b8ad30a1ce874fe83a7', 'd3afedabbca6573bf09cb753693e8c3d']\n",
      "-->['c7e7042eb95d0b8ad30a1ce874fe83a7', '6d2342cf1e4b90b32975a12489887737']\n",
      "Site=cc235a99-d83f-43e1-a2e0-23c475e649ba\n",
      "-->['86b64fc3f4ffabbb43f9c694bfe903e3', '7ad9023d6d988c9fec7ef16a2f280cfc']\n",
      "Site=aebea3ce-8635-40fa-aaab-d23bf1aae603\n",
      "-->['2b33e8e4e44eaaf3dc5e83dd23277d0d', '703ba9b2db196bc2a5bcb92a53ce5f1b']\n",
      "Site=4d9330f1-8816-449a-b3cc-d4e4e0a3313b\n",
      "Site=b72c169b-fdae-4157-859c-e1f6d63c746c\n",
      "-->['1965b37077735462fcdfefed2c283fbd', 'f96232e15e28c5b833cfb9b1b99d10c3']\n",
      "-->['1965b37077735462fcdfefed2c283fbd', '0826d5dfea5e34f87e940654bbed1902']\n",
      "Site=baf57076-5c94-4025-85bb-934cf9ffbd4e\n",
      "-->['eb9c21ce46b70879f24217a6538ddf85', '416c4bcd80dfca87a948279ee6c59b05']\n",
      "Site=c8482263-acb6-496b-8d30-c21c6210e607\n",
      "-->['987761a549d1c78537242bd7906d05f9', 'a0983710432dfd19bc6b2533e66bd09e']\n",
      "Site=022d17f9-0236-469b-b58b-100ffe8451c4\n",
      "Site=5d730b5c-65d2-4061-8b45-c3020c36a087\n",
      "-->['e2fa26c7edbbbd27222c1635029bf2fc', '8665571343e9237a05712cf7cfa0ecb5']\n",
      "-->['c1db0cf188377f8451d1aaee9a5fd687', 'e2fa26c7edbbbd27222c1635029bf2fc', '8665571343e9237a05712cf7cfa0ecb5', '00fdb4d568e9cfbfb7d628739b8c1ef5']\n",
      "-->['8665571343e9237a05712cf7cfa0ecb5', '00fdb4d568e9cfbfb7d628739b8c1ef5']\n",
      "-->['e2fa26c7edbbbd27222c1635029bf2fc', '8665571343e9237a05712cf7cfa0ecb5', '00fdb4d568e9cfbfb7d628739b8c1ef5']\n",
      "Site=71290b51-e05a-404f-b689-08c4e3317e68\n",
      "-->['bc3d469282c6e3d3876bc4b6ff4ba184', '9b6db4b0b1c75a302f4c4587700f1145']\n",
      "-->['9b6db4b0b1c75a302f4c4587700f1145', '17259d9fa7c874c9ad6c2774e6151c26']\n",
      "-->['bc3d469282c6e3d3876bc4b6ff4ba184', '9b6db4b0b1c75a302f4c4587700f1145', '17259d9fa7c874c9ad6c2774e6151c26']\n",
      "Site=8d263206-7b20-4305-aa3d-1f83e50a62e1\n",
      "-->['fde0a4850d939292f4d95b3a7ea3624c', 'e22377e9b0d6b8c478739c3f406f53b0']\n",
      "-->['fae60c21d8f729a5731e442b66c359b3', 'fde0a4850d939292f4d95b3a7ea3624c']\n",
      "-->['fde0a4850d939292f4d95b3a7ea3624c', '661956b12fef326b2ede3a91c85604b5']\n",
      "Site=c24ea0ee-401e-4ad1-83ae-f141777e9403\n",
      "Site=af8d2d16-1bcc-4f65-b6f6-bb7e09622a32\n",
      "-->['d7d5daacaebd0010d4cf83eaf8d497de', '97e824d6ebdde7d225f0b2b8776c3a68']\n",
      "Site=1830368e-a1ae-4aee-a097-c08117f22f48\n",
      "-->['6dd88094f91ee1a9709492ea121e105d', '78e2b5c6f27a63dedb59d7c45e0e7b5a']\n",
      "-->['78e2b5c6f27a63dedb59d7c45e0e7b5a', '6d0b3b38f9d6f8f55065b8521bf2eb38']\n",
      "-->['6dd88094f91ee1a9709492ea121e105d', '78e2b5c6f27a63dedb59d7c45e0e7b5a', '6d0b3b38f9d6f8f55065b8521bf2eb38']\n",
      "-->['6b0a81f010a243e9a06e88cf068dfbd8', '6dd88094f91ee1a9709492ea121e105d']\n",
      "-->['6dd88094f91ee1a9709492ea121e105d', '980fe20e57a8b60cfe9c60298501490b']\n",
      "Site=9a675073-23f9-49c9-bb4d-78d750fef1f3\n",
      "-->['b66056bbdb561d8c6095d6450e3c1e9d', 'c02755e4ea39e16cb03ea9be370d1e0b']\n",
      "-->['6747721de67efa45ba76f85702166f79', 'b66056bbdb561d8c6095d6450e3c1e9d', 'c02755e4ea39e16cb03ea9be370d1e0b', '0dacbf708ab0b43761a65788495faea2']\n",
      "-->['6747721de67efa45ba76f85702166f79', 'b66056bbdb561d8c6095d6450e3c1e9d']\n",
      "Site=7b03dce4-1e08-40aa-b245-f961bc903a8c\n",
      "-->['0ac0f113bc341053403f44c238d33772', '60d710929eae386db0411fc4968111d4']\n",
      "-->['60d710929eae386db0411fc4968111d4', '2dae713227df6065b78281d5929102ad']\n",
      "Site=6896d532-2142-4943-a983-cd644b62c4bb\n",
      "Site=ad4928c1-1738-4639-8ee5-b9d7af5688e3\n",
      "-->['1997276f0c0ad6d45e5fc761ef9d259d', '7ad50e1da24c1edc2e36db0439879f00']\n",
      "-->['d8dd5588093fed19ee677378f15d6784', '1997276f0c0ad6d45e5fc761ef9d259d']\n",
      "Site=d9d87b96-cd25-4faf-be63-8c3a25245985\n",
      "-->['b716030119abb580490da0e21c57b384', '9c815595cc419dadde692c29ec071494']\n",
      "Site=0cc5e1f2-bd04-4b63-a39d-418d69c58d08\n",
      "-->['1a0f12473fb235c1d6c8d960cac85111', 'bcc20a20b7d53ee3dc700fa3d793e2bb']\n",
      "Site=9f27f9bc-5ac4-4c8f-8e47-cff5957b4a7f\n",
      "-->['7891d78546225ef64a9c09dd94d1765e', '6af29de546fe8dbeedb31df14eb5bddd', '578a828091756d355056e312d80f680f']\n",
      "Site=09df1500-f1b3-4501-a805-7af21a1fb90f\n",
      "-->['f31007741523708f8a84ea6211b45cf1', 'ddce40a8017ec20d4c588e0a4ad99c2c']\n"
     ]
    }
   ],
   "source": [
    "#site = '742bf504-6760-4007-8035-a47a934836a1'\n",
    "for site in temp_pd.siteId[::10]:\n",
    "    print(f'Site={site}')\n",
    "    for cycles in temp_pd[temp_pd.siteId==site].cycles:\n",
    "        for cycle in cycles:\n",
    "            print(f'-->{cycle}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strongly connected components\n",
    "* Find strong connected components (SCC) in a graph G\n",
    "* Each node in G belongs to one SCC (alone or with other nodes)\n",
    "* Each node in a SCC is reachable by every other node within a SCC\n",
    "* Intuitively, we think of a SCC as containing one or more simple cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCC:\n",
    "    def __init__(self, graph):\n",
    "        self._graph = graph\n",
    "        self.nodes = self.getNodes()\n",
    "        self.nodeMap = {i : node for i, node in enumerate(self.nodes)}\n",
    "        self.inverse_nodeMap = {node : i for i, node in enumerate(self.nodes)}\n",
    "        self.ids = [-1]*len(self.nodes)  # Initialize all nodes unvisited\n",
    "        self.low = [-1]*len(self.nodes)  # Initialize all nodes low-link is unvisited\n",
    "        self.onStack = [False]*len(self.nodes)  # Initialize all node onStack to False\n",
    "        self.stack = []  # Stack to keep track of visited nodes\n",
    "        self.nodeId = 0\n",
    "        self.scc_map = {}\n",
    "\n",
    "        \n",
    "    def printStack(self):\n",
    "        print([self.nodeMap[i] for i in self.stack])\n",
    "\n",
    "    def getNodes(self):\n",
    "        _nodes=set([])\n",
    "        for k, v in self._graph.items():\n",
    "            items = [k] + v\n",
    "            for item in items:\n",
    "                _nodes.add(item)\n",
    "        return _nodes\n",
    "    \n",
    "    def dfs(self, curr):\n",
    "        self.stack.append(curr)\n",
    "        self.onStack[curr] = True\n",
    "        #print(f\"{self.nodeMap[curr]}={curr}\")\n",
    "        self.ids[curr] = self.nodeId\n",
    "        self.low[curr] = self.nodeId\n",
    "        self.nodeId = self.nodeId + 1\n",
    "\n",
    "\n",
    "        # print(low)\n",
    "        if self.nodeMap[curr] in self._graph:\n",
    "            for neighbour in self._graph[self.nodeMap[curr]]:\n",
    "                to = self.inverse_nodeMap[neighbour]\n",
    "                if self.ids[to] == -1: # Unvisited\n",
    "                    self.dfs(to)  # Run dfs on neighbour if neighbour is unvisited\n",
    "                if to in self.stack: \n",
    "                    self.low[curr] = min(self.low[curr], self.low[to])  # If neighbour on stack then \n",
    "                                                         # Min the low-link value of current node with neighbour\n",
    "\n",
    "        # After visiting all adjacent nodes\n",
    "        # check if current node is root of SCC\n",
    "        # If yes, then pop of all nodes upto and including current node\n",
    "        if self.ids[curr] == self.low[curr]:\n",
    "            while True:\n",
    "                #self.printStack()\n",
    "                temp = self.stack.pop(-1)\n",
    "                self.onStack[temp] = False\n",
    "                self.low[temp] = self.ids[curr]\n",
    "                if temp == curr:\n",
    "                    break\n",
    "        #self.printStack()\n",
    "\n",
    "    def get_scc(self):\n",
    "        for node in self.nodeMap.keys():\n",
    "            if self.ids[node] == -1:\n",
    "                self.dfs(node)\n",
    "                \n",
    "        for k, v in enumerate(self.low):\n",
    "            self.scc_map.setdefault(v, []).append(self.nodeMap[k])\n",
    "\n",
    "        # Only save non-unique components\n",
    "        for k in list(self.scc_map.keys()):\n",
    "            if len(self.scc_map[k])<2:\n",
    "                # continue\n",
    "                _ = self.scc_map.pop(k)    \n",
    "\n",
    "        return self.scc_map\n",
    "    \n",
    "def find_scc(graph):\n",
    "    scc = SCC(graph)\n",
    "    return scc.get_scc()\n",
    "\n",
    "udf_findSCC = F.udf(find_scc, MapType(IntegerType(), ArrayType(StringType())))\n",
    "\n",
    "def count_dict(map_obj):\n",
    "    count = {}\n",
    "    for key in map_obj: \n",
    "        if isinstance(map_obj[key], list): \n",
    "            count[key] = len(map_obj[key])\n",
    "    return count\n",
    "\n",
    "udf_countDict = F.udf(count_dict, MapType(IntegerType(), IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_site_graph_scc = df_site_graph_cycle_true.withColumn('scc_map', udf_findSCC(F.col('graph')))\n",
    "\n",
    "df_site_graph_cycle_to_save = df_site_graph_scc.select('siteId',F.explode('graph').alias('src','dst'))\\\n",
    "                .select('siteId', 'src', F.explode('dst').alias('dst'))\n",
    "\n",
    "\n",
    "df_to_s3(df_site_graph_cycle_to_save.toPandas(), f\"spark_jobs/ruchit-dev/site_graphdb_cycle_true_{dates}_hr_{hour}\",\n",
    "         \"sites_with_edges.csv\")\n",
    "\n",
    "#df_site_graph_scc = df_site_graph_scc.withColumn('scc_count', udf_countDict(F.col('scc_map')))\n",
    "#temp.select('scc_map').show(truncate=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_to_save = df_site_graph_scc.select('siteId',F.explode('scc_map').alias('componentId','value'))\\\n",
    "    .select('siteId','componentId',F.explode('value').alias('nodeId'))\n",
    "\n",
    "temp_pd = temp_to_save.toPandas()\n",
    "df_to_s3(temp_pd, f\"spark_jobs/ruchit-dev/site_graphdb_scc_{dates}_hr_{hour}\",\n",
    "         \"sites_with_scc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = \"ad8b6eef-de92-4a84-a262-8230e40fd893\"\n",
    "df_active_edges.filter(F.col('siteId')==site).select('siteId','src','dst').show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
