{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"\"\" Client edges \"\"\"\n",
    "root\n",
    " |-- siteId: string (nullable = true)\n",
    " |-- orgId: string (nullable = true)\n",
    " |-- wcid: string (nullable = true)\n",
    " |-- clientWlanId: string (nullable = true)\n",
    " |-- ipAddress: string (nullable = true)\n",
    " |-- deviceMac: string (nullable = true)\n",
    " |-- deviceIp: string (nullable = true)\n",
    " |-- band: string (nullable = true)\n",
    " |-- id: string (nullable = true)\n",
    " |-- from: string (nullable = true)\n",
    " |-- to: string (nullable = true)\n",
    " |-- createdAt: long (nullable = true)\n",
    " |-- DFTemporalDedupeTransformer_max_temporal_interval: integer (nullable = true)\n",
    " |-- lastSeenAt: long (nullable = true)\n",
    " |-- expiredAt: integer (nullable = true)\n",
    " |-- lastModifiedAt: integer (nullable = true)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from datetime import datetime, timedelta\n",
    "import timeit\n",
    "import boto3\n",
    "import json\n",
    "import copy\n",
    "import pickle\n",
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Row, Window\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\"\"\"\n",
    "Utility function definitions\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def date_list(endDate, delta=14):\n",
    "    temp = [endDate]\n",
    "    for i in range(1, delta + 1):\n",
    "        temp.append(endDate - timedelta(days=i))\n",
    "    return '{' + ','.join([str(d.date()) for d in temp]) + '}'\n",
    "\n",
    "\n",
    "def readParquet(prefix, dates=\"{*}\", hour=\"{*}\", fields=None):\n",
    "    path = prefix + \"/dt=\" + dates + \"/hr=\" + hour + \"/\"\n",
    "    return spark.read.parquet(path)\n",
    "\n",
    "def readCSV(prefix, dates=\"{*}\", hour=\"{*}\", fields=None):\n",
    "    path = prefix + \"/dt=\" + dates + \"/hr=\" + hour + \"/\"\n",
    "    return spark.read.csv(path)\n",
    "\n",
    "def readSeq(prefix, date, hour=\"*\", fields=None, toPandas=False):\n",
    "    path = prefix + \"/dt=\" + date + \"/hr=\" + hour + \"/\"\n",
    "    temp = sc.sequenceFile(path).values() \\\n",
    "        .map(bytearray.decode).map(json.loads)\n",
    "    if isinstance(fields, list):\n",
    "        temp = temp.flatMap(lambda x: Row([x[field] for field in fields]))\n",
    "    temp = spark.createDataFrame(temp)\n",
    "    if isinstance(fields, list):\n",
    "        for idx, field in enumerate(fields):\n",
    "            temp = temp.withColumnRenamed(\"_\" + str(idx + 1), field)\n",
    "    _schema = copy.deepcopy(temp.schema)\n",
    "    if toPandas:\n",
    "        return temp.toPandas(), _schema\n",
    "    return temp, _schema\n",
    "\n",
    "\n",
    "def roundDatetime(timestamp, interval=0):\n",
    "    tm = datetime.fromtimestamp(timestamp)\n",
    "    if interval > 0:\n",
    "        tm = tm - timedelta(minutes=tm.minute % interval, seconds=tm.second)\n",
    "    return tm\n",
    "\n",
    "\n",
    "curried_roundDatetime = partial(roundDatetime, interval=0)\n",
    "udf_roundDate = F.udf(curried_roundDatetime, TimestampType())\n",
    "\n",
    "udf_scaleToSeconds = F.udf(lambda tm : int(float(tm)/1E6), LongType())\n",
    "udf_numElem = F.udf(lambda x : len(x), IntegerType())\n",
    "\n",
    "@F.pandas_udf(\"int\", F.PandasUDFType.GROUPED_AGG)\n",
    "def median_udf(v):\n",
    "    return v.median()\n",
    "\n",
    "\n",
    "@F.pandas_udf(\"int\", F.PandasUDFType.GROUPED_AGG)\n",
    "def iqr_udf(v):\n",
    "    iqr = v.quantile(0.75) - v.quantile(0.25)\n",
    "    return iqr\n",
    "\n",
    "def gaussian_smooth(df, groups, window=5, std=2):\n",
    "    return df.set_index('time_of_day').groupby(groups)[['upper','lower']] \\\n",
    "                      .rolling(window, win_type='gaussian', min_periods=1, std=std) \\\n",
    "                      .mean().reset_index()\n",
    "\n",
    "def df_to_s3(df, path, filename):\n",
    "    s3 = boto3.resource('s3')\n",
    "    file_type = filename.split(\".\")[-1]\n",
    "    with io.StringIO() as outputBuffer:\n",
    "        if file_type == \"pickle\":\n",
    "            pickle.dump(df, outputBuffer)\n",
    "        elif file_type == \"json\":\n",
    "            df.to_json(outputBuffer, orient='index')\n",
    "            #json.dump(df, buffer)\n",
    "        print(outputBuffer.closed)\n",
    "        outputBuffer.seek(0)\n",
    "        obj = s3.Object('mist-data-science-dev', f'{path}/{filename}')\n",
    "        obj.put(Body=outputBuffer.getvalue())\n",
    "    print(outputBuffer.closed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Variables\n",
    "\"\"\"\n",
    "ENV = 'production'\n",
    "prefix = f\"s3://mist-aggregated-stats-{ENV}/aggregated-stats/graph/snapshots/client-edges/\"\n",
    "\n",
    "#END_DATE = datetime.today()\n",
    "END_DATE = datetime.strptime('2021-02-12', '%Y-%m-%d')\n",
    "LAG = 0\n",
    "\"\"\"\"\"\"\n",
    "\n",
    "dates = date_list(END_DATE, delta=LAG)\n",
    "df_stats = readParquet(prefix, dates)\n",
    "df_stats = df_stats.withColumn('isExpired', F.when(F.isnull(F.col('expiredAt')),'False').otherwise('True'))\n",
    "df_active = df_stats.filter(F.col('isExpired')==False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_site_graph(df_site, site_id=None):\n",
    "    if site_id:\n",
    "        df_site = df_site.filter(F.col('siteId')==site_id)\n",
    "\n",
    "    df_site_adj_list = df_site.select('siteId','from','to')\\\n",
    "                            .groupby('siteId','from')\\\n",
    "                            .agg(F.collect_set(F.col('to')).alias('to'))\n",
    "    df_site_adj_list = df_site_adj_list.groupby(\"siteId\")\\\n",
    "             .agg(F.map_from_arrays(F.collect_list(\"from\"),F.collect_list(\"to\")).alias(\"graph\"))\n",
    "    \n",
    "    return df_site_adj_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _isCycle(graph, v, done, stack): \n",
    "\n",
    "    done[v] = True\n",
    "    stack[v] = True\n",
    "\n",
    "    for neighbour in graph[v] : \n",
    "        if neighbour in graph :\n",
    "            if done[neighbour] == False : \n",
    "                if _isCycle(graph, neighbour, done, stack) == True: \n",
    "                    print(f'Part of cycle: {neighbour}')\n",
    "                    return True\n",
    "            elif stack[neighbour] == True : \n",
    "                print(f'Last neighbour found on stack: {neighbour}')\n",
    "                return True\n",
    "\n",
    "    stack[v] = False\n",
    "    return False\n",
    "\n",
    "\n",
    "def isCycle(graph): \n",
    "    src_vertices = graph.keys()\n",
    "    done = {k:False for k in src_vertices} \n",
    "    stack = {k:False for k in src_vertices} \n",
    "\n",
    "    for node in src_vertices: \n",
    "        if done[node] == False: \n",
    "            if _isCycle(graph, node, done, stack) == True: \n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def cycle_unittest():\n",
    "    graph1 = {'a':['b','c','d'], 'b':['c','e'], 'd':['e','f','g'], 'e':['a','c']}\n",
    "    df_graph1 = spark.createDataFrame(graph1, schema=['graph'])\n",
    "    print(isCycle(graph1))\n",
    "    print('\\n')\n",
    "    graph2 = {'a':['b','c','d'], 'b':['c'], 'd':['e','f','g'], 'e':['a','c']}\n",
    "    print(isCycle(graph2))\n",
    "    \n",
    "udf_isCycle = F.udf(isCycle, BooleanType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last neighbour found on stack: a\n",
      "Part of cycle: e\n",
      "Part of cycle: b\n",
      "True\n",
      "\n",
      "\n",
      "Last neighbour found on stack: a\n",
      "Part of cycle: e\n",
      "Part of cycle: d\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "cycle_unittest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#site_id = '03e668d2-7c72-4451-ae99-2e32b2b97b71'\n",
    "site_id = None\n",
    "\n",
    "df_site_graph = get_site_graph(df_active, site_id=site_id)\n",
    "\n",
    "df_site_graph = df_site_graph.withColumn('cycle_detected', udf_isCycle(F.col('graph')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+--------------+\n",
      "|siteId|graph|cycle_detected|\n",
      "+------+-----+--------------+\n",
      "+------+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_site_graph.filter(F.col('cycle_detected')==False).show()\n",
    "df_site_graph.filter(F.col('cycle_detected')==True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------+\n",
      "|              siteId|               graph|cycle_detected|\n",
      "+--------------------+--------------------+--------------+\n",
      "|0014d8a6-dfa0-4f3...|[ded55039f50b7879...|         false|\n",
      "|0049c3ef-3406-4da...|[8179035e64109b48...|         false|\n",
      "|0101c2ab-c573-4ac...|[a5ea2b030443565b...|         false|\n",
      "|023eb151-afe0-408...|[608dfa66627474b9...|         false|\n",
      "|02a73d0a-06fe-491...|[892301edde69d17b...|         false|\n",
      "|0351532d-4adb-417...|[f8bf3ac422c8d596...|         false|\n",
      "|036da01d-c2ac-49b...|[58c201529841ab55...|         false|\n",
      "|03cdcb4d-0932-4fc...|[81437fc18e762cf8...|         false|\n",
      "|046a45e5-6e83-470...|[26f382d0327189d8...|         false|\n",
      "|0471f96e-a3dd-496...|[5e66612e75190e26...|         false|\n",
      "+--------------------+--------------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'toPandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-9955012f20c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_site_graph_pandas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_site_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'toPandas'"
     ]
    }
   ],
   "source": [
    "df_site_graph_pandas = df_site_graph.show(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
