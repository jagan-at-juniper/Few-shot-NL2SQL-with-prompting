{"metadata": {"kernelspec": {"name": "pysparkkernel", "display_name": "PySpark", "language": "python"}, "language_info": {"name": "pyspark", "mimetype": "text/x-python", "codemirror_mode": {"name": "python", "version": 3}, "file_extension": ".py", "pygments_lexer": "python3"}, "toc": {"nav_menu": {"width": "212px", "height": "77px"}, "number_sections": true, "sideBar": true, "skip_h1_title": false, "base_numbering": 1, "title_cell": "Table of Contents", "title_sidebar": "Contents", "toc_cell": false, "toc_position": {}, "toc_section_display": true, "toc_window_display": false}}, "nbformat_minor": 4, "nbformat": 4, "cells": [{"cell_type": "code", "source": "sc.addPyFile(\"<path_spark_jobs zip file>\")\n\nimport os\nimport pandas as pd\nfrom datetime import datetime\nimport numpy as np\nimport json\nfrom operator import itemgetter\nfrom pprint import pprint\nfrom itertools import groupby\nfrom functools import partial\nfrom elasticsearch_dsl import Search, Q, A\nfrom elasticsearch import Elasticsearch\n\nimport pyspark.sql.functions as F\nfrom pyspark.sql.types import *\nfrom pyspark.sql import Row\n\nfrom common import epoch2datetime\nfrom analytics.utils.time_util import current_epoch_seconds, to_seconds, to_milliseconds\nfrom analytics.jobs.utils import *\nfrom analytics.data_access.utils import parse_search_results\nfrom analytics.transformer.rdd_row_group_transformer import RddRowGroupTransformer\nfrom analytics.transformer.df_2_rdd_transformer import DF2RddTransformer\nfrom analytics.transformer.util import recursive_get_field\nfrom analytics.utils.fs_util import get_file_content\n", "metadata": {"ExecuteTime": {"start_time": "2022-05-20T19:55:28.525596Z", "end_time": "2022-05-20T19:57:21.564933Z"}, "trusted": true}, "execution_count": 1, "outputs": [{"name": "stdout", "text": "Starting Spark application\n", "output_type": "stream"}, {"output_type": "display_data", "data": {"text/plain": "<IPython.core.display.HTML object>", "text/html": "<table>\n<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>300</td><td>application_1652123621130_0302</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-49-255.ec2.internal:20888/proxy/application_1652123621130_0302/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-52-210.ec2.internal:8042/node/containerlogs/container_1652123621130_0302_01_000001/ruchitm\">Link</a></td><td>ruchitm</td><td>\u2714</td></tr></table>"}, "metadata": {}}, {"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}, {"name": "stdout", "text": "SparkSession available as 'spark'.\n", "output_type": "stream"}, {"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}]}, {"cell_type": "markdown", "source": "### Data extract and job execute", "metadata": {}}, {"cell_type": "code", "source": "date = '2022-04-28'\nstart_seconds = int((datetime.strptime(date,'%Y-%m-%d')-datetime(1970,1,1)).total_seconds())\nstart_hour = 1\nn_step = 30\nstart_min=30\ninput_env = 'staging'\ngen_category='t128_log_analysis'\nconfig_file = 'ap-events'\n\n#################################################################\n########### Transform source data and generate event ##########\n#################################################################\n\nstart_time = start_seconds + start_hour*3600 + start_min*60\nend_time = start_time + n_step*60\n\njob = start_debug_job(data_source=config_file, start_epoch=start_time, end_epoch=end_time,\n                      debug_mode=True if input_env=='production' else False)\n\nsource_data = job.data_source_inst.get_data()\n\noperations = job.generators.get(gen_category)\nif operations.get('transformers'):\n    for n, operator in enumerate(operations.get('transformers')):\n        source_data = operator.transform(source_data)\n        \nevent_gen = operations['events'][0]\n", "metadata": {"ExecuteTime": {"start_time": "2022-05-20T19:59:24.406331Z", "end_time": "2022-05-20T19:59:33.681410Z"}, "scrolled": true, "trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "pprint(source_data.take(2))", "metadata": {"ExecuteTime": {"start_time": "2022-05-20T20:15:45.949107Z", "end_time": "2022-05-20T20:15:53.223337Z"}, "trusted": true}, "execution_count": 5, "outputs": [{"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}, {"name": "stdout", "text": "[{'event_type': 'ep-error-events', 'timestamp': 1651109626000, 'source': {'EPEvent': {'topic': 'ep-error-events-', 'hashkey': '3434a400-c693-11ec-aaaa-a6f504427bd9', 'Model': 'SSR', 'Firmware': '5.5.1-1', 'Version': 1, 'ID': '3f42e49b-c693-11ec-8ece-fdf9d8aa11d3', 'Type': 57005, 'Filetype': '', 'Title': 'Error Log message from []', 'Text': '', 'When': '2022-04-28 01:33:46.548277619 +0000 UTC', 'Uptime': 0}, 'Uptime': 0, 'S3filename': 'ssr/3434a400-c693-11ec-aaaa-a6f504427bd9/2022/04/28/01-33-46.548275768_errlog.dump', 'Type': '497659291Z.stack', 'Filetype': 'stack-trace', 'Client_MAC': None, 'BSSID': None, 'EPID': '02-00-01-72-de-91', 'Reason': '', 'DynamicCapture': False}, 'gateway_id': '02000172de91', 'site_id': None, 'org_id': None, 'filetype': 'stack-trace', 'filename': 'ssr/3434a400-c693-11ec-aaaa-a6f504427bd9/2022/04/28/01-33-46.548275768_errlog.dump', 'model': 'SSR', 'firmware': '5.5.1-1', 'ep_timestamp': '2022-04-28 01:33:46.548277619 +0000 UTC'}]", "output_type": "stream"}]}, {"cell_type": "code", "source": "event_rdd = event_gen.generate_event(source_data, job.spark)", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "pprint(event_rdd.take(3))", "metadata": {"trusted": true}, "execution_count": 8, "outputs": [{"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}, {"name": "stdout", "text": "[{'batch_count': 1,\n  'category': 'ssr-availability',\n  'details': {'filename': 'ssr/3434a400-c693-11ec-aaaa-a6f504427bd9/2022/04/28/01-33-46.548275768_errlog.dump',\n              'firmware': '5.5.1-1',\n              'issue_subtype': 'unknown',\n              'issue_type': 'unknown',\n              'model': 'SSR',\n              'signature_id': '000',\n              'stack_frames': []},\n  'detection_delay': 2222510,\n  'detection_time': 1653332136111,\n  'display_entity_id': '02000172de91',\n  'display_entity_type': 'gateway',\n  'enable_action': False,\n  'end_time': 1651109626000,\n  'entity_id': 'none_02000172de91',\n  'entity_type': 'gateway',\n  'event_duration': 0,\n  'event_name': '<placeholder>',\n  'event_type': '<placeholder>',\n  'gateway': {'firmware': '5.5.1-1',\n              'gateway_id': '02000172de91',\n              'model': 'SSR'},\n  'gateway_id': '02000172de91',\n  'mist_only': True,\n  'modification_time': 1651111200000,\n  'occurrence': 1,\n  'org_id': 'none',\n  'row_key': 'none_02000172de91&<placeholder>&<placeholder>&1651109626000',\n  'severity': 60,\n  'site_id': 'none',\n  'start_time': 1651109626000}]", "output_type": "stream"}]}, {"cell_type": "code", "source": "# data_rdd = source_data.map(lambda x: event_gen.filter_and_enrich_event(x)) \\\n#     .filter(lambda x: x.get('site_id', None) is not None) \\\n#     .map(lambda x: event_gen.get_and_classify_trace(x))\n\n# event_gen.logger.info(\"SSR stack trace count : %d\" % (data_rdd.count()))\n\n# group_rdd = data_rdd.groupBy(lambda x: ('_'.join([x['site_id'], x['gateway_id']]), x['timestamp'])) \\\n#     .map(lambda x: (x[0][0], x[1]))\n# feature_rdd = event_gen.compose_entity_features(group_rdd)\n# event_rdd = event_gen.gen_intra_batch_event(feature_rdd)\n# event_rdd = event_gen.cross_batch_event_correlation(event_rdd)", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "# pprint(event_gen.gen_intra_batch_event(feature_rdd).take(3))", "metadata": {"trusted": true}, "execution_count": 5, "outputs": [{"output_type": "display_data", "data": {"text/plain": "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),\u2026", "application/vnd.jupyter.widget-view+json": {"version_major": 2, "version_minor": 0, "model_id": ""}}, "metadata": {}}]}, {"cell_type": "code", "source": "", "metadata": {}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "### Process category", "metadata": {}}, {"cell_type": "code", "source": "# date = '2022-04-28'\n# start_seconds = int((datetime.datetime.strptime(date,'%Y-%m-%d')-datetime.datetime(1970,1,1)).total_seconds())\n# start_hour = 1\n# n_step = 30\n# start_min = 30\n# n_loops = 3*8\n# input_env = 'staging'\n# gen_category='t128_log_analysis'\n# config_file = 'ap-events'\n\n# ##################################################################\n\n# for start_min in range(0, n_loops*n_step, n_step):\n#     start_time = start_seconds + start_hour*3600 + start_min*60\n#     end_time = start_time + n_step*60\n#     job = start_debug_job(data_source=config_file, start_epoch=start_time, end_epoch=end_time,\n#                           debug_mode=True if input_env=='production' else False)\n#     source_data = job.data_source_inst.get_data()\n#     operations = job.generators.get(gen_category)\n#     if operations.get('transformers'):\n#         for n, operator in enumerate(operations.get('transformers')):\n#             operator.spark = spark\n#             source_data = operator.transform(source_data)\n\n#         source_data.persist()\n\n#     if operations.get('events'):\n#         for operator in operations.get('events'):\n#             operator.process_event(source_data, spark)\n\n#         source_data.unpersist()\n#     print(f\"########### {start_min} #########\")", "metadata": {"ExecuteTime": {"start_time": "2021-11-02T19:24:43.112516Z", "end_time": "2021-11-02T19:24:44.346960Z"}}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "", "metadata": {}, "execution_count": null, "outputs": []}]}