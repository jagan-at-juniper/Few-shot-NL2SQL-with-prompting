{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3n://mist-aggregated-stats-production/entity_event/entity_event-production/dt=2020-03-03/hr=00/CapacityAnomalyEvent_*.seq \n",
      " s3n://mist-aggregated-stats-production/entity_event/entity_event-production/dt=2020-03-03/hr=00/CoverageAnomalyEvent_*.seq\n"
     ]
    }
   ],
   "source": [
    "env = \"production\"\n",
    "s3_bucket = \"s3://mist-aggregated-stats-{env}/\".format(env=env)\n",
    "\n",
    "\n",
    "# df = sqlContext.read.format('csv').options(header='true', inferSchema='true').load(s3_path)\n",
    "\n",
    "date_day = \"2020-03-03\"\n",
    "hr = '00'\n",
    "\n",
    "s3_capacity_path = \"entity_event/entity_event-{env}/dt={day}/hr={hr}/CapacityAnomalyEvent_*.seq\".format(env=env, day=date_day, hr=hr)\n",
    "s3_coverage_path = \"entity_event/entity_event-{env}/dt={day}/hr={hr}/CoverageAnomalyEvent_*.seq\".format(env=env, day=date_day, hr=hr)\n",
    "\n",
    "\n",
    "s3_capacity_path = s3_bucket + s3_capacity_path\n",
    "s3_coverage_path = s3_bucket + s3_coverage_path\n",
    "\n",
    "print(s3_capacity_path, \"\\n\", s3_coverage_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.sequenceFile.\n: java.io.IOException: No FileSystem for scheme: s3n\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2660)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:258)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n\tat org.apache.hadoop.mapred.SequenceFileInputFormat.listStatus(SequenceFileInputFormat.java:45)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:204)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1343)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1337)\n\tat org.apache.spark.api.python.SerDeUtil$.pairRDDToPython(SerDeUtil.scala:239)\n\tat org.apache.spark.api.python.PythonRDD$.sequenceFile(PythonRDD.scala:250)\n\tat org.apache.spark.api.python.PythonRDD.sequenceFile(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1e3ae4068b8e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrdd_capacity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequenceFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms3_capacity_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrdd_coverage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequenceFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms3_coverage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.4.4/libexec/python/pyspark/context.py\u001b[0m in \u001b[0;36msequenceFile\u001b[0;34m(self, path, keyClass, valueClass, keyConverter, valueConverter, minSplits, batchSize)\u001b[0m\n\u001b[1;32m    710\u001b[0m         \u001b[0mminSplits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminSplits\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultParallelism\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m         jrdd = self._jvm.PythonRDD.sequenceFile(self._jsc, path, keyClass, valueClass,\n\u001b[0;32m--> 712\u001b[0;31m                                                 keyConverter, valueConverter, minSplits, batchSize)\n\u001b[0m\u001b[1;32m    713\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.4.4/libexec/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.4.4/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/2.4.4/libexec/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.sequenceFile.\n: java.io.IOException: No FileSystem for scheme: s3n\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2660)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\n\tat org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:258)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\n\tat org.apache.hadoop.mapred.SequenceFileInputFormat.listStatus(SequenceFileInputFormat.java:45)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:204)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)\n\tat scala.Option.getOrElse(Option.scala:121)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:251)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1343)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1337)\n\tat org.apache.spark.api.python.SerDeUtil$.pairRDDToPython(SerDeUtil.scala:239)\n\tat org.apache.spark.api.python.PythonRDD$.sequenceFile(PythonRDD.scala:250)\n\tat org.apache.spark.api.python.PythonRDD.sequenceFile(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n"
     ]
    }
   ],
   "source": [
    "rdd_capacity = spark.sparkContext.sequenceFile(s3_capacity_path)\n",
    "rdd_coverage = spark.sparkContext.sequenceFile(s3_coverage_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " '{\"start_time\": 1583192400000, \"end_time\": 1583193600000, \"detection_time\": 1583193761500, \"modification_time\": 1583193290000, \"occurrence\": 2, \"batch_count\": 1, \"site_id\": \"f0cf5c8d-38ce-41ab-8bd4-a3da9d98226f\", \"org_id\": \"604411f1-4e45-4bed-9a69-cc37b247fdf9\", \"ap_id\": \"5c5b35bec198\", \"switch_id\": null, \"event_type\": \"nonwifi\", \"event_name\": \"capacity_anomaly\", \"mist_only\": true, \"enable_action\": false, \"category\": \"capacity\", \"entity_id\": \"f0cf5c8d-38ce-41ab-8bd4-a3da9d98226f_5c5b35bec198_24\", \"entity_type\": \"radio\", \"display_entity_type\": \"ap\", \"detection_delay\": 1361, \"display_entity_id\": \"5c5b35bec198\", \"details\": {\"features\": {\"tot_anomaly_score\": 4.778676754119061, \"max_anomaly_score\": 2.3893383770595307, \"tot_anomaly_count\": 2, \"tot_nclient\": 2.16875, \"max_nclient\": 2.0, \"channel\": {\"1\": 2}, \"bandwidth\": {\"20\": 2}, \"band\": {\"24\": 2}}}, \"row_key\": \"f0cf5c8d-38ce-41ab-8bd4-a3da9d98226f_5c5b35bec198_24&capacity_anomaly&1583192400000\", \"radio\": {\"band\": \"24\", \"dev\": \"r0\", \"channel\": 1, \"bandwidth\": 20}, \"ap\": {\"switch_ip\": \"55.167.143.62\", \"switch_name\": \"s-bak-1.s04974.us.s04974.us.wal-mart.com\", \"switch_port_id\": \"Gi1/0/6\", \"model\": \"AP41-US\", \"firmware\": \"0.5.17230\", \"cpu_lotno\": \"1906\", \"lotno\": \"CBV4600000\", \"scanning_lotno\": \"CBV4600000\", \"chip_ver\": \"C0W1\", \"chip_pn\": \"71.64346.B04\"}, \"severity\": 80, \"event_duration\": 1200}')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_capacity.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# rdd_site = rdd.map(lambda x: json.loads(x[1])).map(lambda x: (x.get(\"site_id\"), x.get(\"ap_id\"))).groupByKey()\n",
    "# rdd_site = rdd.map(lambda x: json.loads(x[1])).map(lambda x: (x.get(\"site_id\"))).groupBy('site_id')\n",
    "# df_capacity = rdd_capacity.toDF() #.map(lambda x: json.loads(x[1])).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd_site.take(1)\n",
    "rdd_capacity_by_site= rdd_capacity.map(lambda x: json.loads(x[1])).map(lambda x: (x.get(\"site_id\"),1)).countByKey()\n",
    "\n",
    "rdd_coverage_by_site= rdd_coverage.map(lambda x: json.loads(x[1])).map(lambda x: (x.get(\"site_id\"),1)).countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_capacity_sorted = sorted(rdd_capacity_by_site.items(), key=lambda v:v[1], reverse=True) #, key=lambda(k, v): v)\n",
    "site_coverage_sorted = sorted(rdd_coverage_by_site.items(), key=lambda v:v[1], reverse=True) #, key=lambda(k, v): v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('96488ea8-6dee-11e6-a43a-02e208b2d34f', 111),\n",
       " ('c14381b7-a3fe-4a85-ac51-11813efded19', 48),\n",
       " ('cc0ca445-6fe9-4266-a21c-6fd6ef178950', 26),\n",
       " ('5e8fe474-a9ee-4d01-a2b6-b022b0f9c869', 26),\n",
       " ('f2543ad9-331f-48de-984f-4f1190837df1', 26),\n",
       " ('13d68843-7ebd-42a1-8528-495c41ccf8ef', 24),\n",
       " ('6d440882-04fe-41d1-a82e-336eea598713', 24),\n",
       " ('67151fdb-2d69-4e07-a2b9-0652c3d5bec5', 24),\n",
       " ('39d7f957-4723-416d-82a4-0ef179830e80', 22),\n",
       " ('b030902f-567b-4f32-9f01-621f9fbd5531', 21)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "site_capacity_sorted[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('96488ea8-6dee-11e6-a43a-02e208b2d34f', 53),\n",
       " ('5cf9de1a-8034-4472-86f6-9a100ebdb792', 39),\n",
       " ('50fa3df4-053f-4779-86d5-f53d74343077', 33),\n",
       " ('a799e12a-fc28-4ce7-a563-c2dd6d498e2f', 30),\n",
       " ('68fdb9ba-7b27-4293-9bf2-ad1c987c9466', 27),\n",
       " ('c14381b7-a3fe-4a85-ac51-11813efded19', 25),\n",
       " ('43143ae2-d1ce-4802-b112-fa688bc7dca4', 20),\n",
       " ('28a11fdb-7d32-4abc-b8c0-c1eda38b9f92', 15),\n",
       " ('469fc38e-33da-4562-8b4c-6720c43a53e0', 13),\n",
       " ('d8b5b67f-dc58-409e-94f1-f1f5c6246b03', 12)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "site_coverage_sorted[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd_coverage_by_site.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://mist-aggregated-stats-production/aggregated-stats/top_1_time_epoch_by_site_ap_ap2_band/dt=2020-03-03/hr=00/last_0.25_day/*.csv\n",
      "root\n",
      " |-- site: string (nullable = true)\n",
      " |-- ap: string (nullable = true)\n",
      " |-- ap2: string (nullable = true)\n",
      " |-- band: string (nullable = true)\n",
      " |-- time_epoch: string (nullable = true)\n",
      " |-- rssi: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# file_path = 's3://mist-secorapp-{env}/oc-stats-analytics/oc-stats-analytics-{env}/dt={date}/*'.format(env=env, date=DATE)\n",
    "\n",
    "s3_apscan_bucket = 's3://mist-aggregated-stats-{ENV}/aggregated-stats/top_1_time_epoch_by_site_ap_ap2_band/'.format(ENV=env)\n",
    "\n",
    "\n",
    "s3_apscan_path = s3_apscan_bucket + \"dt={date}/hr={hour}/last_0.25_day/*.csv\".format(date=date_day, hour=hr)\n",
    "\n",
    "print(s3_apscan_path)\n",
    "\n",
    "# file_path = \"s3a://mist-aggregated-stats-production/aggregated-stats/top_1_time_epoch_by_site_ap_ap2_band/dt=2020-02-26/hr=17/last_0.25_day/part-00000-8b45ee45-d247-4bc9-b4df-b621135297eb-c000.csv\"\n",
    "df_apscan= spark.read.option(\"header\", True).csv(s3_apscan_path)\n",
    "\n",
    "# df = spark.read.parquet(file_path)\n",
    "df_apscan.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+------------+----+----------+-----+----+\n",
      "|                site|          ap|         ap2|band|time_epoch| rssi|date|\n",
      "+--------------------+------------+------------+----+----------+-----+----+\n",
      "|00193f17-4e86-476...|5c5b357eec90|5c5b35aed787|  24|1583197064|-65.0|null|\n",
      "|00193f17-4e86-476...|5c5b35aed61f|5c5b35aed791|  24|1583197062|-63.0|null|\n",
      "|00193f17-4e86-476...|5c5b35aed7e6|5c5b35aed7cd|  24|1583191062|-67.0|null|\n",
      "|00226d33-cd80-4b1...|5c5b35501a7c|        null|  24|1583197014|-61.0|null|\n",
      "|00272816-8dc2-4f1...|5c5b357f2822|5c5b35ae6ced|  24|1583197063|-67.0|null|\n",
      "|00272816-8dc2-4f1...|5c5b357f284f|5c5b35ae7152|   5|1583197062|-59.0|null|\n",
      "|00272816-8dc2-4f1...|5c5b357f32c7|5c5b35ae6d1f|  24|1583196761|-76.0|null|\n",
      "|00272816-8dc2-4f1...|5c5b357f3308|        null|   5|1583197064|-59.0|null|\n",
      "|00272816-8dc2-4f1...|5c5b35ae6e55|5c5b35ae6c89|   5|1583197062|-64.0|null|\n",
      "|00272816-8dc2-4f1...|5c5b35ae6e55|5c5b35ae6d97|   5|1583197062|-69.0|null|\n",
      "|00272816-8dc2-4f1...|5c5b35ae7152|5c5b35ae6e5a|   5|1583197060|-72.0|null|\n",
      "|00458c29-d67b-4cf...|5c5b352ee141|5c5b353e5708|  24|1583197025|-71.0|null|\n",
      "|00458c29-d67b-4cf...|5c5b353e56cc|5c5b353e56f9|   5|1583197104|-59.0|null|\n",
      "|00458c29-d67b-4cf...|5c5b353e579e|5c5b353e571c|  24|1583197088|-63.0|null|\n",
      "|00458c29-d67b-4cf...|5c5b353e6455|5c5b353e56f9|  24|1583197107|-58.0|null|\n",
      "|00458c29-d67b-4cf...|5c5b353e6455|5c5b353e573f|   5|1583197107|-63.0|null|\n",
      "|00458c29-d67b-4cf...|5c5b353e6469|        null|  24|1583197092|-73.0|null|\n",
      "|00544ad7-df28-4b9...|5c5b358f167a|5c5b358ef9b5|   5|1583197138|-51.0|null|\n",
      "|00544ad7-df28-4b9...|5c5b358f167a|5c5b358f1620|  24|1583197138|-58.0|null|\n",
      "|00638b36-6912-40c...|5c5b35ceaa55|5c5b35ceac8f|   5|1583177929|-69.0|null|\n",
      "+--------------------+------------+------------+----+----------+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import DateType\n",
    "\n",
    "# df_ap.select(\"time_epoch\").show()\n",
    "df_apscan.withColumn('date', df_apscan['time_epoch'].cast(DateType())).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graphframes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-02cbcaf60c0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgraphframes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'graphframes'"
     ]
    }
   ],
   "source": [
    "from graphframes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
