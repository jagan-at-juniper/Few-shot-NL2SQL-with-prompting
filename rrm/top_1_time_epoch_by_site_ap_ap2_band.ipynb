{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://mist-aggregated-stats-production/aggregated-stats/top_1_time_epoch_by_site_ap_ap2_band/dt=2020-03-19/hr=20/*.csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, size, avg, count, col,sum, explode\n",
    "import json\n",
    "\n",
    "env = \"production\"\n",
    "#s3://mist-aggregated-stats-production/aggregated-stats/top_1_time_epoch_by_site_ap_ap2_band/dt=2020-03-14/hr=16/part-00000-76298f84-5414-49c5-88f6-a981ff06d94d-c000.csv .\n",
    "\n",
    "s3_bucket = \"s3://mist-aggregated-stats-{env}/aggregated-stats/\".format(env=env)\n",
    "\n",
    "date_day = \"2020-03-19\"\n",
    "hr = '20'\n",
    "\n",
    "rrm_local_path = \"top_1_time_epoch_by_site_ap_ap2_band/dt={day}/hr={hr}/*.csv\".format(env=env, day=date_day, hr=hr)\n",
    "\n",
    "rrm_local_path = s3_bucket + rrm_local_path\n",
    "print(rrm_local_path)\n",
    "\n",
    "\n",
    "\n",
    "# user_org_id = \"38a18d4d-1623-4985-86d0-1bb06e5e2a48\"  # UPS\n",
    "# user_site_USNYELE = '9aa6ecdb-ddee-41b4-a8d9-872d7962b3c1'  #UPS USNYELE\n",
    "\n",
    "# # user_org_id = '38a18d4d-1623-4985-86d0-1bb06e5e2a48'  # UPS\n",
    "# user_site_USFLLON = 'c23667f3-47e9-44aa-9761-de49d34ed9f9' # USFLLON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['site,ap,ap2,band,time_epoch,rssi',\n",
       " '0009dab3-7a6b-4ace-85ff-e6dc20768bcb,5c5b358e297c,\"\",5,1584651493,-50.0',\n",
       " '000cb16d-91d5-42e2-9c1f-a0319a8116bd,5c5b35506699,5c5b3550629d,5,1584651407,-80.0']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_rrm_local = spark.sparkContext.textFile(rrm_local_path)\n",
    "rdd_rrm_local.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd_rrm_local.map(lambda line: (line.split(',')[0], line.split(',')[1])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_scan = spark.read.format(\"csv\")\\\n",
    "   .option(\"header\", \"true\")\\\n",
    "   .option(\"inferSchema\", \"true\")\\\n",
    "   .load(rrm_local_path)\n",
    "df_scan.createOrReplaceTempView(\"scan_data\")\n",
    "df_Schema = df_scan.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(site,StringType,true),StructField(ap,StringType,true),StructField(ap2,StringType,true),StructField(band,IntegerType,true),StructField(time_epoch,IntegerType,true),StructField(rssi,DoubleType,true)))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+------------+----+----------+-----+\n",
      "|                site|          ap|         ap2|band|time_epoch| rssi|\n",
      "+--------------------+------------+------------+----+----------+-----+\n",
      "|0009dab3-7a6b-4ac...|5c5b358e297c|        null|   5|1584651493|-50.0|\n",
      "|000cb16d-91d5-42e...|5c5b35506699|5c5b3550629d|   5|1584651407|-80.0|\n",
      "|00193f17-4e86-476...|5c5b357eeb7d|5c5b35aed61f|   5|1584651461|-78.0|\n",
      "|00193f17-4e86-476...|5c5b35aed7e6|5c5b35aed818|  24|1584651462|-59.0|\n",
      "|00193f17-4e86-476...|5c5b35aed872|5c5b35aed7e6|  24|1584651465|-66.0|\n",
      "|00193f17-4e86-476...|5c5b35aed87c|5c5b35aed877|  24|1584651466|-75.0|\n",
      "|00226d33-cd80-4b1...|5c5b35501f9f|5c5b35502003|  24|1584651085|-64.0|\n",
      "|00252227-5f8f-4e6...|5c5b352e8f25|        null|  24|1584651485|-61.0|\n",
      "|00272816-8dc2-4f1...|5c5b35ae6dc9|5c5b357f32c7|  24|1584651463|-72.0|\n",
      "|00458c29-d67b-4cf...|5c5b353e5573|5c5b353e5555|  24|1584651478|-61.0|\n",
      "|00458c29-d67b-4cf...|5c5b353e565e|5c5b353e579e|   5|1584651499|-67.0|\n",
      "|00458c29-d67b-4cf...|5c5b353e5712|5c5b353e573a|  24|1584651487|-55.0|\n",
      "|00458c29-d67b-4cf...|5c5b353e571c|5c5b353e6464|  24|1584651513|-63.0|\n",
      "|00458c29-d67b-4cf...|5c5b35ceffdc|5c5b353e645a|   5|1584651533|-69.0|\n",
      "|00638b36-6912-40c...|5c5b35ceac8f|5c5b35ceaf41|  24|1584648227|-72.0|\n",
      "|00638b36-6912-40c...|5c5b35ceaca3|5c5b35ceab90|   5|1584651491|-73.0|\n",
      "|008626f2-7bc2-4ab...|5c5b357e016d|5c5b357e0140|  24|1584651179|-81.0|\n",
      "|008626f2-7bc2-4ab...|5c5b359ef749|        null|   5|1584651515|-71.0|\n",
      "|008626f2-7bc2-4ab...|5c5b359efb63|        null|  24|1584651489|-76.0|\n",
      "|0093c451-3a9b-4d0...|5c5b352f6a1c|        null|  24|1584649920|-78.0|\n",
      "+--------------------+------------+------------+----+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_scan.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------+------------+-----------------+--------------------+------------------+\n",
      "|summary|                site|          ap|         ap2|             band|          time_epoch|              rssi|\n",
      "+-------+--------------------+------------+------------+-----------------+--------------------+------------------+\n",
      "|  count|             2698488|     2698488|     2472892|          2698488|             2698488|           2698488|\n",
      "|   mean|                null|        null|        null|16.10877943500212|1.5846512685131106E9|-66.59862078319415|\n",
      "| stddev|                null|        null|        null|9.362791315364358|   630.5341625638516| 8.445860009209433|\n",
      "|    min|0009dab3-7a6b-4ac...|5c5b350e011c|5c5b350e0090|                5|          1584648000|             -96.0|\n",
      "|    max|fffb6917-ee0e-45c...|d420b080bb76|d420b080bb76|               24|          1584651598|             -11.0|\n",
      "+-------+--------------------+------------+------------+-----------------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_scan.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2698488"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scan.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_id = \"d1ee1d22-4b55-4c97-97c4-9d757144f45b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scan_site = df_scan.filter(col('site')==site_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scan_site.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+------------+----+----------+-----+\n",
      "|                site|          ap|         ap2|band|time_epoch| rssi|\n",
      "+--------------------+------------+------------+----+----------+-----+\n",
      "|d1ee1d22-4b55-4c9...|5c5b3552b796|5c5b3552b96c|  24|1584651471|-63.0|\n",
      "|d1ee1d22-4b55-4c9...|5c5b3552b94e|5c5b3552b660|   5|1584651376|-63.0|\n",
      "|d1ee1d22-4b55-4c9...|5c5b3552b7cd|5c5b3552b796|  24|1584651396|-53.0|\n",
      "|d1ee1d22-4b55-4c9...|5c5b3552b7cd|5c5b3552bb6f|  24|1584651396|-57.0|\n",
      "|d1ee1d22-4b55-4c9...|5c5b3552b7cd|5c5b3552b5e3|   5|1584651396|-66.0|\n",
      "|d1ee1d22-4b55-4c9...|5c5b3552b96c|5c5b3552b796|   5|1584651357|-68.0|\n",
      "|d1ee1d22-4b55-4c9...|5c5b3552b70a|5c5b3552b7cd|  24|1584651450|-66.0|\n",
      "|d1ee1d22-4b55-4c9...|5c5b3552b714|5c5b3552b796|  24|1584651358|-56.0|\n",
      "|d1ee1d22-4b55-4c9...|5c5b3552ba70|5c5b3552b5e3|  24|1584651347|-55.0|\n",
      "|d1ee1d22-4b55-4c9...|5c5b3552b94e|5c5b3552b5e3|  24|1584651376|-65.0|\n",
      "|d1ee1d22-4b55-4c9...|5c5b3552ba70|5c5b3552b660|   5|1584651347|-61.0|\n",
      "|d1ee1d22-4b55-4c9...|5c5b3552b96c|5c5b3552bb6f|   5|1584651357|-68.0|\n",
      "|d1ee1d22-4b55-4c9...|d420b000a866|5c5b3552ba70|  24|1584649944|-80.0|\n",
      "|d1ee1d22-4b55-4c9...|5c5b3552b5e3|5c5b3552b660|  24|1584651395|-56.0|\n",
      "|d1ee1d22-4b55-4c9...|5c5b3552b96c|5c5b3552b7cd|   5|1584651357|-54.0|\n",
      "|d1ee1d22-4b55-4c9...|5c5b3552b5e3|5c5b3552b94e|  24|1584651395|-67.0|\n",
      "|d1ee1d22-4b55-4c9...|5c5b3552b94e|5c5b3552b660|  24|1584651376|-56.0|\n",
      "|d1ee1d22-4b55-4c9...|5c5b3552bb6f|        null|  24|1584651502|-70.0|\n",
      "|d1ee1d22-4b55-4c9...|5c5b3552b796|5c5b3552b70a|   5|1584651471|-60.0|\n",
      "|d1ee1d22-4b55-4c9...|5c5b3552b660|5c5b3552b70a|   5|1584651389|-60.0|\n",
      "+--------------------+------------+------------+----+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_scan_site.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphFrames\n",
    "The following command creates your first GraphFrame. The GraphFrame accepts two DataFrames as inputs—vertices and edges. GraphFrames like to have a naming convention in the column name, which you need to follow. Those rules are defined as follows:\n",
    "* A DataFrame that represents vertices should contain a column named id. Here, personsDf contains a column name \"id\".\n",
    "\n",
    "* A DataFrame that represents edges should contain columns named src and dst. Here, reationshipDf contains the columns \"src\" and \"dst\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices= df_scan_site.selectExpr(\"ap as id\").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges=df_scan_site.selectExpr(\"ap as src\", \"ap2 as dst\", \"rssi as weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import *\n",
    "g = GraphFrame(vertices, edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|          id|\n",
      "+------------+\n",
      "|5c5b3552b796|\n",
      "|5c5b3552ba70|\n",
      "|d420b000a866|\n",
      "|5c5b3552b96c|\n",
      "|5c5b3552bb6f|\n",
      "|5c5b3552b70a|\n",
      "|5c5b3552b714|\n",
      "|5c5b3552b94e|\n",
      "|5c5b3552b5e3|\n",
      "|5c5b3552b7cd|\n",
      "|5c5b3552b660|\n",
      "+------------+\n",
      "\n",
      "+------------+------------+------+\n",
      "|         src|         dst|weight|\n",
      "+------------+------------+------+\n",
      "|5c5b3552b796|5c5b3552b96c| -63.0|\n",
      "|5c5b3552b94e|5c5b3552b660| -63.0|\n",
      "|5c5b3552b7cd|5c5b3552b796| -53.0|\n",
      "|5c5b3552b7cd|5c5b3552bb6f| -57.0|\n",
      "|5c5b3552b7cd|5c5b3552b5e3| -66.0|\n",
      "|5c5b3552b96c|5c5b3552b796| -68.0|\n",
      "|5c5b3552b70a|5c5b3552b7cd| -66.0|\n",
      "|5c5b3552b714|5c5b3552b796| -56.0|\n",
      "|5c5b3552ba70|5c5b3552b5e3| -55.0|\n",
      "|5c5b3552b94e|5c5b3552b5e3| -65.0|\n",
      "|5c5b3552ba70|5c5b3552b660| -61.0|\n",
      "|5c5b3552b96c|5c5b3552bb6f| -68.0|\n",
      "|d420b000a866|5c5b3552ba70| -80.0|\n",
      "|5c5b3552b5e3|5c5b3552b660| -56.0|\n",
      "|5c5b3552b96c|5c5b3552b7cd| -54.0|\n",
      "|5c5b3552b5e3|5c5b3552b94e| -67.0|\n",
      "|5c5b3552b94e|5c5b3552b660| -56.0|\n",
      "|5c5b3552bb6f|        null| -70.0|\n",
      "|5c5b3552b796|5c5b3552b70a| -60.0|\n",
      "|5c5b3552b660|5c5b3552b70a| -60.0|\n",
      "+------------+------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------------+------+\n",
      "|          id|degree|\n",
      "+------------+------+\n",
      "|5c5b3552b796|    36|\n",
      "|        null|    22|\n",
      "|5c5b3552ba70|    39|\n",
      "|d420b000a866|     7|\n",
      "|5c5b3552b96c|    36|\n",
      "|5c5b3552bb6f|    20|\n",
      "|5c5b3552b70a|    34|\n",
      "|5c5b3552b714|    31|\n",
      "|5c5b3552b94e|    36|\n",
      "|5c5b3552b5e3|    36|\n",
      "|5c5b3552b7cd|    38|\n",
      "|5c5b3552b660|    33|\n",
      "+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g.vertices.show()\n",
    "g.edges.show()\n",
    "## Check the number of edges of each vertex\n",
    "g.degrees.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphFrame(v:[id: string], e:[src: string, dst: string ... 1 more field])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|          id|degree|\n",
      "+------------+------+\n",
      "|5c5b3552b5e3|    36|\n",
      "+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The following code gets the number of incoming links to Andrew. This is obtained by calling the inDegrees method.\n",
    "\n",
    "g.degrees.filter(\"id = '5c5b3552b5e3'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+\n",
      "|          id|outDegree|\n",
      "+------------+---------+\n",
      "|5c5b3552b5e3|       18|\n",
      "+------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The following code shows how to get the number of links coming out from Andrew using the outDegrees method .\n",
    "\n",
    "g.outDegrees.filter(\"id = '5c5b3552b5e3'\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = g.pageRank(resetProbability=0.10, maxIter=5)\n",
    "# display(ranks.vertices.select(\"id\",\"pagerank\").orderBy(desc(\"pagerank\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, pagerank: double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[src: string, dst: string, weight: double, weight: double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(ranks.vertices)\n",
    "display(ranks.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"cannot resolve '`ap1`' given input columns: [id]; line 1 pos 5;\\n'Filter (id#463 = 'ap1)\\n+- Deduplicate [id#463]\\n   +- Project [ap#11 AS id#463]\\n      +- Filter (site#10 = d1ee1d22-4b55-4c97-97c4-9d757144f45b)\\n         +- Relation[site#10,ap#11,ap2#12,band#13,time_epoch#14,rssi#15] csv\\n\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/mistsys/ds_incubator/venv/spark-2.4.4-bin-without-hadoop/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/mistsys/ds_incubator/venv/spark-2.4.4-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o171.run.\n: org.apache.spark.sql.AnalysisException: cannot resolve '`ap1`' given input columns: [id]; line 1 pos 5;\n'Filter (id#463 = 'ap1)\n+- Deduplicate [id#463]\n   +- Project [ap#11 AS id#463]\n      +- Filter (site#10 = d1ee1d22-4b55-4c97-97c4-9d757144f45b)\n         +- Relation[site#10,ap#11,ap2#12,band#13,time_epoch#14,rssi#15] csv\n\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:280)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:329)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:278)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)\n\tat org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:176)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:182)\n\tat org.apache.spark.sql.Dataset$.apply(Dataset.scala:64)\n\tat org.apache.spark.sql.Dataset.withTypedPlan(Dataset.scala:3417)\n\tat org.apache.spark.sql.Dataset.filter(Dataset.scala:1490)\n\tat org.graphframes.lib.BFS$.org$graphframes$lib$BFS$$run(BFS.scala:137)\n\tat org.graphframes.lib.BFS.run(BFS.scala:124)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-7e0e6e447af4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0map1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"5c5b3552b96c\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0map2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"5c5b3552b5e3\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbfs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"id = ap1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"id = ap2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mpaths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/private/var/folders/h4/fwrdz9nj4w71vjkf35vqgk4h0000gn/T/spark-7e971628-f650-467a-976c-11592f896ab5/userFiles-21b5350a-b560-41df-b4fc-70f271c9fceb/graphframes_graphframes-0.8.0-spark2.4-s_2.11.jar/graphframes/graphframe.py\u001b[0m in \u001b[0;36mbfs\u001b[0;34m(self, fromExpr, toExpr, edgeFilter, maxPathLength)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0medgeFilter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m             \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medgeFilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medgeFilter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m         \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sqlContext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/mistsys/ds_incubator/venv/spark-2.4.4-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/mistsys/ds_incubator/venv/spark-2.4.4-bin-without-hadoop/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"cannot resolve '`ap1`' given input columns: [id]; line 1 pos 5;\\n'Filter (id#463 = 'ap1)\\n+- Deduplicate [id#463]\\n   +- Project [ap#11 AS id#463]\\n      +- Filter (site#10 = d1ee1d22-4b55-4c97-97c4-9d757144f45b)\\n         +- Relation[site#10,ap#11,ap2#12,band#13,time_epoch#14,rssi#15] csv\\n\""
     ]
    }
   ],
   "source": [
    "# Search from \"Esther\" for users of age < 32.\n",
    "\n",
    "ap1 = \"5c5b3552b96c\"\n",
    "ap2 = \"5c5b3552b5e3\"\n",
    "paths = g.bfs(\"id = ap1\", \"id = ap2\")\n",
    "paths.show()\n",
    "\n",
    "# Specify edge filters or max path lengths.\n",
    "g.bfs(\"id = ap1\", \"id = ap2\",\\\n",
    "  edgeFilter=\"rssi > -75\", maxPathLength=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|          id|   component|\n",
      "+------------+------------+\n",
      "|5c5b3552b796|274877906944|\n",
      "|5c5b3552b714|274877906944|\n",
      "|5c5b3552b660|274877906944|\n",
      "|5c5b3552b70a|274877906944|\n",
      "|5c5b3552b7cd|274877906944|\n",
      "|5c5b3552b94e|274877906944|\n",
      "|5c5b3552ba70|274877906944|\n",
      "|5c5b3552b5e3|274877906944|\n",
      "|5c5b3552bb6f|274877906944|\n",
      "|5c5b3552b96c|274877906944|\n",
      "|d420b000a866|558345748480|\n",
      "+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# g.connectedComponents()\n",
    "\n",
    "result1 = g.stronglyConnectedComponents(maxIter=10)\n",
    "result1.select(\"id\", \"component\").orderBy(\"component\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, component: bigint]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|          id|   component|\n",
      "+------------+------------+\n",
      "|5c5b3552b70a|274877906944|\n",
      "|5c5b3552b796|274877906944|\n",
      "|5c5b3552b7cd|274877906944|\n",
      "|5c5b3552b660|274877906944|\n",
      "|5c5b3552bb6f|274877906944|\n",
      "|5c5b3552b714|274877906944|\n",
      "|5c5b3552b96c|274877906944|\n",
      "|5c5b3552b5e3|274877906944|\n",
      "|5c5b3552b94e|274877906944|\n",
      "|5c5b3552ba70|274877906944|\n",
      "|d420b000a866|558345748480|\n",
      "+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result2 = g.stronglyConnectedComponents(maxIter=10)\n",
    "result2.select(\"id\", \"component\").orderBy(\"component\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, component: bigint]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2020-03-1920'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_day + hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://mist-test-bucket/ s3://mist-test-bucket/\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o432.parquet.\n: org.apache.hadoop.fs.s3a.AWSS3IOException: getFileStatus on : com.amazonaws.services.s3.model.AmazonS3Exception: The provided token has expired. (Service: Amazon S3; Status Code: 400; Error Code: ExpiredToken; Request ID: 94C7F957ACE921E9), S3 Extended Request ID: AkMigxMFtCX4gAWlM4ADF02zC8komIGrBGt27VdO3KxwIRd0y2pMzgQKayQDapblniWDzPJ9uEE=: The provided token has expired. (Service: Amazon S3; Status Code: 400; Error Code: ExpiredToken; Request ID: 94C7F957ACE921E9)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:178)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1635)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:117)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1437)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.exists(S3AFileSystem.java:2040)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:93)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.amazonaws.services.s3.model.AmazonS3Exception: The provided token has expired. (Service: Amazon S3; Status Code: 400; Error Code: ExpiredToken; Request ID: 94C7F957ACE921E9), S3 Extended Request ID: AkMigxMFtCX4gAWlM4ADF02zC8komIGrBGt27VdO3KxwIRd0y2pMzgQKayQDapblniWDzPJ9uEE=\n\tat com.amazonaws.http.AmazonHttpClient.handleErrorResponse(AmazonHttpClient.java:1182)\n\tat com.amazonaws.http.AmazonHttpClient.executeOneRequest(AmazonHttpClient.java:770)\n\tat com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:489)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:310)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3785)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3738)\n\tat com.amazonaws.services.s3.AmazonS3Client.listObjects(AmazonS3Client.java:653)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.listObjects(S3AFileSystem.java:918)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1611)\n\t... 36 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-49-8d8f4c0f296f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms3_out_bucket_vertices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms3_out_bucket_edges\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvertices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms3_out_bucket_vertices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medges\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms3_out_bucket_vertices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# s3_out_bucket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/mistsys/ds_incubator/venv/spark-2.4.4-bin-without-hadoop/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    841\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/mistsys/ds_incubator/venv/spark-2.4.4-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/mistsys/ds_incubator/venv/spark-2.4.4-bin-without-hadoop/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/mistsys/ds_incubator/venv/spark-2.4.4-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o432.parquet.\n: org.apache.hadoop.fs.s3a.AWSS3IOException: getFileStatus on : com.amazonaws.services.s3.model.AmazonS3Exception: The provided token has expired. (Service: Amazon S3; Status Code: 400; Error Code: ExpiredToken; Request ID: 94C7F957ACE921E9), S3 Extended Request ID: AkMigxMFtCX4gAWlM4ADF02zC8komIGrBGt27VdO3KxwIRd0y2pMzgQKayQDapblniWDzPJ9uEE=: The provided token has expired. (Service: Amazon S3; Status Code: 400; Error Code: ExpiredToken; Request ID: 94C7F957ACE921E9)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:178)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1635)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:117)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1437)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.exists(S3AFileSystem.java:2040)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:93)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:566)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.amazonaws.services.s3.model.AmazonS3Exception: The provided token has expired. (Service: Amazon S3; Status Code: 400; Error Code: ExpiredToken; Request ID: 94C7F957ACE921E9), S3 Extended Request ID: AkMigxMFtCX4gAWlM4ADF02zC8komIGrBGt27VdO3KxwIRd0y2pMzgQKayQDapblniWDzPJ9uEE=\n\tat com.amazonaws.http.AmazonHttpClient.handleErrorResponse(AmazonHttpClient.java:1182)\n\tat com.amazonaws.http.AmazonHttpClient.executeOneRequest(AmazonHttpClient.java:770)\n\tat com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:489)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:310)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3785)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3738)\n\tat com.amazonaws.services.s3.AmazonS3Client.listObjects(AmazonS3Client.java:653)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.listObjects(S3AFileSystem.java:918)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1611)\n\t... 36 more\n"
     ]
    }
   ],
   "source": [
    "s3_out_bucket = \"s3://mist-test-bucket/\" #top_1_time_epoch_by_site_ap_ap2_band/{}/{}/\".format(date_day, hr)\n",
    "s3_out_bucket_vertices = s3_out_bucket #+ \"vertices/\"\n",
    "s3_out_bucket_edges = s3_out_bucket #+ \"edges/\"\n",
    "\n",
    "print(s3_out_bucket_vertices, s3_out_bucket_edges)\n",
    "g.vertices.write.parquet(s3_out_bucket_vertices)\n",
    "g.edges.write.parquet(s3_out_bucket_vertices)\n",
    "# s3_out_bucket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3_out_bucket, rrm_local_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DST',\n",
       " 'ID',\n",
       " 'SRC',\n",
       " '_ATTR',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_edges',\n",
       " '_jvm_gf_api',\n",
       " '_jvm_graph',\n",
       " '_sc',\n",
       " '_sqlContext',\n",
       " '_vertices',\n",
       " 'aggregateMessages',\n",
       " 'bfs',\n",
       " 'cache',\n",
       " 'connectedComponents',\n",
       " 'degrees',\n",
       " 'dropIsolatedVertices',\n",
       " 'edges',\n",
       " 'filterEdges',\n",
       " 'filterVertices',\n",
       " 'find',\n",
       " 'inDegrees',\n",
       " 'labelPropagation',\n",
       " 'outDegrees',\n",
       " 'pageRank',\n",
       " 'parallelPersonalizedPageRank',\n",
       " 'persist',\n",
       " 'pregel',\n",
       " 'shortestPaths',\n",
       " 'stronglyConnectedComponents',\n",
       " 'svdPlusPlus',\n",
       " 'triangleCount',\n",
       " 'triplets',\n",
       " 'unpersist',\n",
       " 'vertices']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph.degrees.filter(\"id = 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GraphFrame provides the following built-in algorithms:\n",
    "Connected components\n",
    "\n",
    "Label propagation\n",
    "\n",
    "PageRank\n",
    "\n",
    "SVD++\n",
    "\n",
    "Shortest Path\n",
    "\n",
    "Strongly connected components\n",
    "\n",
    "Triangle count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
