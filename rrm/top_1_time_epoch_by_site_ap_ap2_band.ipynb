{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://mist-aggregated-stats-production/aggregated-stats/top_1_time_epoch_by_site_ap_ap2_band/dt=2020-08-10/hr=20/*.csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, size, avg, count, col,sum, explode\n",
    "import json\n",
    "\n",
    "env = \"production\"\n",
    "#s3://mist-aggregated-stats-production/aggregated-stats/top_1_time_epoch_by_site_ap_ap2_band/dt=2020-03-14/hr=16/part-00000-76298f84-5414-49c5-88f6-a981ff06d94d-c000.csv .\n",
    "\n",
    "s3_bucket = \"s3://mist-aggregated-stats-{env}/aggregated-stats/\".format(env=env)\n",
    "date_day = \"2020-08-10\"\n",
    "hr = '20'\n",
    "\n",
    "ap_neighbors_path = \"top_1_time_epoch_by_site_ap_ap2_band/dt={day}/hr={hr}/*.csv\".format(env=env, day=date_day, hr=hr)\n",
    "\n",
    "ap_neighbors_path = s3_bucket + ap_neighbors_path\n",
    "print(ap_neighbors_path)\n",
    "\n",
    "\n",
    "\n",
    "# user_org_id = \"38a18d4d-1623-4985-86d0-1bb06e5e2a48\"  # UPS\n",
    "# user_site_USNYELE = '9aa6ecdb-ddee-41b4-a8d9-872d7962b3c1'  #UPS USNYELE\n",
    "\n",
    "# # user_org_id = '38a18d4d-1623-4985-86d0-1bb06e5e2a48'  # UPS\n",
    "# user_site_USFLLON = 'c23667f3-47e9-44aa-9761-de49d34ed9f9' # USFLLON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['site,ap,ap2,band,time_epoch,rssi',\n",
       " '0000063a-a4ce-4091-8921-c37560d46e47,d420b000ee16,d420b000ee07,5,1597092510,-68.0',\n",
       " '0000063a-a4ce-4091-8921-c37560d46e47,d420b000ee5c,d420b000ee7f,24,1597092493,-54.0']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_ap_neigbhors = spark.sparkContext.textFile(ap_neighbors_path)\n",
    "rdd_ap_neigbhors.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd_rrm_local.map(lambda line: (line.split(',')[0], line.split(',')[1])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_scan = spark.read.format(\"csv\")\\\n",
    "   .option(\"header\", \"true\")\\\n",
    "   .option(\"inferSchema\", \"true\")\\\n",
    "   .load(ap_neighbors_path)\n",
    "df_scan.createOrReplaceTempView(\"scan_data\")\n",
    "df_Schema = df_scan.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(site,StringType,true),StructField(ap,StringType,true),StructField(ap2,StringType,true),StructField(band,IntegerType,true),StructField(time_epoch,IntegerType,true),StructField(rssi,DoubleType,true)))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+------------+----+----------+-----+\n",
      "|                site|          ap|         ap2|band|time_epoch| rssi|\n",
      "+--------------------+------------+------------+----+----------+-----+\n",
      "|0000063a-a4ce-409...|d420b000ee16|d420b000ee07|   5|1597092510|-68.0|\n",
      "|0000063a-a4ce-409...|d420b000ee5c|d420b000ee7f|  24|1597092493|-54.0|\n",
      "|0000063a-a4ce-409...|d420b000ee70|d420b000ee20|  24|1597092511|-56.0|\n",
      "|0000063a-a4ce-409...|d420b080eb69|d420b080eb37|   5|1597092479|-56.0|\n",
      "|000cb16d-91d5-42e...|5c5b35505c85|        null|  24|1597092441|-74.0|\n",
      "|000e9e7e-d400-4b2...|5c5b352e7328|5c5b352e73a0|   5|1597092495|-62.0|\n",
      "|0014d8a6-dfa0-4f3...|d420b0804df3|d420b08030b1|   5|1597092510|-44.0|\n",
      "|00193f17-4e86-476...|5c5b35aed886|5c5b35aed877|   5|1597092481|-74.0|\n",
      "|00226d33-cd80-4b1...|5c5b35502003|5c5b35506d24|  24|1597092473|-61.0|\n",
      "|00272816-8dc2-4f1...|5c5b357f32c7|5c5b35ae6df6|   5|1597092491|-72.0|\n",
      "|00272816-8dc2-4f1...|5c5b357f3308|5c5b35ae6c89|  24|1597092176|-71.0|\n",
      "|00272816-8dc2-4f1...|5c5b35ae6d47|5c5b35ae6c89|   5|1597092472|-68.0|\n",
      "|00272816-8dc2-4f1...|5c5b35ae6d60|5c5b35ae6e46|  24|1597092519|-68.0|\n",
      "|00272816-8dc2-4f1...|5c5b35ae6dab|        null|  24|1597092485|-62.0|\n",
      "|00272816-8dc2-4f1...|5c5b35ae6dd3|5c5b35ae6d79|   5|1597092482|-67.0|\n",
      "|0027af11-eb72-425...|5c5b35bea64f|5c5b35bea528|  24|1597092523|-81.0|\n",
      "|0027af11-eb72-425...|5c5b35beaa2d|5c5b35bea528|  24|1597091884|-44.0|\n",
      "|002a327c-5fe7-465...|5c5b358e92d6|5c5b358e956a|  24|1597092517|-59.0|\n",
      "|002eb842-e20e-48f...|5c5b358e8de5|5c5b358e8df4|  24|1597092526|-68.0|\n",
      "|00458c29-d67b-4cf...|5c5b353e558c|5c5b353e571c|   5|1597092473|-64.0|\n",
      "+--------------------+------------+------------+----+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_scan.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------+------------+------------------+--------------------+------------------+\n",
      "|summary|                site|          ap|         ap2|              band|          time_epoch|              rssi|\n",
      "+-------+--------------------+------------+------------+------------------+--------------------+------------------+\n",
      "|  count|             3555639|     3555639|     3291690|           3555639|             3555639|           3555639|\n",
      "|   mean|                null|        null|        null|16.190116319457626|1.5970923231360433E9|-66.08483763396677|\n",
      "| stddev|                null|        null|        null| 9.348450748950679|   511.4168608160232| 8.199924671641448|\n",
      "|    min|0000063a-a4ce-409...|5c5b350e01c7|5c5b350e0090|                 5|          1597089600|            -112.0|\n",
      "|    max|fffca7ff-4d2f-455...|d420b0c12660|d420b0c12660|                24|          1597092774|             -11.0|\n",
      "+-------+--------------------+------------+------------+------------------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_scan.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3555639"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scan.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_id = \"d1ee1d22-4b55-4c97-97c4-9d757144f45b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scan_site = df_scan.filter(col('site')==site_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scan_site.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+------------+----+----------+-----+\n",
      "|                site|          ap|         ap2|band|time_epoch| rssi|\n",
      "+--------------------+------------+------------+----+----------+-----+\n",
      "|d1ee1d22-4b55-4c9...|5c5b3552b70a|5c5b3552b796|   5|1597092480|-63.0|\n",
      "|d1ee1d22-4b55-4c9...|5c5b3552b94e|5c5b3552b70a|   5|1597092474|-63.0|\n",
      "|d1ee1d22-4b55-4c9...|5c5b3552c2fe|5c5b3552ba70|  24|1597092505|-67.0|\n",
      "|d1ee1d22-4b55-4c9...|5c5b3552b660|5c5b3552b5e3|   5|1597092465|-58.0|\n",
      "|d1ee1d22-4b55-4c9...|5c5b3552b94e|5c5b3552b5e3|  24|1597092474|-46.0|\n",
      "|d1ee1d22-4b55-4c9...|5c5b3552b660|5c5b3552c2bd|  24|1597092465|-53.0|\n",
      "|d1ee1d22-4b55-4c9...|5c5b3552c2bd|5c5b3552bb6f|  24|1597092504|-69.0|\n",
      "|d1ee1d22-4b55-4c9...|5c5b3552b714|5c5b3552b94e|   5|1597092466|-58.0|\n",
      "|d1ee1d22-4b55-4c9...|5c5b3552b94e|5c5b3552c2bd|  24|1597092474|-49.0|\n",
      "|d1ee1d22-4b55-4c9...|5c5b3552b5e3|5c5b3552b7cd|  24|1597092476|-46.0|\n",
      "|d1ee1d22-4b55-4c9...|5c5b3552b7cd|5c5b3552b94e|  24|1597092477|-57.0|\n",
      "|d1ee1d22-4b55-4c9...|5c5b3552ba70|5c5b3552c2bd|  24|1597092467|-40.0|\n",
      "|d1ee1d22-4b55-4c9...|5c5b3552b5e3|5c5b3552b7cd|   5|1597092476|-62.0|\n",
      "|d1ee1d22-4b55-4c9...|5c5b3552b660|5c5b3552b70a|  24|1597092465|-58.0|\n",
      "|d1ee1d22-4b55-4c9...|5c5b3552b70a|5c5b3552b660|   5|1597092480|-58.0|\n",
      "|d1ee1d22-4b55-4c9...|5c5b3552b796|5c5b3552b714|  24|1597092505|-49.0|\n",
      "|d1ee1d22-4b55-4c9...|5c5b3552ba70|5c5b3552b714|   5|1597092467|-68.0|\n",
      "|d1ee1d22-4b55-4c9...|5c5b3552b660|5c5b3552b94e|   5|1597092465|-61.0|\n",
      "|d1ee1d22-4b55-4c9...|5c5b3552c2bd|5c5b3552b5e3|   5|1597092504|-62.0|\n",
      "|d1ee1d22-4b55-4c9...|5c5b3552b660|5c5b3552b714|   5|1597092465|-63.0|\n",
      "+--------------------+------------+------------+----+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_scan_site.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphFrames\n",
    "The following command creates your first GraphFrame. The GraphFrame accepts two DataFrames as inputs—vertices and edges. GraphFrames like to have a naming convention in the column name, which you need to follow. Those rules are defined as follows:\n",
    "* A DataFrame that represents vertices should contain a column named id. Here, personsDf contains a column name \"id\".\n",
    "\n",
    "* A DataFrame that represents edges should contain columns named src and dst. Here, reationshipDf contains the columns \"src\" and \"dst\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices= df_scan_site.selectExpr(\"ap as id\").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges=df_scan_site.selectExpr(\"ap as src\", \"ap2 as dst\", \"rssi as weight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import *\n",
    "g = GraphFrame(vertices, edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|          id|\n",
      "+------------+\n",
      "|5c5b3552c2fe|\n",
      "|5c5b3552b796|\n",
      "|5c5b3552ba70|\n",
      "|d420b000a866|\n",
      "|5c5b3552c2bd|\n",
      "|5c5b3552bb6f|\n",
      "|5c5b3552b70a|\n",
      "|5c5b3552b714|\n",
      "|5c5b3552b94e|\n",
      "|5c5b3552b5e3|\n",
      "|5c5b3552b7cd|\n",
      "|5c5b3552b660|\n",
      "+------------+\n",
      "\n",
      "+------------+------------+------+\n",
      "|         src|         dst|weight|\n",
      "+------------+------------+------+\n",
      "|5c5b3552b70a|5c5b3552b796| -63.0|\n",
      "|5c5b3552b94e|5c5b3552b70a| -63.0|\n",
      "|5c5b3552c2fe|5c5b3552ba70| -67.0|\n",
      "|5c5b3552b660|5c5b3552b5e3| -58.0|\n",
      "|5c5b3552b94e|5c5b3552b5e3| -46.0|\n",
      "|5c5b3552b660|5c5b3552c2bd| -53.0|\n",
      "|5c5b3552c2bd|5c5b3552bb6f| -69.0|\n",
      "|5c5b3552b714|5c5b3552b94e| -58.0|\n",
      "|5c5b3552b94e|5c5b3552c2bd| -49.0|\n",
      "|5c5b3552b5e3|5c5b3552b7cd| -46.0|\n",
      "|5c5b3552b7cd|5c5b3552b94e| -57.0|\n",
      "|5c5b3552ba70|5c5b3552c2bd| -40.0|\n",
      "|5c5b3552b5e3|5c5b3552b7cd| -62.0|\n",
      "|5c5b3552b660|5c5b3552b70a| -58.0|\n",
      "|5c5b3552b70a|5c5b3552b660| -58.0|\n",
      "|5c5b3552b796|5c5b3552b714| -49.0|\n",
      "|5c5b3552ba70|5c5b3552b714| -68.0|\n",
      "|5c5b3552b660|5c5b3552b94e| -61.0|\n",
      "|5c5b3552c2bd|5c5b3552b5e3| -62.0|\n",
      "|5c5b3552b660|5c5b3552b714| -63.0|\n",
      "+------------+------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------------+------+\n",
      "|          id|degree|\n",
      "+------------+------+\n",
      "|5c5b3552c2fe|    19|\n",
      "|5c5b3552b796|    36|\n",
      "|        null|    24|\n",
      "|5c5b3552ba70|    40|\n",
      "|d420b000a866|     9|\n",
      "|5c5b3552c2bd|    37|\n",
      "|5c5b3552bb6f|    21|\n",
      "|5c5b3552b70a|    35|\n",
      "|5c5b3552b714|    34|\n",
      "|5c5b3552b94e|    37|\n",
      "|5c5b3552b5e3|    43|\n",
      "|5c5b3552b7cd|    41|\n",
      "|5c5b3552b660|    40|\n",
      "+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g.vertices.show()\n",
    "g.edges.show()\n",
    "## Check the number of edges of each vertex\n",
    "g.degrees.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphFrame(v:[id: string], e:[src: string, dst: string ... 1 more field])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+\n",
      "|          id|degree|\n",
      "+------------+------+\n",
      "|5c5b3552b5e3|    43|\n",
      "+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The following code gets the number of incoming links to Andrew. This is obtained by calling the inDegrees method.\n",
    "\n",
    "g.degrees.filter(\"id = '5c5b3552b5e3'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+\n",
      "|          id|outDegree|\n",
      "+------------+---------+\n",
      "|5c5b3552b5e3|       21|\n",
      "+------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The following code shows how to get the number of links coming out from Andrew using the outDegrees method .\n",
    "\n",
    "g.outDegrees.filter(\"id = '5c5b3552b5e3'\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setCheckpointDir('graphframes_cps')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = g.pageRank(resetProbability=0.10, maxIter=5)\n",
    "# display(ranks.vertices.select(\"id\",\"pagerank\").orderBy(desc(\"pagerank\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: string, pagerank: double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[src: string, dst: string, weight: double, weight: double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(ranks.vertices)\n",
    "display(ranks.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|          id|\n",
      "+------------+\n",
      "|5c5b3552c2fe|\n",
      "|5c5b3552b796|\n",
      "|5c5b3552ba70|\n",
      "|d420b000a866|\n",
      "|5c5b3552c2bd|\n",
      "|5c5b3552bb6f|\n",
      "|5c5b3552b70a|\n",
      "|5c5b3552b714|\n",
      "|5c5b3552b94e|\n",
      "|5c5b3552b5e3|\n",
      "|5c5b3552b7cd|\n",
      "|5c5b3552b660|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vertices.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o57.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 247.0 failed 1 times, most recent failure: Lost task 4.0 in stage 247.0 (TID 7053, localhost, executor driver): org.apache.hadoop.fs.s3a.AWSS3IOException: getFileStatus on s3://mist-aggregated-stats-production/aggregated-stats/top_1_time_epoch_by_site_ap_ap2_band/dt=2020-08-10/hr=20/part-00000-ba6e840e-a010-4399-8105-4441601be503-c000.csv: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: AF6ABB0E7C037941), S3 Extended Request ID: o1bWIJCf/VnAFWKfrsfHl7T8eI/5zwTxdkc0jycy3NkkK5B9XGWsVe1NkVLpyn22B4m1GfhIPbo=: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: AF6ABB0E7C037941)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:178)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:101)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1568)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:521)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:787)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:85)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:65)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:47)\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.readFile(CSVDataSource.scala:199)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anonfun$buildReader$2.apply(CSVFileFormat.scala:142)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anonfun$buildReader$2.apply(CSVFileFormat.scala:136)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:148)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:132)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: AF6ABB0E7C037941), S3 Extended Request ID: o1bWIJCf/VnAFWKfrsfHl7T8eI/5zwTxdkc0jycy3NkkK5B9XGWsVe1NkVLpyn22B4m1GfhIPbo=\n\tat com.amazonaws.http.AmazonHttpClient.handleErrorResponse(AmazonHttpClient.java:1182)\n\tat com.amazonaws.http.AmazonHttpClient.executeOneRequest(AmazonHttpClient.java:770)\n\tat com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:489)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:310)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3785)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1050)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1027)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:904)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1553)\n\t... 28 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.hadoop.fs.s3a.AWSS3IOException: getFileStatus on s3://mist-aggregated-stats-production/aggregated-stats/top_1_time_epoch_by_site_ap_ap2_band/dt=2020-08-10/hr=20/part-00000-ba6e840e-a010-4399-8105-4441601be503-c000.csv: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: AF6ABB0E7C037941), S3 Extended Request ID: o1bWIJCf/VnAFWKfrsfHl7T8eI/5zwTxdkc0jycy3NkkK5B9XGWsVe1NkVLpyn22B4m1GfhIPbo=: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: AF6ABB0E7C037941)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:178)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:101)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1568)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:521)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:787)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:85)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:65)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:47)\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.readFile(CSVDataSource.scala:199)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anonfun$buildReader$2.apply(CSVFileFormat.scala:142)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anonfun$buildReader$2.apply(CSVFileFormat.scala:136)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:148)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:132)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: AF6ABB0E7C037941), S3 Extended Request ID: o1bWIJCf/VnAFWKfrsfHl7T8eI/5zwTxdkc0jycy3NkkK5B9XGWsVe1NkVLpyn22B4m1GfhIPbo=\n\tat com.amazonaws.http.AmazonHttpClient.handleErrorResponse(AmazonHttpClient.java:1182)\n\tat com.amazonaws.http.AmazonHttpClient.executeOneRequest(AmazonHttpClient.java:770)\n\tat com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:489)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:310)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3785)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1050)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1027)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:904)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1553)\n\t... 28 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-de21e6464c7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvertices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# xap_s\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/mistsys/ds_incubator/venv/spark-2.4.4-bin-without-hadoop/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    378\u001b[0m         \"\"\"\n\u001b[1;32m    379\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/mistsys/ds_incubator/venv/spark-2.4.4-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/mistsys/ds_incubator/venv/spark-2.4.4-bin-without-hadoop/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/mistsys/ds_incubator/venv/spark-2.4.4-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o57.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 247.0 failed 1 times, most recent failure: Lost task 4.0 in stage 247.0 (TID 7053, localhost, executor driver): org.apache.hadoop.fs.s3a.AWSS3IOException: getFileStatus on s3://mist-aggregated-stats-production/aggregated-stats/top_1_time_epoch_by_site_ap_ap2_band/dt=2020-08-10/hr=20/part-00000-ba6e840e-a010-4399-8105-4441601be503-c000.csv: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: AF6ABB0E7C037941), S3 Extended Request ID: o1bWIJCf/VnAFWKfrsfHl7T8eI/5zwTxdkc0jycy3NkkK5B9XGWsVe1NkVLpyn22B4m1GfhIPbo=: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: AF6ABB0E7C037941)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:178)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:101)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1568)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:521)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:787)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:85)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:65)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:47)\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.readFile(CSVDataSource.scala:199)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anonfun$buildReader$2.apply(CSVFileFormat.scala:142)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anonfun$buildReader$2.apply(CSVFileFormat.scala:136)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:148)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:132)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: AF6ABB0E7C037941), S3 Extended Request ID: o1bWIJCf/VnAFWKfrsfHl7T8eI/5zwTxdkc0jycy3NkkK5B9XGWsVe1NkVLpyn22B4m1GfhIPbo=\n\tat com.amazonaws.http.AmazonHttpClient.handleErrorResponse(AmazonHttpClient.java:1182)\n\tat com.amazonaws.http.AmazonHttpClient.executeOneRequest(AmazonHttpClient.java:770)\n\tat com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:489)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:310)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3785)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1050)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1027)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:904)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1553)\n\t... 28 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.hadoop.fs.s3a.AWSS3IOException: getFileStatus on s3://mist-aggregated-stats-production/aggregated-stats/top_1_time_epoch_by_site_ap_ap2_band/dt=2020-08-10/hr=20/part-00000-ba6e840e-a010-4399-8105-4441601be503-c000.csv: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: AF6ABB0E7C037941), S3 Extended Request ID: o1bWIJCf/VnAFWKfrsfHl7T8eI/5zwTxdkc0jycy3NkkK5B9XGWsVe1NkVLpyn22B4m1GfhIPbo=: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: AF6ABB0E7C037941)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:178)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:101)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1568)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:521)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:787)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:85)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:65)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:47)\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.readFile(CSVDataSource.scala:199)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anonfun$buildReader$2.apply(CSVFileFormat.scala:142)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anonfun$buildReader$2.apply(CSVFileFormat.scala:136)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:148)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:132)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithKeys_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: AF6ABB0E7C037941), S3 Extended Request ID: o1bWIJCf/VnAFWKfrsfHl7T8eI/5zwTxdkc0jycy3NkkK5B9XGWsVe1NkVLpyn22B4m1GfhIPbo=\n\tat com.amazonaws.http.AmazonHttpClient.handleErrorResponse(AmazonHttpClient.java:1182)\n\tat com.amazonaws.http.AmazonHttpClient.executeOneRequest(AmazonHttpClient.java:770)\n\tat com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:489)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:310)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3785)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1050)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1027)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:904)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1553)\n\t... 28 more\n"
     ]
    }
   ],
   "source": [
    "# vertices.show()\n",
    "# xap_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search from \"Esther\" for users of age < 32.\n",
    "\n",
    "# ap1 = \"5c5b3552b96c\"\n",
    "# ap2 = \"5c5b3552b5e3\"\n",
    "# paths = g.bfs(\"id = ap1\", \"id = ap2\")\n",
    "# paths.show()\n",
    "\n",
    "# # Specify edge filters or max path lengths.\n",
    "# g.bfs(\"id = ap1\", \"id = ap2\",\\\n",
    "#   edgeFilter=\"rssi > -75\", maxPathLength=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g.connectedComponents()\n",
    "\n",
    "result1 = g.stronglyConnectedComponents(maxIter=10)\n",
    "result1.select(\"id\", \"component\").orderBy(\"component\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = g.stronglyConnectedComponents(maxIter=10)\n",
    "result2.select(\"id\", \"component\").orderBy(\"component\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_day + hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_out_bucket = \"s3://mist-test-bucket/\" #top_1_time_epoch_by_site_ap_ap2_band/{}/{}/\".format(date_day, hr)\n",
    "s3_out_bucket_vertices = s3_out_bucket #+ \"vertices/\"\n",
    "s3_out_bucket_edges = s3_out_bucket #+ \"edges/\"\n",
    "\n",
    "print(s3_out_bucket_vertices, s3_out_bucket_edges)\n",
    "g.vertices.write.parquet(s3_out_bucket_vertices)\n",
    "g.edges.write.parquet(s3_out_bucket_vertices)\n",
    "# s3_out_bucket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3_out_bucket, rrm_local_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "gp = nx.from_pandas_edgelist(edges.toPandas(),'src','dst')\n",
    "nx.draw(gp, with_labels = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph.degrees.filter(\"id = 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GraphFrame provides the following built-in algorithms:\n",
    "# Connected components\n",
    "\n",
    "# Label propagation\n",
    "\n",
    "# PageRank\n",
    "\n",
    "# SVD++\n",
    "\n",
    "# Shortest Path\n",
    "\n",
    "# Strongly connected components\n",
    "\n",
    "# Triangle count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
