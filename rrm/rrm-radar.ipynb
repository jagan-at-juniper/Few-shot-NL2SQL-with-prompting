{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://mist-aggregated-stats-staging/aggregated-stats/rrm_radar_channel_stat/dt=2020-05-*/hr=*/*.csv\n",
      "s3://mist-aggregated-stats-staging/aggregated-stats/top_1_time_epoch_by_site_ap_ap2_band/dt=2020-05-*/hr=*/*.csv\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf, size, avg, count, col,sum, explode\n",
    "import json\n",
    "\n",
    "env = \"production\"\n",
    "env = \"staging\"\n",
    "\n",
    "s3_bucket = \"s3://mist-aggregated-stats-{env}/aggregated-stats/\".format(env=env)\n",
    "date_day = \"2020-05-*\"\n",
    "hr = \"*\" #now().hour-1\n",
    "\n",
    "rrm_radar_path = s3_bucket + \"rrm_radar_channel_stat/dt={day}/hr={hr}/*.csv\".format(env=env, day=date_day, hr=hr)\n",
    "print(rrm_radar_path)\n",
    "\n",
    "\n",
    "date_day = \"2020-05-*\"\n",
    "curr_hr = '*'  # latest \n",
    "ap_neighbors_path = s3_bucket + \"top_1_time_epoch_by_site_ap_ap2_band/dt={day}/hr={hr}/*.csv\".format(env=env, day=date_day, hr=hr)\n",
    "\n",
    "print(ap_neighbors_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['site_id,ap_id,pre_channel,radar_channel_count,last_radar_time,time',\n",
       " '70e0f468-fc13-11e5-85ad-0242ac110008,5c5b350eb3f2,122,1,1588293468,1588291200',\n",
       " 'site_id,ap_id,pre_channel,radar_channel_count,last_radar_time,time',\n",
       " 'a8178443-ecb5-461c-b854-f16627619ab3,5c5b353e4f51,52,1,1588309135,1588305600',\n",
       " 'a8178443-ecb5-461c-b854-f16627619ab3,5c5b353e5393,108,1,1588306596,1588305600',\n",
       " 'site_id,ap_id,pre_channel,radar_channel_count,last_radar_time,time',\n",
       " 'd7c802c4-1d3a-4aa5-90fa-afd14ac7e089,5c5b352e20a8,0,1,1588327560,1588327200',\n",
       " 'site_id,ap_id,pre_channel,radar_channel_count,last_radar_time,time',\n",
       " 'a8178443-ecb5-461c-b854-f16627619ab3,5c5b353e4e84,116,1,1588337189,1588334400',\n",
       " 'site_id,ap_id,pre_channel,radar_channel_count,last_radar_time,time']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rdd_rrm_local = spark.sparkContext.sequenceFile(rrm_local_path)\n",
    "rdd_rrm_events = spark.sparkContext.textFile(rrm_radar_path)\n",
    "rdd_rrm_events.take(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_radar = spark.read.format(\"csv\")\\\n",
    "   .option(\"header\", \"true\")\\\n",
    "   .option(\"inferSchema\", \"true\")\\\n",
    "   .load(rrm_radar_path)\n",
    "df_radar.createOrReplaceTempView(\"scan_data\")\n",
    "df_Schema = df_radar.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-----------+-------------------+---------------+----------+\n",
      "|             site_id|       ap_id|pre_channel|radar_channel_count|last_radar_time|      time|\n",
      "+--------------------+------------+-----------+-------------------+---------------+----------+\n",
      "|c943d077-1b67-491...|5c5b35500622|        104|                  1|     1589096727|1589094000|\n",
      "|c943d077-1b67-491...|5c5b35500924|        116|                  1|     1589095299|1589094000|\n",
      "|c943d077-1b67-491...|5c5b35500686|        104|                  1|     1589095247|1589094000|\n",
      "|c943d077-1b67-491...|5c5b35500587|         64|                  1|     1589096287|1589094000|\n",
      "|c943d077-1b67-491...|5c5b35500839|         64|                  1|     1588919882|1588917600|\n",
      "+--------------------+------------+-----------+-------------------+---------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_radar.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impacted Sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|             site_id|count|\n",
      "+--------------------+-----+\n",
      "|ee915a44-b963-11e...|   11|\n",
      "|f688779c-e335-4f8...|    3|\n",
      "|70e0f468-fc13-11e...|    1|\n",
      "|e80749be-963c-4d2...|    2|\n",
      "|6db15e18-bc94-11e...|    2|\n",
      "|67970e46-4e12-11e...|   12|\n",
      "|fc656275-b157-43f...|    1|\n",
      "|9c3f2be6-397d-11e...|    2|\n",
      "|35670395-458f-4ec...|    2|\n",
      "|d23f418c-a5ab-4e3...|    6|\n",
      "|a8178443-ecb5-461...|   21|\n",
      "|20325077-2d75-4fa...|    2|\n",
      "|96ca2c79-b7db-402...|    1|\n",
      "|d7c802c4-1d3a-4aa...|    9|\n",
      "|12c07c11-51a3-495...|    3|\n",
      "|f733a5d7-5179-4df...|    2|\n",
      "|fe1f336e-fba7-44d...|    1|\n",
      "|d59ee965-e757-45a...|    1|\n",
      "|b0bbfdc0-aaba-11e...|    1|\n",
      "|4a6427ac-4514-4ab...|    1|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#impacted Sites\n",
    "df_radar_sites = df_radar.select('site_id', 'radar_channel_count').groupBy(\"site_id\").count()\n",
    "df_radar_sites.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(site_id='ee915a44-b963-11e5-b14c-1258369c38a9', count=11),\n",
       " Row(site_id='67970e46-4e12-11e6-9188-0242ac110007', count=12),\n",
       " Row(site_id='a8178443-ecb5-461c-b854-f16627619ab3', count=21),\n",
       " Row(site_id='c943d077-1b67-4915-a690-e698543d2790', count=114)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "impacted_sites = df_radar_sites.filter(col('count')>10).select(\"site_id\", \"count\").collect()\n",
    "impacted_sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Impacted-site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "impacted_site_id = impacted_sites[0]['site_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+---------------+\n",
      "|       ap_id|radar_channel|last_radar_time|\n",
      "+------------+-------------+---------------+\n",
      "|5c5b3550032f|          100|     1588928836|\n",
      "|5c5b3550032f|          100|     1588938523|\n",
      "|5c5b3550032f|          100|     1588934289|\n",
      "|5c5b3550032f|          132|     1588948689|\n",
      "|5c5b3550032f|          132|     1589037054|\n",
      "|5c5b3550032f|           52|     1589209332|\n",
      "|5c5b3550032f|           52|     1588952779|\n",
      "|5c5b3550032f|          132|     1588969154|\n",
      "|5c5b3550032f|           52|     1588960942|\n",
      "|5c5b3550032f|          116|     1588963397|\n",
      "|5c5b3550032f|          100|     1589019556|\n",
      "+------------+-------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#impacted APs\n",
    "df_radar_in_site = df_radar.filter(col(\"site_id\")==impacted_site_id).select('ap_id',col(\"pre_channel\").alias(\"radar_channel\"), 'last_radar_time')\n",
    "\n",
    "df_radar_in_site.show()\n",
    "# df_radar_ap_lastseen.show() )\n",
    "\n",
    "# df_radar_ap_lastseen = df_radar_in_site.select('pre_channel', 'last_radar_time').groupBy(\"ap_id\", col(\"pre_channel\").alias(\"radar_channel\"), 'last_radar_time').max()\n",
    "# df_radar_ap_lastseen.show()\n",
    "\n",
    "# df_radar_ap_lastseen = df_radar.filter(col(\"site_id\")==impacted_site_id).select(\"ap_id\", 'pre_channel', 'last_radar_time').groupBy(\"ap_id\", col(\"pre_channel\").alias(\"radar_channel\"), 'last_radar_time').max()\n",
    "# df_radar_ap_lastseen.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+---------+----+-----+\n",
      "|       ap_id|radar_channel|dayofweek|hour|count|\n",
      "+------------+-------------+---------+----+-----+\n",
      "|5c5b3550032f|           52|        0|   8|    1|\n",
      "|5c5b3550032f|          132|        4|  13|    1|\n",
      "|5c5b3550032f|           52|        4|   8|    1|\n",
      "|5c5b3550032f|          100|        4|   3|    1|\n",
      "|5c5b3550032f|          100|        5|   3|    1|\n",
      "|5c5b3550032f|           52|        4|  11|    1|\n",
      "|5c5b3550032f|          116|        4|  11|    1|\n",
      "|5c5b3550032f|          100|        4|   2|    1|\n",
      "|5c5b3550032f|          132|        5|   8|    1|\n",
      "|5c5b3550032f|          100|        4|   4|    1|\n",
      "|5c5b3550032f|          132|        4|   7|    1|\n",
      "+------------+-------------+---------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "timestamp = 1589019556\n",
    "get_hour = udf(lambda t: datetime.fromtimestamp(t).hour, IntegerType())\n",
    "get_weekday = udf(lambda t: datetime.fromtimestamp(t).weekday(), IntegerType())\n",
    "df_radar_in_site= df_radar_in_site.withColumn(\"hour\", get_hour(col(\"last_radar_time\")))\\\n",
    "                                  .withColumn(\"dayofweek\", get_weekday(col(\"last_radar_time\")))\n",
    "\n",
    "df_radar_in_site.select(\"ap_id\", 'radar_channel', \"dayofweek\", \"hour\")\\\n",
    ".groupBy(\"ap_id\", 'radar_channel', \"dayofweek\", \"hour\").count().show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+---------+----+-----+\n",
      "|       ap_id|radar_channel|dayofweek|hour|count|\n",
      "+------------+-------------+---------+----+-----+\n",
      "|5c5b3550032f|           52|        0|   8|    1|\n",
      "|5c5b3550032f|          132|        4|  13|    1|\n",
      "|5c5b3550032f|           52|        4|   8|    1|\n",
      "|5c5b3550032f|          100|        4|   3|    1|\n",
      "|5c5b3550032f|          100|        5|   3|    1|\n",
      "|5c5b3550032f|           52|        4|  11|    1|\n",
      "|5c5b3550032f|          116|        4|  11|    1|\n",
      "|5c5b3550032f|          100|        4|   2|    1|\n",
      "|5c5b3550032f|          132|        5|   8|    1|\n",
      "|5c5b3550032f|          100|        4|   4|    1|\n",
      "|5c5b3550032f|          132|        4|   7|    1|\n",
      "+------------+-------------+---------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_radar_in_site.select(\"ap_id\", 'radar_channel', \"dayofweek\", \"hour\")\\\n",
    ".groupBy(\"ap_id\", 'radar_channel', \"dayofweek\", \"hour\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impacted APs, channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+-----+\n",
      "|       ap_id|radar_channel|count|\n",
      "+------------+-------------+-----+\n",
      "|5c5b3550032f|          116|    1|\n",
      "|5c5b3550032f|           52|    3|\n",
      "|5c5b3550032f|          132|    3|\n",
      "|5c5b3550032f|          100|    4|\n",
      "+------------+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_radar_by_ap = df_radar_in_site.select('ap_id', 'radar_channel').groupBy('ap_id', 'radar_channel').count()\n",
    "df_radar_by_ap.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|radar_channel|max(last_radar_time)|\n",
      "+-------------+--------------------+\n",
      "|          132|          1589037054|\n",
      "|           52|          1589209332|\n",
      "|          100|          1589019556|\n",
      "|          116|          1588963397|\n",
      "+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_radar_by_channel = df_radar_in_site.select('radar_channel', 'last_radar_time').groupBy('radar_channel').agg({'last_radar_time':'max'})\n",
    "df_radar_by_channel.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|hour|count|\n",
      "+----+-----+\n",
      "|  13|    1|\n",
      "|   3|    2|\n",
      "|   4|    1|\n",
      "|   8|    3|\n",
      "|   7|    1|\n",
      "|  11|    2|\n",
      "|   2|    1|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.sql.types import *\n",
    "# time_to_hour = udf(lambda t: int(t)//3600*3600, IntegerType())\n",
    "\n",
    "df_radar_ch_by_hour= df_radar_in_site.select('hour').groupBy('hour').count()\n",
    "df_radar_ch_by_hour.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_radar_ch_by_hour.groupBy(\"last_radar_time\").count().show()\n",
    "\n",
    "# from pyspark.sql.functions  import hour,minute,second\n",
    "# # import org.apache.spark.sql.functions.{col,}\n",
    "# # from pyspark.sql.functions import date_format\n",
    "# # df_radar_in_site.withColumn(\"hour\", hour(col(\"last_radar_time\").cast(TimestampType))).show()\n",
    "# df_radar_in_site.select(col(\"last_radar_time\").cast(TimestampType))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# de-Weight function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "\n",
    "Radar_detected AP1\n",
    "\n",
    "Wt_t1(ap1) = w_t0(ap2)* weight_decay(t1, site_last_seen) + weight(channel, map)\n",
    "UNII-22 channel, weather_radio ,  \n",
    "\n",
    "\n",
    "\n",
    "Neighbors: \n",
    "Wt_t1 (ap2)  = wt_t(ap2)  + weight_From_neighbor (rssi_ap1_ap2)  \n",
    "\n",
    "Functions:\n",
    "\n",
    "weight_decay(t1, last_seen) \n",
    "weight_0 * exp(- t/decay_lifetime),   decay_lifetime = 1 week,\n",
    "\n",
    "weight(channel, map)\n",
    "DFS Channel, \n",
    "Weather channel\n",
    " weight_From_neighbor (rssi) \n",
    "Weight_from_connectivity (SLE anomaly) \n",
    "\n",
    "\n",
    "Goal:  (org, site, ap, channel, weight_0, last_seen) in Redis\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "def weight_by_dfs_channel(chn, wt_0=0.4, wt_weather_ch=0.1 ):\n",
    "    if 116<=chn<132:\n",
    "        wt = wt_0 + wt_weather_ch\n",
    "    else:\n",
    "        wt = wt_0\n",
    "    return wt\n",
    "\n",
    "def weight_by_dist(dist_to_airport=100.0, max_wt=0.8):\n",
    "    wt = 0.0\n",
    "    if dist_to_airport< 10:\n",
    "        wt = max_wt * np.exp(-dist_to_airport**2/5**2)\n",
    "    \n",
    "    return wt\n",
    "\n",
    "def weight_impacted_neighbor(rssi, rssi_0 = -65):\n",
    "    wt = 1.0\n",
    "    if rssi < rssi_0:\n",
    "        wt =  np.exp((rssi - rssi_0)/15) \n",
    "    return wt\n",
    "    \n",
    "    \n",
    "def weight_tot(ch, rssi=-40, dist_to_airport=100.0, theta=[0.33, 0.33, 0.33]):\n",
    "    wt=[0, 0, 0]\n",
    "    wt[0] = weight_by_dfs_channel(ch)\n",
    "    wt[1] = weight_by_dist(dist_to_airport)\n",
    "    wt[2] = weight_impacted_neighbor(rssi)\n",
    "    \n",
    "    wt_0 = 0.0\n",
    "    for i in range(2):\n",
    "        wt_0 += wt[i] * theta[i]\n",
    "    \n",
    "#     print(wt_0, wt)\n",
    "#     wt_0 = 1.0/(1.0 + np.exp(-0.6*wt_0))\n",
    "    wt_0 = (wt[0] + wt[1]) * wt[2]\n",
    "\n",
    "    return wt_0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-85 0.10543885524629071\n",
      "-80 0.14715177646857694\n",
      "-75 0.2053668476130368\n",
      "-70 0.2866125242295157\n",
      "-65 0.4\n",
      "-60 0.4\n",
      "-55 0.4\n"
     ]
    }
   ],
   "source": [
    "ch = 40\n",
    "d = 100\n",
    "for r in range(-85, -50, 5):\n",
    "    print(r, weight_tot(ch, r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 meter 0.8\n",
      "1 meter 0.7686315513218586\n",
      "2 meter 0.6817150311729692\n",
      "3 meter 0.5581410608568248\n",
      "4 meter 0.42183393923443885\n",
      "5 meter 0.2943035529371539\n",
      "6 meter 0.18954220694569743\n",
      "7 meter 0.112686736736836\n",
      "8 meter 0.0618437923546398\n",
      "9 meter 0.031331116079189654\n",
      "10 meter 0.0\n",
      "11 meter 0.0\n",
      "12 meter 0.0\n",
      "13 meter 0.0\n",
      "14 meter 0.0\n",
      "15 meter 0.0\n",
      "16 meter 0.0\n",
      "17 meter 0.0\n",
      "18 meter 0.0\n",
      "19 meter 0.0\n"
     ]
    }
   ],
   "source": [
    "for m in range(0, 20):\n",
    "    print(m, \"meter\",  weight_by_dist(m))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-40 db 1.0\n",
      "-45 db 1.0\n",
      "-50 db 1.0\n",
      "-55 db 0.7165313105737893\n",
      "-60 db 0.513417119032592\n",
      "-65 db 0.36787944117144233\n",
      "-70 db 0.26359713811572677\n",
      "-75 db 0.18887560283756183\n"
     ]
    }
   ],
   "source": [
    "for m in range(-40, -80, -5):\n",
    "    print(m, \"db\",  weight_impacted_neighbor(m, -50))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_radar_in_site = df_radar.filter(col(\"site_id\")==impacted_site_id)\n",
    "# df_radar_in_site.groupBy(\"ap_id\", col(\"pre_channel\").alias(\"radar_channel\")).agg({'radar_channel':'count', 'last_radar_time':'max'})\n",
    "# df_radar_ap_in_site.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nImpacting on weight and  (Org, site, ap1, ap2, rssi ) () s3://mist-aggregated-stats-production/aggregated-stats/top_1_time_epoch_by_site_ap_ap2_band/\\nRadar_detected AP\\nWt_t1(ap0) = w_t0(ap0)* weight_decay(t1, last_seen) + weight(channel, map)\\nNeighbors: \\nWt_t1 (ap1)  = wt_t(ap1)  + weight_From_neighbor (rssi)  \\nFunctions:\\nweight_decay(t1, last_seen) \\nweight_0 * exp(- t/decay_lifetime),   decay_lifetime = 1 week,\\nweight(channel, map)\\nDFS Channel, \\nWeather channel\\n weight_From_neighbor (rssi) \\nWeight_from_connectivity (SLE anomaly) \\nGoal:  (org, site, ap, channel, weight_0, last_seen) in Redis\\n\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Impacting on weight and  (Org, site, ap1, ap2, rssi ) () s3://mist-aggregated-stats-production/aggregated-stats/top_1_time_epoch_by_site_ap_ap2_band/\n",
    "Radar_detected AP\n",
    "Wt_t1(ap0) = w_t0(ap0)* weight_decay(t1, last_seen) + weight(channel, map)\n",
    "Neighbors: \n",
    "Wt_t1 (ap1)  = wt_t(ap1)  + weight_From_neighbor (rssi)  \n",
    "Functions:\n",
    "weight_decay(t1, last_seen) \n",
    "weight_0 * exp(- t/decay_lifetime),   decay_lifetime = 1 week,\n",
    "weight(channel, map)\n",
    "DFS Channel, \n",
    "Weather channel\n",
    " weight_From_neighbor (rssi) \n",
    "Weight_from_connectivity (SLE anomaly) \n",
    "Goal:  (org, site, ap, channel, weight_0, last_seen) in Redis\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+---------------+----+---------+\n",
      "|       ap_id|radar_channel|last_radar_time|hour|dayofweek|\n",
      "+------------+-------------+---------------+----+---------+\n",
      "|5c5b3550032f|          100|     1588928836|   2|        4|\n",
      "|5c5b3550032f|          100|     1588938523|   4|        4|\n",
      "|5c5b3550032f|          100|     1588934289|   3|        4|\n",
      "|5c5b3550032f|          132|     1588948689|   7|        4|\n",
      "|5c5b3550032f|          132|     1589037054|   8|        5|\n",
      "|5c5b3550032f|           52|     1589209332|   8|        0|\n",
      "|5c5b3550032f|           52|     1588952779|   8|        4|\n",
      "|5c5b3550032f|          132|     1588969154|  13|        4|\n",
      "|5c5b3550032f|           52|     1588960942|  11|        4|\n",
      "|5c5b3550032f|          116|     1588963397|  11|        4|\n",
      "|5c5b3550032f|          100|     1589019556|   3|        5|\n",
      "+------------+-------------+---------------+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_radar_in_site.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+-----------+\n",
      "|       ap_id|radar_channel|max(weight)|\n",
      "+------------+-------------+-----------+\n",
      "|5c5b3550032f|          116|        0.8|\n",
      "|5c5b3550032f|           52|        0.6|\n",
      "|5c5b3550032f|          132|        0.6|\n",
      "|5c5b3550032f|          100|        0.6|\n",
      "+------------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def weight_by_dfs_channel(chn, wt_0=0.4, wt_weather_ch=0.1 ):\n",
    "    if 116<=chn<132:\n",
    "        wt = wt_0 + wt_weather_ch\n",
    "    else:\n",
    "        wt = wt_0\n",
    "    return wt\n",
    "\n",
    "weight_channel = udf(lambda ch: weight_by_dfs_channel(ch, 0.6, 0.2))\n",
    "df_radar_in_site_weight = df_radar_in_site.withColumn(\"weight\", weight_channel(col('radar_channel')))\n",
    "df_radar_in_site_weight= df_radar_in_site_weight.select(\"ap_id\", \"radar_channel\", \"weight\").groupBy(\"ap_id\", \"radar_channel\").agg({\"weight\":'max'})\n",
    "df_radar_in_site_weight.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impacts on neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['site,ap,ap2,band,time_epoch,rssi',\n",
       " '1916d52a-4a90-11e5-8b45-1258369c38a9,5c5b350e6064,5c5b350e06e0,24,1588294658,-61.0',\n",
       " '63b31b9a-e5c4-4613-8bd9-47ae12809b27,5c5b350ebe93,5c5b353e4e75,5,1588291686,-46.0']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rdd_ap_neigbhors = spark.sparkContext.textFile(ap_neighbors_path)\n",
    "# rdd_ap_neigbhors.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o270.load.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 100.0 failed 1 times, most recent failure: Lost task 2.0 in stage 100.0 (TID 4621, localhost, executor driver): org.apache.hadoop.fs.s3a.AWSS3IOException: getFileStatus on s3://mist-aggregated-stats-staging/aggregated-stats/top_1_time_epoch_by_site_ap_ap2_band/dt=2020-05-01/hr=05/part-00000-057c5a1d-c34e-4844-a33a-69b175b1a3a8-c000.csv: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: E4C6C3E478865076), S3 Extended Request ID: ZPDGKgvaEqND+Y5a/bHInqpr5pl3q+lBRt+buLzEpugGFSEniYLrL9EHu0M/U6UzkAK4p8tH8jE=: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: E4C6C3E478865076)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:178)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:101)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1568)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:521)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:787)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:85)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:65)\n\tat org.apache.spark.sql.execution.datasources.text.TextFileFormat$$anonfun$readToUnsafeMem$1.apply(TextFileFormat.scala:119)\n\tat org.apache.spark.sql.execution.datasources.text.TextFileFormat$$anonfun$readToUnsafeMem$1.apply(TextFileFormat.scala:116)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:148)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:132)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1334)\n\tat org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1122)\n\tat org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1122)\n\tat org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157)\n\tat org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: E4C6C3E478865076), S3 Extended Request ID: ZPDGKgvaEqND+Y5a/bHInqpr5pl3q+lBRt+buLzEpugGFSEniYLrL9EHu0M/U6UzkAK4p8tH8jE=\n\tat com.amazonaws.http.AmazonHttpClient.handleErrorResponse(AmazonHttpClient.java:1182)\n\tat com.amazonaws.http.AmazonHttpClient.executeOneRequest(AmazonHttpClient.java:770)\n\tat com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:489)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:310)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3785)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1050)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1027)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:904)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1553)\n\t... 37 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n\tat org.apache.spark.rdd.RDD$$anonfun$aggregate$1.apply(RDD.scala:1124)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1117)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$.infer(CSVInferSchema.scala:44)\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.inferFromDataset(CSVDataSource.scala:261)\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:237)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:68)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:63)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$6.apply(DataSource.scala:180)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$6.apply(DataSource.scala:180)\n\tat scala.Option.orElse(Option.scala:289)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:179)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:373)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.hadoop.fs.s3a.AWSS3IOException: getFileStatus on s3://mist-aggregated-stats-staging/aggregated-stats/top_1_time_epoch_by_site_ap_ap2_band/dt=2020-05-01/hr=05/part-00000-057c5a1d-c34e-4844-a33a-69b175b1a3a8-c000.csv: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: E4C6C3E478865076), S3 Extended Request ID: ZPDGKgvaEqND+Y5a/bHInqpr5pl3q+lBRt+buLzEpugGFSEniYLrL9EHu0M/U6UzkAK4p8tH8jE=: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: E4C6C3E478865076)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:178)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:101)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1568)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:521)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:787)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:85)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:65)\n\tat org.apache.spark.sql.execution.datasources.text.TextFileFormat$$anonfun$readToUnsafeMem$1.apply(TextFileFormat.scala:119)\n\tat org.apache.spark.sql.execution.datasources.text.TextFileFormat$$anonfun$readToUnsafeMem$1.apply(TextFileFormat.scala:116)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:148)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:132)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1334)\n\tat org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1122)\n\tat org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1122)\n\tat org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157)\n\tat org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: E4C6C3E478865076), S3 Extended Request ID: ZPDGKgvaEqND+Y5a/bHInqpr5pl3q+lBRt+buLzEpugGFSEniYLrL9EHu0M/U6UzkAK4p8tH8jE=\n\tat com.amazonaws.http.AmazonHttpClient.handleErrorResponse(AmazonHttpClient.java:1182)\n\tat com.amazonaws.http.AmazonHttpClient.executeOneRequest(AmazonHttpClient.java:770)\n\tat com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:489)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:310)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3785)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1050)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1027)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:904)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1553)\n\t... 37 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-f533baec0d09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m    \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"inferSchema\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m    \u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0map_neighbors_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mdf_scan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateOrReplaceTempView\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"scan_data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf_Schema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_scan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/mistsys/ds_incubator/venv/spark-2.4.4-bin-without-hadoop/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/mistsys/ds_incubator/venv/spark-2.4.4-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/mistsys/ds_incubator/venv/spark-2.4.4-bin-without-hadoop/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/mistsys/ds_incubator/venv/spark-2.4.4-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o270.load.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 100.0 failed 1 times, most recent failure: Lost task 2.0 in stage 100.0 (TID 4621, localhost, executor driver): org.apache.hadoop.fs.s3a.AWSS3IOException: getFileStatus on s3://mist-aggregated-stats-staging/aggregated-stats/top_1_time_epoch_by_site_ap_ap2_band/dt=2020-05-01/hr=05/part-00000-057c5a1d-c34e-4844-a33a-69b175b1a3a8-c000.csv: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: E4C6C3E478865076), S3 Extended Request ID: ZPDGKgvaEqND+Y5a/bHInqpr5pl3q+lBRt+buLzEpugGFSEniYLrL9EHu0M/U6UzkAK4p8tH8jE=: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: E4C6C3E478865076)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:178)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:101)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1568)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:521)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:787)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:85)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:65)\n\tat org.apache.spark.sql.execution.datasources.text.TextFileFormat$$anonfun$readToUnsafeMem$1.apply(TextFileFormat.scala:119)\n\tat org.apache.spark.sql.execution.datasources.text.TextFileFormat$$anonfun$readToUnsafeMem$1.apply(TextFileFormat.scala:116)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:148)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:132)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1334)\n\tat org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1122)\n\tat org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1122)\n\tat org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157)\n\tat org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: E4C6C3E478865076), S3 Extended Request ID: ZPDGKgvaEqND+Y5a/bHInqpr5pl3q+lBRt+buLzEpugGFSEniYLrL9EHu0M/U6UzkAK4p8tH8jE=\n\tat com.amazonaws.http.AmazonHttpClient.handleErrorResponse(AmazonHttpClient.java:1182)\n\tat com.amazonaws.http.AmazonHttpClient.executeOneRequest(AmazonHttpClient.java:770)\n\tat com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:489)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:310)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3785)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1050)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1027)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:904)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1553)\n\t... 37 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2158)\n\tat org.apache.spark.rdd.RDD$$anonfun$aggregate$1.apply(RDD.scala:1124)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.aggregate(RDD.scala:1117)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVInferSchema$.infer(CSVInferSchema.scala:44)\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.inferFromDataset(CSVDataSource.scala:261)\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:237)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:68)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:63)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$6.apply(DataSource.scala:180)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$6.apply(DataSource.scala:180)\n\tat scala.Option.orElse(Option.scala:289)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:179)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:373)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:223)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:178)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.hadoop.fs.s3a.AWSS3IOException: getFileStatus on s3://mist-aggregated-stats-staging/aggregated-stats/top_1_time_epoch_by_site_ap_ap2_band/dt=2020-05-01/hr=05/part-00000-057c5a1d-c34e-4844-a33a-69b175b1a3a8-c000.csv: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: E4C6C3E478865076), S3 Extended Request ID: ZPDGKgvaEqND+Y5a/bHInqpr5pl3q+lBRt+buLzEpugGFSEniYLrL9EHu0M/U6UzkAK4p8tH8jE=: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: E4C6C3E478865076)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:178)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:101)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1568)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:521)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:787)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:85)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:65)\n\tat org.apache.spark.sql.execution.datasources.text.TextFileFormat$$anonfun$readToUnsafeMem$1.apply(TextFileFormat.scala:119)\n\tat org.apache.spark.sql.execution.datasources.text.TextFileFormat$$anonfun$readToUnsafeMem$1.apply(TextFileFormat.scala:116)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:148)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:132)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)\n\tat scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1334)\n\tat scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214)\n\tat scala.collection.AbstractIterator.aggregate(Iterator.scala:1334)\n\tat org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1122)\n\tat org.apache.spark.rdd.RDD$$anonfun$aggregate$1$$anonfun$22.apply(RDD.scala:1122)\n\tat org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157)\n\tat org.apache.spark.SparkContext$$anonfun$36.apply(SparkContext.scala:2157)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: E4C6C3E478865076), S3 Extended Request ID: ZPDGKgvaEqND+Y5a/bHInqpr5pl3q+lBRt+buLzEpugGFSEniYLrL9EHu0M/U6UzkAK4p8tH8jE=\n\tat com.amazonaws.http.AmazonHttpClient.handleErrorResponse(AmazonHttpClient.java:1182)\n\tat com.amazonaws.http.AmazonHttpClient.executeOneRequest(AmazonHttpClient.java:770)\n\tat com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:489)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:310)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3785)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1050)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1027)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:904)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1553)\n\t... 37 more\n"
     ]
    }
   ],
   "source": [
    "df_scan = spark.read.format(\"csv\")\\\n",
    "   .option(\"header\", \"true\")\\\n",
    "   .option(\"inferSchema\", \"true\")\\\n",
    "   .load(ap_neighbors_path)\n",
    "df_scan.createOrReplaceTempView(\"scan_data\")\n",
    "df_Schema = df_scan.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scan.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scan_in_site = df_scan.filter(col(\"site\")==impacted_site_id)\n",
    "df_scan_in_site.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "impact_aps = df_radar_in_site.select(\"ap_id\").distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impacted_ap_id = impact_aps[0]['ap_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scan_neighbor = df_scan_in_site.filter((col(\"ap\")==impacted_ap_id)&(col('rssi')>-75)&(col(\"ap2\")!=''))\\\n",
    "                                .select('ap2', 'rssi').groupBy(\"ap2\").agg({\"rssi\": 'max'})\n",
    "\n",
    "df_scan_neighbor.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_scan_neighbor.select('ap2', 'rssi').groupBy(\"ap2\").agg({\"rssi\": 'max'}).show()\n",
    "df_scan_neighbor = df_scan_neighbor.select(col(\"ap2\").alias(\"ap\"), col(\"max(rssi)\").alias(\"max_rssi\"))\n",
    "df_scan_neighbor.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weight_neighbor = udf(lambda r: weight_impacted_neighbor(r, -65))\n",
    "df_scan_neighbor_weight = df_scan_neighbor.withColumn(\"weight\", weight_channel(col('max_rssi')))\n",
    "\n",
    "df_scan_neighbor_weight.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
