{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start with graphframe \n",
    "\n",
    "$ find venv/ -name *graphx*.jar\n",
    "\n",
    "  venv//spark-2.4.4-bin-without-hadoop/jars/spark-graphx_2.11-2.4.4.jar\n",
    "\n",
    "$ ./dev-scripts/okta_spark jupyter-notebook --packages graphframes:graphframes:0.8.0-spark2.4-s_2.11\n",
    "\n",
    "\n",
    "\n",
    "## data sources:\n",
    "* scan \n",
    "* coverage-anommaly\n",
    "* sticky-clients\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://mist-aggregated-stats-production/aggregated-stats/top_1_time_epoch_by_site_ap_ap2_band/dt=2020-08-21/hr=*/*.csv\n",
      "s3://mist-secorapp-production/sle-coverage-anomaly/sle-coverage-anomaly-production/dt=2020-08-21/hr=*/*.seq\n",
      "s3://mist-secorapp-production/sticky-client/sticky-client-production/dt=2020-08-21/hr=*/*.seq\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf, size, avg, count, col,sum, explode\n",
    "import json\n",
    "\n",
    "env = \"production\"\n",
    "#s3://mist-aggregated-stats-production/aggregated-stats/top_1_time_epoch_by_site_ap_ap2_band/dt=2020-03-14/hr=16/part-00000-76298f84-5414-49c5-88f6-a981ff06d94d-c000.csv .\n",
    "\n",
    "s3_bucket = \"s3://mist-aggregated-stats-{env}/aggregated-stats/\".format(env=env)\n",
    "date_day = \"2020-08-21\"\n",
    "hr = '*'\n",
    "\n",
    "ap_neighbors_path = \"top_1_time_epoch_by_site_ap_ap2_band/dt={day}/hr={hr}/*.csv\".format(env=env, day=date_day, hr=hr)\n",
    "\n",
    "ap_neighbors_path = s3_bucket + ap_neighbors_path\n",
    "print(ap_neighbors_path)\n",
    "\n",
    "\n",
    "\n",
    "# date_day = \"2020-08-11\"\n",
    "# hr = '*'\n",
    "\n",
    "s3_coverage_bucket = \"s3://mist-secorapp-{env}/sle-coverage-anomaly/sle-coverage-anomaly-{env}/\".format(env=env)\n",
    "s3_coverage_path = s3_coverage_bucket + \"dt={day}/hr={hr}/*.seq\".format(day=date_day, hr=hr)\n",
    "print(s3_coverage_path)\n",
    "\n",
    "\n",
    "s3_sticky_bucket = \"s3://mist-secorapp-{env}/sticky-client/sticky-client-{env}/\".format(env=env)\n",
    "s3_sticky_path = s3_sticky_bucket + \"dt={day}/hr={hr}/*.seq\".format(day=date_day, hr=hr)\n",
    "print(s3_sticky_path)\n",
    "\n",
    "\n",
    "# user_org_id = \"38a18d4d-1623-4985-86d0-1bb06e5e2a48\"  # UPS\n",
    "# user_site_USNYELE = '9aa6ecdb-ddee-41b4-a8d9-872d7962b3c1'  #UPS USNYELE\n",
    "\n",
    "# # user_org_id = '38a18d4d-1623-4985-86d0-1bb06e5e2a48'  # UPS\n",
    "# user_site_USFLLON = 'c23667f3-47e9-44aa-9761-de49d34ed9f9' # USFLLON\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  coverage-anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_coverage_t = sqlContext.read.format('sequencefile').load(s3_coverage_path)\n",
    "# df_coverage_t.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.sequenceFile.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.io.IOException: Failed to connect to ip-192-168-1-57.ec2.internal:62800\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:245)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:187)\n\tat org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$downloadClient(NettyRpcEnv.scala:368)\n\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$openChannel$1.apply$mcV$sp(NettyRpcEnv.scala:336)\n\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$openChannel$1.apply(NettyRpcEnv.scala:335)\n\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$openChannel$1.apply(NettyRpcEnv.scala:335)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)\n\tat org.apache.spark.rpc.netty.NettyRpcEnv.openChannel(NettyRpcEnv.scala:339)\n\tat org.apache.spark.util.Utils$.doFetchFile(Utils.scala:693)\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:509)\n\tat org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:811)\n\tat org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:803)\n\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)\n\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)\n\tat scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:130)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n\tat org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:803)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:375)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\tSuppressed: java.lang.NullPointerException\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1402)\n\t\t... 17 more\nCaused by: java.net.UnknownHostException: ip-192-168-1-57.ec2.internal\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1281)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1193)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1127)\n\tat java.net.InetAddress.getByName(InetAddress.java:1077)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:146)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:143)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:143)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:43)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:63)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:57)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:32)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:108)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:208)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:49)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:188)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:174)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:507)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:481)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:420)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:82)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:978)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:512)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:423)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:482)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\n\t... 1 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1364)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1337)\n\tat org.apache.spark.api.python.SerDeUtil$.pairRDDToPython(SerDeUtil.scala:239)\n\tat org.apache.spark.api.python.PythonRDD$.sequenceFile(PythonRDD.scala:250)\n\tat org.apache.spark.api.python.PythonRDD.sequenceFile(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: Failed to connect to ip-192-168-1-57.ec2.internal:62800\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:245)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:187)\n\tat org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$downloadClient(NettyRpcEnv.scala:368)\n\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$openChannel$1.apply$mcV$sp(NettyRpcEnv.scala:336)\n\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$openChannel$1.apply(NettyRpcEnv.scala:335)\n\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$openChannel$1.apply(NettyRpcEnv.scala:335)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)\n\tat org.apache.spark.rpc.netty.NettyRpcEnv.openChannel(NettyRpcEnv.scala:339)\n\tat org.apache.spark.util.Utils$.doFetchFile(Utils.scala:693)\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:509)\n\tat org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:811)\n\tat org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:803)\n\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)\n\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)\n\tat scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:130)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n\tat org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:803)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:375)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\tSuppressed: java.lang.NullPointerException\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1402)\n\t\t... 17 more\nCaused by: java.net.UnknownHostException: ip-192-168-1-57.ec2.internal\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1281)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1193)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1127)\n\tat java.net.InetAddress.getByName(InetAddress.java:1077)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:146)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:143)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:143)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:43)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:63)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:57)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:32)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:108)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:208)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:49)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:188)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:174)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:507)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:481)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:420)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:82)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:978)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:512)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:423)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:482)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-bc11a7c37f87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Coverage-anomaly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrdd_coverage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequenceFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms3_coverage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# rdd_coverage.first()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_coverage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd_coverage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#.map(lambda x: json.loads(x[1])).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/mistsys/ds_incubator/venv/spark-2.4.4-bin-without-hadoop/python/pyspark/context.py\u001b[0m in \u001b[0;36msequenceFile\u001b[0;34m(self, path, keyClass, valueClass, keyConverter, valueConverter, minSplits, batchSize)\u001b[0m\n\u001b[1;32m    710\u001b[0m         \u001b[0mminSplits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mminSplits\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultParallelism\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m         jrdd = self._jvm.PythonRDD.sequenceFile(self._jsc, path, keyClass, valueClass,\n\u001b[0;32m--> 712\u001b[0;31m                                                 keyConverter, valueConverter, minSplits, batchSize)\n\u001b[0m\u001b[1;32m    713\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/mistsys/ds_incubator/venv/spark-2.4.4-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/mistsys/ds_incubator/venv/spark-2.4.4-bin-without-hadoop/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/mistsys/ds_incubator/venv/spark-2.4.4-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.sequenceFile.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.io.IOException: Failed to connect to ip-192-168-1-57.ec2.internal:62800\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:245)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:187)\n\tat org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$downloadClient(NettyRpcEnv.scala:368)\n\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$openChannel$1.apply$mcV$sp(NettyRpcEnv.scala:336)\n\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$openChannel$1.apply(NettyRpcEnv.scala:335)\n\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$openChannel$1.apply(NettyRpcEnv.scala:335)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)\n\tat org.apache.spark.rpc.netty.NettyRpcEnv.openChannel(NettyRpcEnv.scala:339)\n\tat org.apache.spark.util.Utils$.doFetchFile(Utils.scala:693)\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:509)\n\tat org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:811)\n\tat org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:803)\n\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)\n\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)\n\tat scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:130)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n\tat org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:803)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:375)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\tSuppressed: java.lang.NullPointerException\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1402)\n\t\t... 17 more\nCaused by: java.net.UnknownHostException: ip-192-168-1-57.ec2.internal\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1281)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1193)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1127)\n\tat java.net.InetAddress.getByName(InetAddress.java:1077)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:146)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:143)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:143)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:43)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:63)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:57)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:32)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:108)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:208)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:49)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:188)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:174)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:507)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:481)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:420)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:82)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:978)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:512)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:423)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:482)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\n\t... 1 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1364)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1337)\n\tat org.apache.spark.api.python.SerDeUtil$.pairRDDToPython(SerDeUtil.scala:239)\n\tat org.apache.spark.api.python.PythonRDD$.sequenceFile(PythonRDD.scala:250)\n\tat org.apache.spark.api.python.PythonRDD.sequenceFile(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.io.IOException: Failed to connect to ip-192-168-1-57.ec2.internal:62800\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:245)\n\tat org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:187)\n\tat org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$downloadClient(NettyRpcEnv.scala:368)\n\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$openChannel$1.apply$mcV$sp(NettyRpcEnv.scala:336)\n\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$openChannel$1.apply(NettyRpcEnv.scala:335)\n\tat org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$openChannel$1.apply(NettyRpcEnv.scala:335)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)\n\tat org.apache.spark.rpc.netty.NettyRpcEnv.openChannel(NettyRpcEnv.scala:339)\n\tat org.apache.spark.util.Utils$.doFetchFile(Utils.scala:693)\n\tat org.apache.spark.util.Utils$.fetchFile(Utils.scala:509)\n\tat org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:811)\n\tat org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:803)\n\tat scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733)\n\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)\n\tat scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)\n\tat scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)\n\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)\n\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:130)\n\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732)\n\tat org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:803)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:375)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n\tSuppressed: java.lang.NullPointerException\n\t\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1402)\n\t\t... 17 more\nCaused by: java.net.UnknownHostException: ip-192-168-1-57.ec2.internal\n\tat java.net.InetAddress.getAllByName0(InetAddress.java:1281)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1193)\n\tat java.net.InetAddress.getAllByName(InetAddress.java:1127)\n\tat java.net.InetAddress.getByName(InetAddress.java:1077)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:146)\n\tat io.netty.util.internal.SocketUtils$8.run(SocketUtils.java:143)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat io.netty.util.internal.SocketUtils.addressByName(SocketUtils.java:143)\n\tat io.netty.resolver.DefaultNameResolver.doResolve(DefaultNameResolver.java:43)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:63)\n\tat io.netty.resolver.SimpleNameResolver.resolve(SimpleNameResolver.java:55)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:57)\n\tat io.netty.resolver.InetSocketAddressResolver.doResolve(InetSocketAddressResolver.java:32)\n\tat io.netty.resolver.AbstractAddressResolver.resolve(AbstractAddressResolver.java:108)\n\tat io.netty.bootstrap.Bootstrap.doResolveAndConnect0(Bootstrap.java:208)\n\tat io.netty.bootstrap.Bootstrap.access$000(Bootstrap.java:49)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:188)\n\tat io.netty.bootstrap.Bootstrap$1.operationComplete(Bootstrap.java:174)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListener0(DefaultPromise.java:507)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListenersNow(DefaultPromise.java:481)\n\tat io.netty.util.concurrent.DefaultPromise.notifyListeners(DefaultPromise.java:420)\n\tat io.netty.util.concurrent.DefaultPromise.trySuccess(DefaultPromise.java:104)\n\tat io.netty.channel.DefaultChannelPromise.trySuccess(DefaultChannelPromise.java:82)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.safeSetSuccess(AbstractChannel.java:978)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.register0(AbstractChannel.java:512)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe.access$200(AbstractChannel.java:423)\n\tat io.netty.channel.AbstractChannel$AbstractUnsafe$1.run(AbstractChannel.java:482)\n\tat io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:403)\n\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:463)\n\tat io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858)\n\tat io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# Coverage-anomaly \n",
    "rdd_coverage = spark.sparkContext.sequenceFile(s3_coverage_path)\n",
    "# rdd_coverage.first()\n",
    "\n",
    "df_coverage = rdd_coverage.map(lambda x: json.loads(x[1])).toDF() #.map(lambda x: json.loads(x[1])).\n",
    "\n",
    "df_coverage_0 = df_coverage.filter(col(\"band\")==\"5\")\\\n",
    "    .select(\"org\", \"site\", \"ap\", \"anomaly_type\", \"band\", \"error_rate\", \n",
    "            \"rssi_mean\", \"rssi_mean_base\", \"rssi_deviation\", \"sle_coverage\", \"sle_coverage_base\", \"sle_coverage_anomaly_score\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coverage_0.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df_coverage = spark.read.format(\"sequenceFile\")\\\n",
    "#    .option(\"header\", \"true\")\\\n",
    "#    .option(\"inferSchema\", \"true\")\\\n",
    "#    .load(s3_coverage_path)\n",
    "# df_coverage.createOrReplaceTempView(\"sequenceFile\")\n",
    "# df_Schema = df_scan.schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# rrm-graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd_ap_neigbhors = spark.sparkContext.textFile(ap_neighbors_path)\n",
    "# rdd_ap_neigbhors.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd_rrm_local.map(lambda line: (line.split(',')[0], line.split(',')[1])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_scan = spark.read.format(\"csv\")\\\n",
    "   .option(\"header\", \"true\")\\\n",
    "   .option(\"inferSchema\", \"true\")\\\n",
    "   .load(ap_neighbors_path)\n",
    "df_scan.createOrReplaceTempView(\"scan_data\")\n",
    "df_Schema = df_scan.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scan.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scan.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scan.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test-site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_id = \"5e8fe474-a9ee-4d01-a2b6-b022b0f9c869\"  # GEG1 , AmazonOTFC-prod\n",
    "\n",
    "site_id = \"a7092875-257f-43f3-9514-ca1ab688bec0\" # Sam's club. 4989"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# site_id = \"d1ee1d22-4b55-4c97-97c4-9d757144f45b\"\n",
    "df_coverage_site = df_coverage_0.filter(col(\"site\")==site_id)\n",
    "print(\"count\", df_coverage_site.count() )\n",
    "df_coverage_site.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, FloatType, StringType\n",
    "\n",
    "def mac_format(mac):\n",
    "    return mac.replace(\"-\", \"\")\n",
    "mac_format = udf(mac_format, StringType())\n",
    "\n",
    "df_coverage_site= df_coverage_site.withColumn(\"ap\", mac_format(col(\"ap\")))\n",
    "df_coverage_site.show(3)                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scan_site = df_scan.filter(col('site')==site_id)\n",
    "\n",
    "print(\"count\", df_scan_site.count())\n",
    "\n",
    "df_scan_site.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_scan_site.select(\"ap\").groupBy(\"ap\").count().show()\n",
    "\n",
    "df_scan_site.select(F.countDistinct(\"ap\")).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scan_site.select(F.countDistinct(\"ap2\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scan_site.select(F.countDistinct(\"ap\", \"ap2\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphFrames\n",
    "The following command creates your first GraphFrame. The GraphFrame accepts two DataFrames as inputsâ€”vertices and edges. GraphFrames like to have a naming convention in the column name, which you need to follow. Those rules are defined as follows:\n",
    "* A DataFrame that represents vertices should contain a column named id. Here, personsDf contains a column name \"id\".\n",
    "\n",
    "* A DataFrame that represents edges should contain columns named src and dst. Here, reationshipDf contains the columns \"src\" and \"dst\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scan_site.show(3)\n",
    "df_coverage_site.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scan_site.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "df_scan_site_2 = df_scan_site.join(df_coverage_site, [df_scan_site.ap==df_coverage_site.ap], \"inner\")\n",
    "df_scan_site_2.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices= df_scan_site.selectExpr(\"ap as id\").distinct()\n",
    "vertices.show(3)\n",
    "\n",
    "vertices_2 = vertices.join(df_coverage_site, [vertices.id==df_coverage_site.ap], \"inner\")\n",
    "vertices_2.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices.count(), vertices_2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices_3 = vertices.join(df_coverage_site, [vertices.id==df_coverage_site.ap], how='left')\n",
    "vertices_3.show(3)\n",
    "vertices_3.count()\n",
    "vertices_3.select(\"id\", \"sle_coverage_anomaly_score\").show(41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices_4 = vertices.join(df_coverage_site, [vertices.id==df_coverage_site.ap], how='right')\n",
    "# vertices_4.show(3)\n",
    "vertices_4.count()\n",
    "vertices_4.select(\"id\", \"sle_coverage_anomaly_score\").show(41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vertices_3.select(\"id\", \"sle_coverage_anomaly_score\").show(41)\n",
    "# vertices_3= vertices_3.selectExpr(\"ap as id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vertices_3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vertices= df_scan_site.selectExpr(\"ap as id\").distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vertices.show(5)\n",
    "# df_scan_site.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges=df_scan_site.filter(col(\"rssi\")>-68)\\\n",
    "    .select(\"ap\", \"ap2\", \"rssi\").groupBy(\"ap\", \"ap2\").agg(F.max(\"rssi\").alias(\"weight\"))\\\n",
    "    .selectExpr(\"ap as src\", \"ap2 as dst\", \"weight\")\n",
    "\n",
    "edges.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enrich vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges.count()\n",
    "# vertices_3.count(), edges.count()/vertices_3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# edges.show()\n",
    "edges.filter(col(\"src\")==ap203).show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_scan_site.filter(col(\"rssi\")>-68).select(\"ap\", \"ap2\", \"rssi\").groupBy(\"ap\", \"ap2\").agg(F.max(\"rssi\").alias(\"weight\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import *\n",
    "g = GraphFrame(vertices_3, edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.vertices.show()\n",
    "g.edges.show()\n",
    "## Check the number of edges of each vertex\n",
    "g.degrees.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aps = vertices.select(\"id\").collect()\n",
    "ap1 = aps[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code gets the number of incoming links to Andrew. This is obtained by calling the inDegrees method.\n",
    "filter_query= \"id ='{}'\".format(ap1)\n",
    "g.degrees.filter(filter_query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code shows how to get the number of links coming out from Andrew using the outDegrees method .\n",
    "\n",
    "# g.outDegrees.filter(\"id = '5c5b3552b5e3'\").show()\n",
    "\n",
    "ap203 = \"5c5b35ae16bc\"\n",
    "\n",
    "g.degrees.filter(\"id ='{}'\".format(ap203)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g.edges.filter(\"id ='{}'\".format(ap203))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.setCheckpointDir('graphframes_cps')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = g.pageRank(resetProbability=0.10, maxIter=5)\n",
    "# display(ranks.vertices.select(\"id\",\"pagerank\").orderBy(desc(\"pagerank\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ranks.vertices)\n",
    "display(ranks.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vertices.show()\n",
    "# xap_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search from \"Esther\" for users of age < 32.\n",
    "\n",
    "# ap1 = \"5c5b3552b96c\"\n",
    "# ap2 = \"5c5b3552b5e3\"\n",
    "# paths = g.bfs(\"id = ap1\", \"id = ap2\")\n",
    "# paths.show()\n",
    "\n",
    "# # Specify edge filters or max path lengths.\n",
    "# g.bfs(\"id = ap1\", \"id = ap2\",\\\n",
    "#   edgeFilter=\"rssi > -75\", maxPathLength=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g.connectedComponents()\n",
    "\n",
    "stronglyConnectedComponents = g.stronglyConnectedComponents(maxIter=10)\n",
    "# stronglyConnectedComponents = g.stronglyConnectedComponents(maxIter=10)\n",
    "# stronglyConnectedComponents.select(\"id\", \"component\").orderBy(\"component\").show()\n",
    "stronglyConnectedComponents.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# result2.select(\"id\", \"component\").orderBy(\"component\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stronglyConnectedComponents.select(\"component\").groupBy(\"component\").count().orderBy(\"count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result2.show()\n",
    "stronglyConnectedComponents.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connectedComponents = g.connectedComponents()\n",
    "# result2.select(\"id\", \"component\").orderBy(\"component\").show()\n",
    "connectedComponents.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connectedComponents.select(\"ap\", \"component\").show(connectedComponents.count())  #.groupBy(\"component\").count().orderBy(\"count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertices_2.select(\"ap\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.vertices.select(\"ap\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_day + hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_out_bucket = \"s3://mist-test-bucket/wenfeng/{}/hr={}\".format(date_day, hr)  #top_1_time_epoch_by_site_ap_ap2_band/{}/{}/\".format(date_day, hr)\n",
    "s3_out_bucket_vertices = s3_out_bucket #+ \"vertices/\"\n",
    "s3_out_bucket_edges = s3_out_bucket #+ \"edges/\"\n",
    "\n",
    "print(s3_out_bucket_vertices, s3_out_bucket_edges)\n",
    "g.vertices.write.mode(\"overwrite\").parquet(s3_out_bucket_vertices)\n",
    "g.edges.write.mode(\"overwrite\").parquet(s3_out_bucket_vertices)\n",
    "# s3_out_bucket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3_out_bucket, rrm_local_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "import networkx as nx\n",
    "gp = nx.from_pandas_edgelist(edges.toPandas(),'src','dst')\n",
    "nx.draw(gp, with_labels = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figsize(15, 20)\n",
    "plt.figure(figsize=(20,10)) \n",
    "nx.draw(gp, with_labels = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph.degrees.filter(\"id = 1\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GraphFrame provides the following built-in algorithms:\n",
    "# Connected components\n",
    "\n",
    "# Label propagation\n",
    "\n",
    "# PageRank\n",
    "\n",
    "# SVD++\n",
    "\n",
    "# Shortest Path\n",
    "\n",
    "# Strongly connected components\n",
    "\n",
    "# Triangle count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "mist_g = nx.read_gpickle(\"../../mist-rrm-exp/test-notebooks/mistG_sams_4989.gpickle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_coverage_site = df_coverage_0.filter(col(\"site\")==site_id)\n",
    "df_coverage_site.show(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
