
from pyspark.sql.functions import udf, size, avg, count, col,sum, explode

import pyspark.sql.functions as F

import json
from datetime import datetime,timedelta
env = "production"
# env = "staging"



import re
from uuid import UUID
def convert_bytes_to_mac(mac_bytes, sep=''):
    '''
    Convert protobuf Mac bytearray generated by GO to mac string
    >>> convert_bytes_to_mac(bytearray(b'\x88\xc6&\xb2B\xad'))
    '88c626b242ad'
    >>> convert_bytes_to_mac(bytearray(b'\x88\xc6&\xb2B\xad'), sep='-')
    '88-c6-26-b2-42-ad'
    :param mac_bytes:
    :return:
    '''
    if mac_bytes is None or len(mac_bytes) != 6:
        return ""

    return sep.join(["{:02x}".format(b) for b in mac_bytes])


def convert_bytes_to_uuid(uuid_bytearray):
    '''
    Convert protobuf UUID bytearray generated by GO to uuid string
    >>> convert_bytes_to_uuid(bytearray(b'\xcd\xa2\x8b\x7fw\xc1El\xae/\x12\xb7\x03\xd8,\x16'))
    'cda28b7f-77c1-456c-ae2f-12b703d82c16'
    >>> convert_bytes_to_uuid(bytearray(b'v\xd6\x9c<\x05\xaeO\x8a\xae\x16\x0c\x08\x98\xc2\xf2\xf0'))
    '76d69c3c-05ae-4f8a-ae16-0c0898c2f2f0'
    :param uuid_bytearray:
    :return:
    '''
    int_val = int.from_bytes(uuid_bytearray, byteorder='big')
    return str(UUID(int=int_val))


from pyspark.sql.types import *
bytes_to_uuid = F.udf(convert_bytes_to_uuid, FloatType())
bytes_to_mac = F.udf(convert_bytes_to_mac, StringType())


now = datetime.now()  - timedelta(hours=5)
date_day = now.strftime("%Y-%m-%d")
date_hour = now.strftime("%H")
date_day = "2021-11-1[23456]/"
date_hour = "*"

s3_bucket = "gs://mist-secorapp-{env}/ap-stats-full/ap-stats-full-{env}/".format(env=env)
s3_bucket += "dt={date}/hr={hr}/*.parquet".format(date=date_day, hr=date_hour)

print(s3_bucket)

df= spark.read.parquet(s3_bucket)
df.printSchema()

