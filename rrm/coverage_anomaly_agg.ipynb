{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://mist-secorapp-staging/sle-coverage-anomaly/sle-coverage-anomaly-staging/dt=2020-03-1*/hr=*/*.seq\n"
     ]
    }
   ],
   "source": [
    "env = \"production\"\n",
    "env = \"staging\"\n",
    "\n",
    "\n",
    "# s3_bucket = \"s3://mist-aggregated-stats-{env}/entity_event/entity_event-{env}/\".format(env=env)\n",
    "date_day = \"2020-03-1*\"\n",
    "hr = '*'\n",
    "\n",
    "s3_bucket = \"s3://mist-secorapp-staging/sle-coverage-anomaly/sle-coverage-anomaly-staging/\".format(env=env)\n",
    "s3_path = s3_bucket + \"dt={day}/hr={hr}/*.seq\".format(day=date_day, hr=hr)\n",
    "\n",
    "print(s3_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4109,\n",
       " bytearray(b'{\"radio_5s\":1,\"util_nonwifi_std_base\":10.686252287455485,\"channel\":11,\"sle_coverage_base\":0.9739787605262338,\"util_nonwifi_deviation\":-0.953568937864141,\"rssi_mean\":-89.0,\"rad_id\":\"r24\",\"sle_capacity_anomaly_score\":-0.001005110795669877,\"util_all_mean_base\":26.527651431022772,\"anomaly_score\":2.698591450511145,\"dev\":\"r1\",\"anomaly_type\":\"weak_signal\",\"band\":\"24\",\"util_cochannel_std\":5.0,\"sle_coverage_anomaly\":true,\"util_cochannel_mean\":13.068257183736655,\"util_cochannel_std_base\":5.0,\"events\":[],\"avg_nclients\":1.0,\"sle_capacity_error\":0.348010216963685,\"util_nonwifi_mean_base\":14.23794877362038,\"util_ap\":1.971286614351615,\"sle_coverage_error\":0.3517354443316347,\"util_cochannel_deviation\":-3.5514748490356416,\"ap\":\"5c-5b-35-0e-b3-ac\",\"rssi_hist\":{\"-89\":180},\"error_rate\":0.7976085778719924,\"sle_coverage\":0.0,\"user_seconds_util_high\":0,\"sle_coverage_base_std\":0.13839426885618095,\"sle_coverage_anomaly_score\":2.698591450511145,\"interval\":60,\"impacted_wlans\":[\"c43d016e-8de6-407b-a41e-03fd0756f232\"],\"rssi_std_base\":6.996248275043327,\"util_cochannel_mean_base\":10.65140570162021,\"rssi_watermark\":-72,\"sle_capacity_base_std\":0.1,\"anomaly\":\"\",\"user_seconds_rssi_weak\":180,\"util_all_std\":5.0,\"sle_capacity_impacted\":1.0,\"user_seconds\":180,\"power\":17,\"user_seconds_util_high_impacted\":0,\"timestamp\":1583799600,\"rssi_std\":3.0,\"org\":\"7e20d5ce-510c-11e5-997e-1258369c38a9\",\"bandwidth\":20,\"util_nonwifi_mean\":10.46431062135859,\"sle_capacity_anomaly\":false,\"util_nonwifi_std\":5.0,\"rssi_mean_base\":-55.92400520481363,\"util_all_deviation\":0.0,\"sle_capacity\":1.0,\"site\":\"fcea653f-28eb-45f6-9d89-0361eada8ca1\",\"util_all_std_base\":11.92410447225129,\"user_seconds_rssi_weak_impacted\":180,\"sle_coverage_impacted\":0.0,\"util_watermark\":20,\"util_all_mean\":22.477797045313846,\"max_nclients\":1.0,\"sle_capacity_base\":0.9996502111739264,\"rssi_deviation\":5.156477211347004}'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rdd_capacity = spark.sparkContext.sequenceFile(s3_capacity_path)\n",
    "# rdd_coverage = spark.sparkContext.sequenceFile(s3_coverage_path)\n",
    "rdd_coverage = spark.sparkContext.sequenceFile(s3_path)\n",
    "rdd_coverage.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/mistsys/ds_incubator/venv/spark-2.4.4-bin-without-hadoop/python/pyspark/sql/session.py:366: UserWarning: Using RDD of dict to inferSchema is deprecated. Use pyspark.sql.Row instead\n",
      "  warnings.warn(\"Using RDD of dict to inferSchema is deprecated. \"\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# rdd_site = rdd.map(lambda x: json.loads(x[1])).map(lambda x: (x.get(\"site_id\"), x.get(\"ap_id\"))).groupByKey()\n",
    "# rdd_site = rdd.map(lambda x: json.loads(x[1])).map(lambda x: (x.get(\"site_id\"))).groupBy('site_id')\n",
    "df_coverage = rdd_coverage.map(lambda x: json.loads(x[1])).toDF() #.map(lambda x: json.loads(x[1])).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- anomaly: string (nullable = true)\n",
      " |-- anomaly_score: double (nullable = true)\n",
      " |-- anomaly_type: string (nullable = true)\n",
      " |-- ap: string (nullable = true)\n",
      " |-- avg_nclients: double (nullable = true)\n",
      " |-- band: string (nullable = true)\n",
      " |-- bandwidth: long (nullable = true)\n",
      " |-- channel: long (nullable = true)\n",
      " |-- dev: string (nullable = true)\n",
      " |-- error_rate: double (nullable = true)\n",
      " |-- events: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- impacted_wlans: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- interval: long (nullable = true)\n",
      " |-- max_nclients: double (nullable = true)\n",
      " |-- org: string (nullable = true)\n",
      " |-- power: long (nullable = true)\n",
      " |-- rad_id: string (nullable = true)\n",
      " |-- radio_5s: long (nullable = true)\n",
      " |-- rssi_deviation: double (nullable = true)\n",
      " |-- rssi_hist: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: long (valueContainsNull = true)\n",
      " |-- rssi_mean: double (nullable = true)\n",
      " |-- rssi_mean_base: double (nullable = true)\n",
      " |-- rssi_std: double (nullable = true)\n",
      " |-- rssi_std_base: double (nullable = true)\n",
      " |-- rssi_watermark: long (nullable = true)\n",
      " |-- site: string (nullable = true)\n",
      " |-- sle_capacity: double (nullable = true)\n",
      " |-- sle_capacity_anomaly: boolean (nullable = true)\n",
      " |-- sle_capacity_anomaly_score: double (nullable = true)\n",
      " |-- sle_capacity_base: double (nullable = true)\n",
      " |-- sle_capacity_base_std: double (nullable = true)\n",
      " |-- sle_capacity_error: double (nullable = true)\n",
      " |-- sle_capacity_impacted: double (nullable = true)\n",
      " |-- sle_coverage: double (nullable = true)\n",
      " |-- sle_coverage_anomaly: boolean (nullable = true)\n",
      " |-- sle_coverage_anomaly_score: double (nullable = true)\n",
      " |-- sle_coverage_base: double (nullable = true)\n",
      " |-- sle_coverage_base_std: double (nullable = true)\n",
      " |-- sle_coverage_error: double (nullable = true)\n",
      " |-- sle_coverage_impacted: double (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- user_seconds: long (nullable = true)\n",
      " |-- user_seconds_rssi_weak: long (nullable = true)\n",
      " |-- user_seconds_rssi_weak_impacted: long (nullable = true)\n",
      " |-- user_seconds_util_high: long (nullable = true)\n",
      " |-- user_seconds_util_high_impacted: long (nullable = true)\n",
      " |-- util_all_deviation: double (nullable = true)\n",
      " |-- util_all_mean: double (nullable = true)\n",
      " |-- util_all_mean_base: double (nullable = true)\n",
      " |-- util_all_std: double (nullable = true)\n",
      " |-- util_all_std_base: double (nullable = true)\n",
      " |-- util_ap: double (nullable = true)\n",
      " |-- util_cochannel_deviation: double (nullable = true)\n",
      " |-- util_cochannel_mean: double (nullable = true)\n",
      " |-- util_cochannel_mean_base: double (nullable = true)\n",
      " |-- util_cochannel_std: double (nullable = true)\n",
      " |-- util_cochannel_std_base: double (nullable = true)\n",
      " |-- util_nonwifi_deviation: double (nullable = true)\n",
      " |-- util_nonwifi_mean: double (nullable = true)\n",
      " |-- util_nonwifi_mean_base: double (nullable = true)\n",
      " |-- util_nonwifi_std: double (nullable = true)\n",
      " |-- util_nonwifi_std_base: double (nullable = true)\n",
      " |-- util_watermark: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df_capacity\n",
    "df_coverage.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df_capacity.select(\"org\", \"site\",  \"anomaly\", \"anomaly_score\", \"anomaly_type\", \"sle_capacity_anomaly\", \"util_nonwifi_mean\", \"util_cochannel_mean\").show()\n",
    "\n",
    "df_coverage_g = df_coverage.select(\"org\", \"site\", \"anomaly_type\").groupBy(\"org\", \"site\", \"anomaly_type\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coverage_g.orderBy('count', ascending=False).take(10)\n",
    "\n",
    "# df_capacity_g.sort('count', ascending=False).show() #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd_site.take(1)\n",
    "rdd_capacity_by_site= rdd_capacity.map(lambda x: json.loads(x[1])).map(lambda x: (x.get(\"site_id\"),1)).countByKey()\n",
    "\n",
    "rdd_coverage_by_site= rdd_coverage.map(lambda x: json.loads(x[1])).map(lambda x: (x.get(\"site_id\"),1)).countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_capacity_sorted = sorted(rdd_capacity_by_site.items(), key=lambda v:v[1], reverse=True) #, key=lambda(k, v): v)\n",
    "site_coverage_sorted = sorted(rdd_coverage_by_site.items(), key=lambda v:v[1], reverse=True) #, key=lambda(k, v): v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('5cf9de1a-8034-4472-86f6-9a100ebdb792', 738),\n",
       " ('96488ea8-6dee-11e6-a43a-02e208b2d34f', 590),\n",
       " ('866808ba-1ff2-4e99-8e17-34ee1d3c70f4', 365),\n",
       " ('b3c32004-a0dc-4353-b697-1065fbd4f639', 325),\n",
       " ('64f4d304-5015-4777-911f-d71e809397c2', 315),\n",
       " ('3cf29ebc-21f1-49e5-9956-25495fe8f41b', 312),\n",
       " ('e0e7ada3-59bb-4189-8e1f-ca489812828c', 292),\n",
       " ('46247212-c9ab-47f3-ba04-ac38446640a3', 273),\n",
       " ('67151fdb-2d69-4e07-a2b9-0652c3d5bec5', 269),\n",
       " ('68fdb9ba-7b27-4293-9bf2-ad1c987c9466', 250)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "site_capacity_sorted[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('5cf9de1a-8034-4472-86f6-9a100ebdb792', 657),\n",
       " ('866808ba-1ff2-4e99-8e17-34ee1d3c70f4', 398),\n",
       " ('b3c32004-a0dc-4353-b697-1065fbd4f639', 357),\n",
       " ('96488ea8-6dee-11e6-a43a-02e208b2d34f', 354),\n",
       " ('a799e12a-fc28-4ce7-a563-c2dd6d498e2f', 316),\n",
       " ('3cf29ebc-21f1-49e5-9956-25495fe8f41b', 273),\n",
       " ('50fa3df4-053f-4779-86d5-f53d74343077', 225),\n",
       " ('64f4d304-5015-4777-911f-d71e809397c2', 200),\n",
       " ('469fc38e-33da-4562-8b4c-6720c43a53e0', 186),\n",
       " ('c14381b7-a3fe-4a85-ac51-11813efded19', 176)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "site_coverage_sorted[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd_coverage_by_site.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "user_org_id = \"38a18d4d-1623-4985-86d0-1bb06e5e2a48\"  # UPS\n",
    "user_site_USNYELE = '9aa6ecdb-ddee-41b4-a8d9-872d7962b3c1'  #UPS USNYELE\n",
    "\n",
    "# user_org_id = '38a18d4d-1623-4985-86d0-1bb06e5e2a48'  # UPS\n",
    "user_site_USFLLON = 'c23667f3-47e9-44aa-9761-de49d34ed9f9' # USFLLON\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_capacity_by_site.get(user_site_USNYELE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_capacity_by_site.get(user_site_USFLLON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://mist-aggregated-stats-production/aggregated-stats/top_1_time_epoch_by_site_ap_ap2_band/dt=2020-03-03/hr=00/last_0.25_day/*.csv\n",
      "root\n",
      " |-- site: string (nullable = true)\n",
      " |-- ap: string (nullable = true)\n",
      " |-- ap2: string (nullable = true)\n",
      " |-- band: string (nullable = true)\n",
      " |-- time_epoch: string (nullable = true)\n",
      " |-- rssi: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# file_path = 's3://mist-secorapp-{env}/oc-stats-analytics/oc-stats-analytics-{env}/dt={date}/*'.format(env=env, date=DATE)\n",
    "\n",
    "s3_apscan_bucket = 's3://mist-aggregated-stats-{ENV}/aggregated-stats/top_1_time_epoch_by_site_ap_ap2_band/'.format(ENV=env)\n",
    "\n",
    "\n",
    "s3_apscan_path = s3_apscan_bucket + \"dt={date}/hr={hour}/last_0.25_day/*.csv\".format(date=date_day, hour=hr)\n",
    "\n",
    "print(s3_apscan_path)\n",
    "\n",
    "# file_path = \"s3a://mist-aggregated-stats-production/aggregated-stats/top_1_time_epoch_by_site_ap_ap2_band/dt=2020-02-26/hr=17/last_0.25_day/part-00000-8b45ee45-d247-4bc9-b4df-b621135297eb-c000.csv\"\n",
    "df_apscan= spark.read.option(\"header\", True).csv(s3_apscan_path)\n",
    "\n",
    "# df = spark.read.parquet(file_path)\n",
    "df_apscan.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+------------+----+----------+-----+----+\n",
      "|                site|          ap|         ap2|band|time_epoch| rssi|date|\n",
      "+--------------------+------------+------------+----+----------+-----+----+\n",
      "|00193f17-4e86-476...|5c5b357eec90|5c5b35aed787|  24|1583197064|-65.0|null|\n",
      "|00193f17-4e86-476...|5c5b35aed61f|5c5b35aed791|  24|1583197062|-63.0|null|\n",
      "|00193f17-4e86-476...|5c5b35aed7e6|5c5b35aed7cd|  24|1583191062|-67.0|null|\n",
      "|00226d33-cd80-4b1...|5c5b35501a7c|        null|  24|1583197014|-61.0|null|\n",
      "|00272816-8dc2-4f1...|5c5b357f2822|5c5b35ae6ced|  24|1583197063|-67.0|null|\n",
      "|00272816-8dc2-4f1...|5c5b357f284f|5c5b35ae7152|   5|1583197062|-59.0|null|\n",
      "|00272816-8dc2-4f1...|5c5b357f32c7|5c5b35ae6d1f|  24|1583196761|-76.0|null|\n",
      "|00272816-8dc2-4f1...|5c5b357f3308|        null|   5|1583197064|-59.0|null|\n",
      "|00272816-8dc2-4f1...|5c5b35ae6e55|5c5b35ae6c89|   5|1583197062|-64.0|null|\n",
      "|00272816-8dc2-4f1...|5c5b35ae6e55|5c5b35ae6d97|   5|1583197062|-69.0|null|\n",
      "|00272816-8dc2-4f1...|5c5b35ae7152|5c5b35ae6e5a|   5|1583197060|-72.0|null|\n",
      "|00458c29-d67b-4cf...|5c5b352ee141|5c5b353e5708|  24|1583197025|-71.0|null|\n",
      "|00458c29-d67b-4cf...|5c5b353e56cc|5c5b353e56f9|   5|1583197104|-59.0|null|\n",
      "|00458c29-d67b-4cf...|5c5b353e579e|5c5b353e571c|  24|1583197088|-63.0|null|\n",
      "|00458c29-d67b-4cf...|5c5b353e6455|5c5b353e56f9|  24|1583197107|-58.0|null|\n",
      "|00458c29-d67b-4cf...|5c5b353e6455|5c5b353e573f|   5|1583197107|-63.0|null|\n",
      "|00458c29-d67b-4cf...|5c5b353e6469|        null|  24|1583197092|-73.0|null|\n",
      "|00544ad7-df28-4b9...|5c5b358f167a|5c5b358ef9b5|   5|1583197138|-51.0|null|\n",
      "|00544ad7-df28-4b9...|5c5b358f167a|5c5b358f1620|  24|1583197138|-58.0|null|\n",
      "|00638b36-6912-40c...|5c5b35ceaa55|5c5b35ceac8f|   5|1583177929|-69.0|null|\n",
      "+--------------------+------------+------------+----+----------+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import DateType\n",
    "\n",
    "# df_ap.select(\"time_epoch\").show()\n",
    "df_apscan.withColumn('date', df_apscan['time_epoch'].cast(DateType())).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# import pyspark\n",
    "# from pyspark.sql import SparkSession, Row\n",
    "# from pyspark.sql.types import StringType, IntegerType, ArrayType, FloatType, DataType\n",
    "# import pyspark.sql.functions as fn\n",
    "from pyspark.sql.functions import udf, size, avg, count, col,sum, explode\n",
    "# from operator import itemgetter\n",
    "# import json, datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_apscan_site = df_apscan.filter(col(\"site\")==(\"67970e46-4e12-11e6-9188-0242ac110007\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o68.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 10 in stage 14.0 failed 1 times, most recent failure: Lost task 10.0 in stage 14.0 (TID 65, localhost, executor driver): org.apache.hadoop.fs.s3a.AWSS3IOException: getFileStatus on s3://mist-aggregated-stats-production/aggregated-stats/top_1_time_epoch_by_site_ap_ap2_band/dt=2020-03-03/hr=00/last_0.25_day/part-00000-cbf445ab-42fc-42aa-b78f-c79111c394ad-c000.csv: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: D7299B3BAB74A4A9), S3 Extended Request ID: 6SCpTMCwGs4ZxOZ/Y1xG9ODJTwAAcK/+ZWoSILeUqQZhQI/s1ynWlgeMZCksbVXtNwWB8BvH0e0=: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: D7299B3BAB74A4A9)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:178)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:101)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1568)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:521)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:787)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:85)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:65)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:47)\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.readFile(CSVDataSource.scala:199)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anonfun$buildReader$2.apply(CSVFileFormat.scala:142)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anonfun$buildReader$2.apply(CSVFileFormat.scala:136)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:148)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:132)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: D7299B3BAB74A4A9), S3 Extended Request ID: 6SCpTMCwGs4ZxOZ/Y1xG9ODJTwAAcK/+ZWoSILeUqQZhQI/s1ynWlgeMZCksbVXtNwWB8BvH0e0=\n\tat com.amazonaws.http.AmazonHttpClient.handleErrorResponse(AmazonHttpClient.java:1182)\n\tat com.amazonaws.http.AmazonHttpClient.executeOneRequest(AmazonHttpClient.java:770)\n\tat com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:489)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:310)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3785)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1050)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1027)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:904)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1553)\n\t... 28 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:299)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2836)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2835)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2835)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.hadoop.fs.s3a.AWSS3IOException: getFileStatus on s3://mist-aggregated-stats-production/aggregated-stats/top_1_time_epoch_by_site_ap_ap2_band/dt=2020-03-03/hr=00/last_0.25_day/part-00000-cbf445ab-42fc-42aa-b78f-c79111c394ad-c000.csv: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: D7299B3BAB74A4A9), S3 Extended Request ID: 6SCpTMCwGs4ZxOZ/Y1xG9ODJTwAAcK/+ZWoSILeUqQZhQI/s1ynWlgeMZCksbVXtNwWB8BvH0e0=: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: D7299B3BAB74A4A9)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:178)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:101)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1568)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:521)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:787)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:85)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:65)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:47)\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.readFile(CSVDataSource.scala:199)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anonfun$buildReader$2.apply(CSVFileFormat.scala:142)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anonfun$buildReader$2.apply(CSVFileFormat.scala:136)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:148)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:132)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: D7299B3BAB74A4A9), S3 Extended Request ID: 6SCpTMCwGs4ZxOZ/Y1xG9ODJTwAAcK/+ZWoSILeUqQZhQI/s1ynWlgeMZCksbVXtNwWB8BvH0e0=\n\tat com.amazonaws.http.AmazonHttpClient.handleErrorResponse(AmazonHttpClient.java:1182)\n\tat com.amazonaws.http.AmazonHttpClient.executeOneRequest(AmazonHttpClient.java:770)\n\tat com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:489)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:310)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3785)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1050)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1027)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:904)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1553)\n\t... 28 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-b7a77c0ac9b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_apscan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/mistsys/ds_incubator/venv/spark-2.4.4-bin-without-hadoop/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m         \"\"\"\n\u001b[0;32m--> 523\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/mistsys/ds_incubator/venv/spark-2.4.4-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/mistsys/ds_incubator/venv/spark-2.4.4-bin-without-hadoop/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/mistsys/ds_incubator/venv/spark-2.4.4-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o68.count.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 10 in stage 14.0 failed 1 times, most recent failure: Lost task 10.0 in stage 14.0 (TID 65, localhost, executor driver): org.apache.hadoop.fs.s3a.AWSS3IOException: getFileStatus on s3://mist-aggregated-stats-production/aggregated-stats/top_1_time_epoch_by_site_ap_ap2_band/dt=2020-03-03/hr=00/last_0.25_day/part-00000-cbf445ab-42fc-42aa-b78f-c79111c394ad-c000.csv: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: D7299B3BAB74A4A9), S3 Extended Request ID: 6SCpTMCwGs4ZxOZ/Y1xG9ODJTwAAcK/+ZWoSILeUqQZhQI/s1ynWlgeMZCksbVXtNwWB8BvH0e0=: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: D7299B3BAB74A4A9)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:178)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:101)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1568)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:521)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:787)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:85)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:65)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:47)\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.readFile(CSVDataSource.scala:199)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anonfun$buildReader$2.apply(CSVFileFormat.scala:142)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anonfun$buildReader$2.apply(CSVFileFormat.scala:136)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:148)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:132)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: D7299B3BAB74A4A9), S3 Extended Request ID: 6SCpTMCwGs4ZxOZ/Y1xG9ODJTwAAcK/+ZWoSILeUqQZhQI/s1ynWlgeMZCksbVXtNwWB8BvH0e0=\n\tat com.amazonaws.http.AmazonHttpClient.handleErrorResponse(AmazonHttpClient.java:1182)\n\tat com.amazonaws.http.AmazonHttpClient.executeOneRequest(AmazonHttpClient.java:770)\n\tat com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:489)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:310)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3785)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1050)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1027)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:904)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1553)\n\t... 28 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1889)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1877)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1876)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:299)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2836)\n\tat org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2835)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2835)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.hadoop.fs.s3a.AWSS3IOException: getFileStatus on s3://mist-aggregated-stats-production/aggregated-stats/top_1_time_epoch_by_site_ap_ap2_band/dt=2020-03-03/hr=00/last_0.25_day/part-00000-cbf445ab-42fc-42aa-b78f-c79111c394ad-c000.csv: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: D7299B3BAB74A4A9), S3 Extended Request ID: 6SCpTMCwGs4ZxOZ/Y1xG9ODJTwAAcK/+ZWoSILeUqQZhQI/s1ynWlgeMZCksbVXtNwWB8BvH0e0=: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: D7299B3BAB74A4A9)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:178)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:101)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1568)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:521)\n\tat org.apache.hadoop.fs.FileSystem.open(FileSystem.java:787)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.initialize(LineRecordReader.java:85)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:65)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.<init>(HadoopFileLinesReader.scala:47)\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.readFile(CSVDataSource.scala:199)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anonfun$buildReader$2.apply(CSVFileFormat.scala:142)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat$$anonfun$buildReader$2.apply(CSVFileFormat.scala:136)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:148)\n\tat org.apache.spark.sql.execution.datasources.FileFormat$$anon$1.apply(FileFormat.scala:132)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:124)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:177)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:101)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.agg_doAggregateWithoutKey_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:636)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: com.amazonaws.services.s3.model.AmazonS3Exception: Bad Request (Service: Amazon S3; Status Code: 400; Error Code: 400 Bad Request; Request ID: D7299B3BAB74A4A9), S3 Extended Request ID: 6SCpTMCwGs4ZxOZ/Y1xG9ODJTwAAcK/+ZWoSILeUqQZhQI/s1ynWlgeMZCksbVXtNwWB8BvH0e0=\n\tat com.amazonaws.http.AmazonHttpClient.handleErrorResponse(AmazonHttpClient.java:1182)\n\tat com.amazonaws.http.AmazonHttpClient.executeOneRequest(AmazonHttpClient.java:770)\n\tat com.amazonaws.http.AmazonHttpClient.executeHelper(AmazonHttpClient.java:489)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:310)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:3785)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1050)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1027)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:904)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:1553)\n\t... 28 more\n"
     ]
    }
   ],
   "source": [
    "df_apscan_site.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+---+----+----------+----+\n",
      "|site| ap|ap2|band|time_epoch|rssi|\n",
      "+----+---+---+----+----------+----+\n",
      "+----+---+---+----+----------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_apscan_site.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphframes import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
