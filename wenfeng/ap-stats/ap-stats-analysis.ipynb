{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as Fun\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StringType, ArrayType, DataType, json\n",
    "import seaborn as sea\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.read.parquet(\"./1_9_00000000000634607143.parquet\")\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- site_id: string (nullable = true)\n",
      " |-- org_id: string (nullable = true)\n",
      " |-- firmware_version: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      " |-- uptime: integer (nullable = true)\n",
      " |-- interval: long (nullable = true)\n",
      " |-- config_time: long (nullable = true)\n",
      " |-- when: long (nullable = true)\n",
      " |-- total_client_count: integer (nullable = true)\n",
      " |-- active_client_count: integer (nullable = true)\n",
      " |-- terminator_remote_addr: string (nullable = true)\n",
      " |-- terminator_timestamp: long (nullable = true)\n",
      " |-- dnsServers: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- svis: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- dev: string (nullable = true)\n",
      " |    |    |-- vlan: integer (nullable = true)\n",
      " |    |    |-- ips: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |-- inactive_wired_vlans: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = true)\n",
      " |-- IPv4Routers: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- dest: string (nullable = true)\n",
      " |    |    |-- gateway: string (nullable = true)\n",
      " |-- dhcpclients: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- dev: string (nullable = true)\n",
      " |    |    |-- ip: string (nullable = true)\n",
      " |    |    |-- gateway: string (nullable = true)\n",
      " |-- l2tptunnels: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- id: string (nullable = true)\n",
      " |    |    |-- state: string (nullable = true)\n",
      " |    |    |-- uptime: integer (nullable = true)\n",
      " |    |    |-- sessions: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- remote_end_id: string (nullable = true)\n",
      " |    |    |    |    |-- state: string (nullable = true)\n",
      " |    |    |    |    |-- local_session_id: integer (nullable = true)\n",
      " |    |    |    |    |-- remote_session_id: integer (nullable = true)\n",
      " |-- lldp_nd: struct (nullable = true)\n",
      " |    |-- port_desc: string (nullable = true)\n",
      " |    |-- sys_name: string (nullable = true)\n",
      " |    |-- sys_desc: string (nullable = true)\n",
      " |    |-- mgmt_addr: string (nullable = true)\n",
      " |    |-- port_id: string (nullable = true)\n",
      " |-- lldp_pwr: struct (nullable = true)\n",
      " |    |-- req_cnt: integer (nullable = true)\n",
      " |    |-- xtnd_sup: boolean (nullable = true)\n",
      " |    |-- pse_avail: integer (nullable = true)\n",
      " |    |-- pd_req: integer (nullable = true)\n",
      " |    |-- pd_needs: integer (nullable = true)\n",
      " |-- radios: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- dev: string (nullable = true)\n",
      " |    |    |-- band: string (nullable = true)\n",
      " |    |    |-- channel: integer (nullable = true)\n",
      " |    |    |-- secondary_channel: integer (nullable = true)\n",
      " |    |    |-- bandwidth: integer (nullable = true)\n",
      " |    |    |-- noise_floor: integer (nullable = true)\n",
      " |    |    |-- tx_power: integer (nullable = true)\n",
      " |    |    |-- max_tx_power: integer (nullable = true)\n",
      " |    |    |-- per_antenna_rssi: array (nullable = true)\n",
      " |    |    |    |-- element: integer (containsNull = true)\n",
      " |    |    |-- utilization_all: double (nullable = true)\n",
      " |    |    |-- utilization_tx: double (nullable = true)\n",
      " |    |    |-- utilization_rx_in_bss: double (nullable = true)\n",
      " |    |    |-- utilization_rx_other_bss: double (nullable = true)\n",
      " |    |    |-- utilization_unknown_wifi: double (nullable = true)\n",
      " |    |    |-- utilization_non_wifi: double (nullable = true)\n",
      " |    |    |-- dfs_radar_detected: boolean (nullable = true)\n",
      " |    |    |-- dfs_cac_state: integer (nullable = true)\n",
      " |    |    |-- tx_bytes: long (nullable = true)\n",
      " |    |    |-- tx_pkts: integer (nullable = true)\n",
      " |    |    |-- tx_mgmt: integer (nullable = true)\n",
      " |    |    |-- tx_errors: integer (nullable = true)\n",
      " |    |    |-- rx_bytes: long (nullable = true)\n",
      " |    |    |-- rx_pkts: integer (nullable = true)\n",
      " |    |    |-- rx_mgmt: integer (nullable = true)\n",
      " |    |    |-- rx_errors: integer (nullable = true)\n",
      " |    |    |-- tx_failed_arp: integer (nullable = true)\n",
      " |    |    |-- tx_drop_arp: integer (nullable = true)\n",
      " |    |    |-- tx_psblk_fifo: integer (nullable = true)\n",
      " |    |    |-- tx_toss: integer (nullable = true)\n",
      " |    |    |-- tx_toss_arp: integer (nullable = true)\n",
      " |    |    |-- tx_toss_bcmc: integer (nullable = true)\n",
      " |    |    |-- tx_toss_unicast: integer (nullable = true)\n",
      " |    |    |-- tx_bcmc2unicast: integer (nullable = true)\n",
      " |    |    |-- tx_bcmc2unicast_client: integer (nullable = true)\n",
      " |    |    |-- tx_bcn_intr: integer (nullable = true)\n",
      " |    |    |-- tx_phy_err: integer (nullable = true)\n",
      " |    |    |-- tx_failed: integer (nullable = true)\n",
      " |    |    |-- tx_retries: integer (nullable = true)\n",
      " |    |    |-- tx_retried: integer (nullable = true)\n",
      " |    |    |-- rx_dups: integer (nullable = true)\n",
      " |    |    |-- rx_retried: integer (nullable = true)\n",
      " |    |    |-- tx_bcmc2unicast_bytes: long (nullable = true)\n",
      " |    |    |-- tx_bcmc2unicast_client_bytes: long (nullable = true)\n",
      " |    |    |-- re_init: integer (nullable = true)\n",
      " |    |    |-- re_init_throttle: integer (nullable = true)\n",
      " |    |    |-- tx_mgmt_dropped: integer (nullable = true)\n",
      " |    |    |-- rx_fifo_overflow: integer (nullable = true)\n",
      " |    |    |-- rx_hl_fifo_overflow: integer (nullable = true)\n",
      " |    |    |-- rx_hwprobe_req: integer (nullable = true)\n",
      " |    |    |-- rx_hwprobe_req_qoverflow: integer (nullable = true)\n",
      " |    |    |-- tx_hwprobe_resp: integer (nullable = true)\n",
      " |    |    |-- tx_hwprobe_resp_failed: integer (nullable = true)\n",
      " |    |    |-- tx_hwprobe_resp_drop_timeout: integer (nullable = true)\n",
      " |    |    |-- ampdutx_fifo_full: integer (nullable = true)\n",
      " |    |    |-- ampdutx_drop: integer (nullable = true)\n",
      " |    |    |-- ampdutx_stuck: integer (nullable = true)\n",
      " |    |    |-- ampdutx_orphan: integer (nullable = true)\n",
      " |    |    |-- ampdutx_stuck_ps: integer (nullable = true)\n",
      " |    |    |-- ampdutx_r0hole: integer (nullable = true)\n",
      " |    |    |-- ampdutx_rnhole: integer (nullable = true)\n",
      " |    |    |-- ampdutx_rlag: integer (nullable = true)\n",
      " |    |    |-- ampdutx_tx_add_ba_req: integer (nullable = true)\n",
      " |    |    |-- ampdutx_rx_add_ba_resp: integer (nullable = true)\n",
      " |    |    |-- ampdutx_lost: integer (nullable = true)\n",
      " |    |    |-- ampdutx_tx_bar: integer (nullable = true)\n",
      " |    |    |-- ampdutx_rx_ba: integer (nullable = true)\n",
      " |    |    |-- ampdutx_no_ba: integer (nullable = true)\n",
      " |    |    |-- ampdutx_rx_unexpect: integer (nullable = true)\n",
      " |    |    |-- ampdutx_tx_del_ba: integer (nullable = true)\n",
      " |    |    |-- ampdutx_rx_del_ba: integer (nullable = true)\n",
      " |    |    |-- ampdutx_block_data_fifo: integer (nullable = true)\n",
      " |    |    |-- ampdutx_orphan_bad_ini: integer (nullable = true)\n",
      " |    |    |-- ampdutx_drop_no_buf: integer (nullable = true)\n",
      " |    |    |-- ampdutx_drop_err: integer (nullable = true)\n",
      " |    |    |-- ampdutx_orphan_bad_ini_free: integer (nullable = true)\n",
      " |    |    |-- ampdurx_holes: integer (nullable = true)\n",
      " |    |    |-- ampdurx_stuck: integer (nullable = true)\n",
      " |    |    |-- ampdurx_rx_add_ba_req: integer (nullable = true)\n",
      " |    |    |-- ampdurx_tx_add_ba_resp: integer (nullable = true)\n",
      " |    |    |-- ampdurx_rx_bar: integer (nullable = true)\n",
      " |    |    |-- ampdurx_tx_ba: integer (nullable = true)\n",
      " |    |    |-- ampdurx_rx_unexpect: integer (nullable = true)\n",
      " |    |    |-- ampdurx_tx_del_ba: integer (nullable = true)\n",
      " |    |    |-- ampdurx_rx_del_ba: integer (nullable = true)\n",
      " |    |    |-- num_clients: integer (nullable = true)\n",
      " |    |    |-- radio_missing: boolean (nullable = true)\n",
      " |    |    |-- interrupt_stats_tx_bcn_succ: integer (nullable = true)\n",
      " |    |    |-- interrupt_stats_dmaint: integer (nullable = true)\n",
      " |    |    |-- narstats_queued: integer (nullable = true)\n",
      " |    |    |-- narstats_dequeued: integer (nullable = true)\n",
      " |    |    |-- narstats_held: integer (nullable = true)\n",
      " |    |    |-- narstats_dropped: integer (nullable = true)\n",
      " |    |    |-- phy_type: integer (nullable = true)\n",
      " |    |    |-- rx_probe_req_bc: integer (nullable = true)\n",
      " |    |    |-- rx_probe_req_non_bc: integer (nullable = true)\n",
      " |    |    |-- ignore_probe_req_min_rssi: integer (nullable = true)\n",
      " |    |    |-- tx_probe_resp_sw: integer (nullable = true)\n",
      " |    |    |-- rx_probe_req_rand: integer (nullable = true)\n",
      " |    |    |-- pkt_queue_requested: integer (nullable = true)\n",
      " |    |    |-- pkt_queue_full_dropped: integer (nullable = true)\n",
      " |    |    |-- pkt_queue_dropped: integer (nullable = true)\n",
      " |    |    |-- pkt_queue_retried: integer (nullable = true)\n",
      " |    |    |-- pkt_pend_bk: integer (nullable = true)\n",
      " |    |    |-- pkt_pend_be: integer (nullable = true)\n",
      " |    |    |-- pkt_pend_vi: integer (nullable = true)\n",
      " |    |    |-- pkt_pend_vo: integer (nullable = true)\n",
      " |    |    |-- pkt_pend_bcmc: integer (nullable = true)\n",
      " |    |    |-- pkt_pend_atim: integer (nullable = true)\n",
      " |    |    |-- pkt_pend_common: integer (nullable = true)\n",
      " |    |    |-- pkt_pend_ampdu: integer (nullable = true)\n",
      " |    |    |-- pkt_pend_amsdu: integer (nullable = true)\n",
      " |    |    |-- pkt_pend_ps: integer (nullable = true)\n",
      " |    |    |-- pkt_pend_hw: integer (nullable = true)\n",
      " |    |    |-- pkt_pend_all: integer (nullable = true)\n",
      " |    |    |-- wlans: array (nullable = true)\n",
      " |    |    |    |-- element: string (containsNull = true)\n",
      " |    |    |-- scan_channel: boolean (nullable = true)\n",
      " |    |    |-- ampdutx_tx_ampdu_succ: integer (nullable = true)\n",
      " |    |    |-- ampdurx_rx_mpdu: integer (nullable = true)\n",
      " |    |    |-- ampdurx_rx_ampdu: integer (nullable = true)\n",
      " |    |    |-- radio_reset: boolean (nullable = true)\n",
      " |    |    |-- num_active_clients: integer (nullable = true)\n",
      " |-- wlans: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- id: string (nullable = true)\n",
      " |    |    |-- radio_index: integer (nullable = true)\n",
      " |    |    |-- bssid: string (nullable = true)\n",
      " |    |    |-- dev: string (nullable = true)\n",
      " |    |    |-- ssid: string (nullable = true)\n",
      " |    |    |-- num_clients: integer (nullable = true)\n",
      " |    |    |-- tx_mcast_pkts: integer (nullable = true)\n",
      " |    |    |-- tx_mcast_bytes: long (nullable = true)\n",
      " |    |    |-- radius_auth: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- ip: string (nullable = true)\n",
      " |    |    |    |    |-- port: integer (nullable = true)\n",
      " |    |    |    |    |-- requests: integer (nullable = true)\n",
      " |    |    |    |    |-- retransmissions: integer (nullable = true)\n",
      " |    |    |    |    |-- access_accepts: integer (nullable = true)\n",
      " |    |    |    |    |-- access_rejects: integer (nullable = true)\n",
      " |    |    |    |    |-- access_challenges: integer (nullable = true)\n",
      " |    |    |    |    |-- responses: integer (nullable = true)\n",
      " |    |    |    |    |-- malformed_responses: integer (nullable = true)\n",
      " |    |    |    |    |-- bad_authenticators: integer (nullable = true)\n",
      " |    |    |    |    |-- timeouts: integer (nullable = true)\n",
      " |    |    |    |    |-- unknown_types: integer (nullable = true)\n",
      " |    |    |    |    |-- packets_dropped: integer (nullable = true)\n",
      " |    |    |    |    |-- round_trip_time_ms: integer (nullable = true)\n",
      " |    |    |-- radius_acct: array (nullable = true)\n",
      " |    |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |    |-- ip: string (nullable = true)\n",
      " |    |    |    |    |-- port: integer (nullable = true)\n",
      " |    |    |    |    |-- requests: integer (nullable = true)\n",
      " |    |    |    |    |-- retransmissions: integer (nullable = true)\n",
      " |    |    |    |    |-- responses: integer (nullable = true)\n",
      " |    |    |    |    |-- malformed_responses: integer (nullable = true)\n",
      " |    |    |    |    |-- timeouts: integer (nullable = true)\n",
      " |    |    |    |    |-- unknown_types: integer (nullable = true)\n",
      " |    |    |    |    |-- packets_dropped: integer (nullable = true)\n",
      " |    |    |    |    |-- round_trip_time_ms: integer (nullable = true)\n",
      " |    |    |-- radio: string (nullable = true)\n",
      " |-- pwr_src: string (nullable = true)\n",
      " |-- pwr_budget: integer (nullable = true)\n",
      " |-- pwr_needed: integer (nullable = true)\n",
      " |-- pwr_available: integer (nullable = true)\n",
      " |-- memory_total_kb: long (nullable = true)\n",
      " |-- memory_used_kb: long (nullable = true)\n",
      " |-- cpu_total_time: long (nullable = true)\n",
      " |-- cpu_user: long (nullable = true)\n",
      " |-- cpu_system: long (nullable = true)\n",
      " |-- fs_available_kb: long (nullable = true)\n",
      " |-- fs_total_kb: long (nullable = true)\n",
      " |-- load_total: integer (nullable = true)\n",
      " |-- slab_active_objs: integer (nullable = true)\n",
      " |-- slab_num_objs: integer (nullable = true)\n",
      " |-- slab_obj_size: integer (nullable = true)\n",
      " |-- cloud_last_rtt: float (nullable = true)\n",
      " |-- cloud_pmtu: integer (nullable = true)\n",
      " |-- temperature_attitude: float (nullable = true)\n",
      " |-- temperature_cpu: float (nullable = true)\n",
      " |-- temperature_ambient: float (nullable = true)\n",
      " |-- orientation_accel_x: float (nullable = true)\n",
      " |-- orientation_accel_y: float (nullable = true)\n",
      " |-- orientation_accel_z: float (nullable = true)\n",
      " |-- orientation_magne_x: float (nullable = true)\n",
      " |-- orientation_magne_y: float (nullable = true)\n",
      " |-- orientation_magne_z: float (nullable = true)\n",
      " |-- switchports: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |    |-- link: boolean (nullable = true)\n",
      " |    |    |-- full_duplex: boolean (nullable = true)\n",
      " |    |    |-- mbps: integer (nullable = true)\n",
      " |    |    |-- rx_bytes: long (nullable = true)\n",
      " |    |    |-- rx_pkts: integer (nullable = true)\n",
      " |    |    |-- tx_bytes: long (nullable = true)\n",
      " |    |    |-- tx_pkts: integer (nullable = true)\n",
      " |    |    |-- rx_errors: integer (nullable = true)\n",
      " |    |    |-- rx_pause_pkts: integer (nullable = true)\n",
      " |    |    |-- rx_undersize_errors: integer (nullable = true)\n",
      " |    |    |-- rx_oversize_errors: integer (nullable = true)\n",
      " |    |    |-- rx_jabber_errors: integer (nullable = true)\n",
      " |    |    |-- rx_alignment_errors: integer (nullable = true)\n",
      " |    |    |-- rx_fcs_errors: integer (nullable = true)\n",
      " |    |    |-- rx_fragment_errors: integer (nullable = true)\n",
      " |    |    |-- rx_symbol_errors: integer (nullable = true)\n",
      " |    |    |-- rx_overrun_errors: integer (nullable = true)\n",
      " |    |    |-- rx_discards: integer (nullable = true)\n",
      " |    |    |-- rx_peakbps: long (nullable = true)\n",
      " |    |    |-- tx_peakbps: long (nullable = true)\n",
      " |-- delta: boolean (nullable = true)\n",
      " |-- deltaErrorMsg: string (nullable = true)\n",
      " |-- ap_id: string (nullable = true)\n",
      " |-- delta_interval: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths = ['s3a://mist-secor-production/ap-stats-analytics/ap-stats-analytics-production/dt=2019-05-07/hr=21']\n",
    "\n",
    "# ap_stats_df = spark.read.option(\"basePath\",\"s3a://mist-secor-production\").parquet(*paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'site_id',\n",
       " 'org_id',\n",
       " 'firmware_version',\n",
       " 'model',\n",
       " 'uptime',\n",
       " 'interval',\n",
       " 'config_time',\n",
       " 'when',\n",
       " 'total_client_count',\n",
       " 'active_client_count',\n",
       " 'terminator_remote_addr',\n",
       " 'terminator_timestamp',\n",
       " 'dnsServers',\n",
       " 'svis',\n",
       " 'inactive_wired_vlans',\n",
       " 'IPv4Routers',\n",
       " 'dhcpclients',\n",
       " 'l2tptunnels',\n",
       " 'lldp_nd',\n",
       " 'lldp_pwr',\n",
       " 'radios',\n",
       " 'wlans',\n",
       " 'pwr_src',\n",
       " 'pwr_budget',\n",
       " 'pwr_needed',\n",
       " 'pwr_available',\n",
       " 'memory_total_kb',\n",
       " 'memory_used_kb',\n",
       " 'cpu_total_time',\n",
       " 'cpu_user',\n",
       " 'cpu_system',\n",
       " 'fs_available_kb',\n",
       " 'fs_total_kb',\n",
       " 'load_total',\n",
       " 'slab_active_objs',\n",
       " 'slab_num_objs',\n",
       " 'slab_obj_size',\n",
       " 'cloud_last_rtt',\n",
       " 'cloud_pmtu',\n",
       " 'temperature_attitude',\n",
       " 'temperature_cpu',\n",
       " 'temperature_ambient',\n",
       " 'orientation_accel_x',\n",
       " 'orientation_accel_y',\n",
       " 'orientation_accel_z',\n",
       " 'orientation_magne_x',\n",
       " 'orientation_magne_y',\n",
       " 'orientation_magne_z',\n",
       " 'switchports',\n",
       " 'delta',\n",
       " 'deltaErrorMsg',\n",
       " 'ap_id',\n",
       " 'delta_interval']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(ap_stats_df.schema.names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-36-68e4d944b733>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-36-68e4d944b733>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    df1 = df.select(['site_id', 'id', 'when', $'switchports']) #.filter(lambda x: x['switchports'][0].get(\"link\") )\u001b[0m\n\u001b[0m                                              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "df1 = df.select(['site_id', 'id', 'when', 'switchports']) #.filter(lambda x: x['switchports'][0].get(\"link\") )\n",
    "df2 =df1.select('site_id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------------+----------------+--------------------+\n",
      "|             site_id|               id|            when|         switchports|\n",
      "+--------------------+-----------------+----------------+--------------------+\n",
      "|d635c2b9-3bfb-483...|5c-5b-35-3e-43-a8|1557343429748085|[[eth0, true, tru...|\n",
      "|8103dd7a-8242-498...|5c-5b-35-3e-7d-eb|1557343429601174|[[eth0, true, tru...|\n",
      "|b80482ff-5841-49a...|5c-5b-35-3f-69-68|1557343430071011|[[eth0, true, tru...|\n",
      "|128fae2e-61dc-4ee...|5c-5b-35-2e-26-e8|1557343429897322|[[eth0, true, tru...|\n",
      "|3dd377f2-4abc-40e...|5c-5b-35-0e-c3-a7|1557343429705259|[[eth0, true, tru...|\n",
      "+--------------------+-----------------+----------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'5c-5b-35-3e-43-a8',\n",
       "  u'd635c2b9-3bfb-4833-a3bd-cb7f3fdb76d9',\n",
       "  1557343429748085,\n",
       "  29226,\n",
       "  0,\n",
       "  0]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1_rdd = df1.rdd\n",
    "df1_rdd = df1_rdd.map(lambda x: [x.id,x.site_id, x.when, x.switchports[0].tx_bytes,\n",
    "                                 x.switchports[0].tx_peakbps, x.switchports[0].rx_errors ])\n",
    "df1_rdd.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 33.0 failed 1 times, most recent failure: Lost task 1.0 in stage 33.0 (TID 100, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/mistsys/spark_jobs/venv/spark-2.4.0-bin-without-hadoop/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/opt/mistsys/spark_jobs/venv/spark-2.4.0-bin-without-hadoop/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/mistsys/spark_jobs/venv/spark-2.4.0-bin-without-hadoop/python/lib/pyspark.zip/pyspark/serializers.py\", line 390, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/mistsys/spark_jobs/venv/spark-2.4.0-bin-without-hadoop/python/pyspark/rdd.py\", line 1354, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/opt/mistsys/spark_jobs/venv/spark-2.4.0-bin-without-hadoop/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-37-8e32399993e5>\", line 2, in <lambda>\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/mistsys/spark_jobs/venv/spark-2.4.0-bin-without-hadoop/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/opt/mistsys/spark_jobs/venv/spark-2.4.0-bin-without-hadoop/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/mistsys/spark_jobs/venv/spark-2.4.0-bin-without-hadoop/python/lib/pyspark.zip/pyspark/serializers.py\", line 390, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/mistsys/spark_jobs/venv/spark-2.4.0-bin-without-hadoop/python/pyspark/rdd.py\", line 1354, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/opt/mistsys/spark_jobs/venv/spark-2.4.0-bin-without-hadoop/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-37-8e32399993e5>\", line 2, in <lambda>\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-a966cb8f6753>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf1_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# for dat in df1_rdd.collect():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/mistsys/spark_jobs/venv/spark-2.4.0-bin-without-hadoop/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1360\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m             \u001b[0mitems\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/mistsys/spark_jobs/venv/spark-2.4.0-bin-without-hadoop/python/pyspark/context.pyc\u001b[0m in \u001b[0;36mrunJob\u001b[0;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         \u001b[0;31m# SparkContext#runJob.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m         \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/mistsys/spark_jobs/venv/spark-2.4.0-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/mistsys/spark_jobs/venv/spark-2.4.0-bin-without-hadoop/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/mistsys/spark_jobs/venv/spark-2.4.0-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 33.0 failed 1 times, most recent failure: Lost task 1.0 in stage 33.0 (TID 100, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/mistsys/spark_jobs/venv/spark-2.4.0-bin-without-hadoop/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/opt/mistsys/spark_jobs/venv/spark-2.4.0-bin-without-hadoop/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/mistsys/spark_jobs/venv/spark-2.4.0-bin-without-hadoop/python/lib/pyspark.zip/pyspark/serializers.py\", line 390, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/mistsys/spark_jobs/venv/spark-2.4.0-bin-without-hadoop/python/pyspark/rdd.py\", line 1354, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/opt/mistsys/spark_jobs/venv/spark-2.4.0-bin-without-hadoop/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-37-8e32399993e5>\", line 2, in <lambda>\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/mistsys/spark_jobs/venv/spark-2.4.0-bin-without-hadoop/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/opt/mistsys/spark_jobs/venv/spark-2.4.0-bin-without-hadoop/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/mistsys/spark_jobs/venv/spark-2.4.0-bin-without-hadoop/python/lib/pyspark.zip/pyspark/serializers.py\", line 390, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/mistsys/spark_jobs/venv/spark-2.4.0-bin-without-hadoop/python/pyspark/rdd.py\", line 1354, in takeUpToNumLeft\n    yield next(iterator)\n  File \"/opt/mistsys/spark_jobs/venv/spark-2.4.0-bin-without-hadoop/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-37-8e32399993e5>\", line 2, in <lambda>\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$3.apply(PythonRDD.scala:153)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "x, y = [], []\n",
    "dat = df1_rdd.take(100)\n",
    "# for dat in df1_rdd.collect():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 30.0 failed 1 times, most recent failure: Lost task 2.0 in stage 30.0 (TID 88, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/mistsys/spark_jobs/venv/spark-2.4.0-bin-without-hadoop/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/opt/mistsys/spark_jobs/venv/spark-2.4.0-bin-without-hadoop/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/mistsys/spark_jobs/venv/spark-2.4.0-bin-without-hadoop/python/lib/pyspark.zip/pyspark/serializers.py\", line 390, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/mistsys/spark_jobs/venv/spark-2.4.0-bin-without-hadoop/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-37-8e32399993e5>\", line 2, in <lambda>\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/mistsys/spark_jobs/venv/spark-2.4.0-bin-without-hadoop/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/opt/mistsys/spark_jobs/venv/spark-2.4.0-bin-without-hadoop/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/mistsys/spark_jobs/venv/spark-2.4.0-bin-without-hadoop/python/lib/pyspark.zip/pyspark/serializers.py\", line 390, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/mistsys/spark_jobs/venv/spark-2.4.0-bin-without-hadoop/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-37-8e32399993e5>\", line 2, in <lambda>\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-84c0f6bfdf13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/mistsys/spark_jobs/venv/spark-2.4.0-bin-without-hadoop/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    814\u001b[0m         \"\"\"\n\u001b[1;32m    815\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/mistsys/spark_jobs/venv/spark-2.4.0-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/mistsys/spark_jobs/venv/spark-2.4.0-bin-without-hadoop/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/mistsys/spark_jobs/venv/spark-2.4.0-bin-without-hadoop/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 30.0 failed 1 times, most recent failure: Lost task 2.0 in stage 30.0 (TID 88, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/mistsys/spark_jobs/venv/spark-2.4.0-bin-without-hadoop/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/opt/mistsys/spark_jobs/venv/spark-2.4.0-bin-without-hadoop/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/mistsys/spark_jobs/venv/spark-2.4.0-bin-without-hadoop/python/lib/pyspark.zip/pyspark/serializers.py\", line 390, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/mistsys/spark_jobs/venv/spark-2.4.0-bin-without-hadoop/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-37-8e32399993e5>\", line 2, in <lambda>\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:944)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:166)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/opt/mistsys/spark_jobs/venv/spark-2.4.0-bin-without-hadoop/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in main\n    process()\n  File \"/opt/mistsys/spark_jobs/venv/spark-2.4.0-bin-without-hadoop/python/lib/pyspark.zip/pyspark/worker.py\", line 367, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/opt/mistsys/spark_jobs/venv/spark-2.4.0-bin-without-hadoop/python/lib/pyspark.zip/pyspark/serializers.py\", line 390, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/opt/mistsys/spark_jobs/venv/spark-2.4.0-bin-without-hadoop/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-37-8e32399993e5>\", line 2, in <lambda>\nIndexError: list index out of range\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:452)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:588)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:571)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:406)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:945)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(arr):\n",
    "     \n",
    "    vals= arr[0]\n",
    "    \n",
    "    data_point = Row(\n",
    "                     tx_peakbps = vals['tx_peakbps'],\n",
    "                     rx_peakbps = vals['rx_peakbps'],\n",
    "                     rx_bytes = vals['rx_bytes'],\n",
    "                     tx_bytes = vals['tx_bytes'],\n",
    "                     rx_errors = vals['rx_errors']\n",
    "                    )\n",
    "    return data_point\n",
    "\n",
    "site_ap_stats = df1_rdd.map(lambda x: (x[0], stats(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(site_id=u'd635c2b9-3bfb-4833-a3bd-cb7f3fdb76d9', id=u'5c-5b-35-3e-43-a8', when=1557343429748085, switchports=[Row(name=u'eth0', link=True, full_duplex=True, mbps=1000, rx_bytes=731411, rx_pkts=3648, tx_bytes=29226, tx_pkts=79, rx_errors=0, rx_pause_pkts=0, rx_undersize_errors=0, rx_oversize_errors=0, rx_jabber_errors=0, rx_alignment_errors=0, rx_fcs_errors=0, rx_fragment_errors=0, rx_symbol_errors=0, rx_overrun_errors=207, rx_discards=100399, rx_peakbps=10896, tx_peakbps=0), Row(name=u'eth1', link=False, full_duplex=False, mbps=0, rx_bytes=0, rx_pkts=0, tx_bytes=0, tx_pkts=0, rx_errors=0, rx_pause_pkts=0, rx_undersize_errors=0, rx_oversize_errors=0, rx_jabber_errors=0, rx_alignment_errors=0, rx_fcs_errors=0, rx_fragment_errors=0, rx_symbol_errors=0, rx_overrun_errors=0, rx_discards=0, rx_peakbps=0, tx_peakbps=0), Row(name=u'module', link=True, full_duplex=True, mbps=1000, rx_bytes=9643, rx_pkts=41, tx_bytes=710886, tx_pkts=3403, rx_errors=0, rx_pause_pkts=0, rx_undersize_errors=0, rx_oversize_errors=0, rx_jabber_errors=0, rx_alignment_errors=0, rx_fcs_errors=0, rx_fragment_errors=0, rx_symbol_errors=0, rx_overrun_errors=0, rx_discards=3092, rx_peakbps=95112, tx_peakbps=0)])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interested fields (for now)\n",
    "\n",
    "site_id, id, svis, radios, active_client_count, total_client_count, cloud_last_rtt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distinct_sites = ap_stats_df.select('site_id').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of sites\n",
    "# distinct_sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ap_stats_df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying the paterns in device state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device_state = ap_stats_df.select(['site_id', 'id','when', 'fs_available_kb','fs_total_kb','cpu_system',\n",
    "                    'cpu_total_time','cpu_user','temperature_cpu',\n",
    "                    'temperature_attitude','temperature_ambient']).groupby(['site_id', 'id'])\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "\n",
    "@pandas_udf(\"double\", PandasUDFType.GROUPED_AGG)\n",
    "def myUDF(v):\n",
    "    return v.mean()\n",
    "    \n",
    "device_state.agg(myUDF(ap_stats_df['cpu_system'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def radio_stats(arr):\n",
    "    vals = {}\n",
    "    vals['bandwidth'] = np.mean(list(map(lambda x: x['bandwidth'], arr)))\n",
    "    vals['noise_floor'] = np.mean(list(map(lambda x: x['noise_floor'], arr)))\n",
    "    vals['tx_phy_err'] = np.mean(list(map(lambda x: x['tx_phy_err'], arr)))\n",
    "    vals['utilization_all'] = np.mean(list(map(lambda x: x['utilization_all'], arr)))\n",
    "\n",
    "    return vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max(arr, key):\n",
    "    max_b = float(np.max(list(map(lambda x: x[key], arr) )))   \n",
    "    return max_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(arr):\n",
    "    \n",
    "    vals = {}\n",
    "#     for item in ['temperature_cpu', 'load_total', 'total_client_count', 'cpu_system', 'cpu_total_time','radios']:\n",
    "    for item in ['radios']:\n",
    " \n",
    "#         if item == 'radios':\n",
    "        a = list(map(lambda x: radio_stats(x[item]),arr))\n",
    "\n",
    "        vals['radios_bandwidth'] = get_max(a, 'bandwidth')\n",
    "        vals['radios_noise_floor'] = get_max(a, 'noise_floor')\n",
    "        vals['radios_tx_phy_err'] = get_max(a, 'tx_phy_err')\n",
    "        vals['radios_utilization_all'] = get_max(a, 'utilization_all')\n",
    "\n",
    "#         else:    \n",
    "#             a = list(map(lambda x: x[item],arr))\n",
    "#             mean_a = float(np.mean(a))\n",
    "#             median_a = float(np.median(a))\n",
    "#             max_a = float(np.max(a))\n",
    "#             min_a = float(np.min(a))\n",
    "\n",
    "#         vals[item] =  max_a\n",
    "    \n",
    "    data_point = Row(\n",
    "#                      temperature_cpu= vals['temperature_cpu'], \n",
    "#                      load_total= vals['load_total'],\n",
    "#                      total_client_count= vals['total_client_count'],\n",
    "#                      cpu_system= vals['cpu_system'],\n",
    "#                      cpu_total_time = vals['cpu_total_time'],\n",
    "                     radios_bandwidth = vals['radios_bandwidth'],\n",
    "                     radios_noise_floor = vals['radios_noise_floor'],\n",
    "                     radios_tx_phy_err = vals['radios_tx_phy_err'],\n",
    "                     radios_utilization_all = vals['radios_utilization_all']\n",
    "                    )\n",
    "    return data_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def to_time(t):\n",
    "    '''\n",
    "    t is in microseconds\n",
    "    '''\n",
    "    return time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime(t/1000000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group data based on site_id, ap_mac. Drop all groups that have less than 10 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_state = ap_stats_df.select(['site_id', 'id','when','radios'])\n",
    "site_ap_group = device_state.rdd.filter(lambda x: x is not None).map(lambda x: ((x['site_id'], x['id']), x)).groupByKey()\n",
    "site_ap_group = site_ap_group.filter(lambda x: x[1] is not None and len(x[1]) > 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all distinct ap_macs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# site_ap_group_aps = site_ap_group.map(lambda x: x[0][1]).distinct().collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# site_ap_group_aps[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_ap = site_ap_group.filter(lambda x: x[0][1] == '5c-5b-35-2e-66-df')\n",
    "site_ap_stats = site_ap.map(lambda x: (x[0], stats(x[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_ap_stats_df = site_ap_stats.map(lambda x: (x[0][0], x[0][1],x[0][2],x[0][3],\n",
    "                                                x[1]['radios_bandwidth'],\n",
    "                                                x[1]['radios_noise_floor'],\n",
    "                                                x[1]['radios_tx_phy_err'],\n",
    "                                                x[1]['radios_utilization_all']\n",
    "                                               )\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(site_ap.take(1)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%spark -o site_ap_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%local\n",
    "import numpy as np\n",
    "import seaborn as sea\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "site_ap_stats_df_sorted =site_ap_stats_df.sort_values(by=['_3'])\n",
    "\n",
    "sea.distplot(site_ap_stats_df_sorted['_7'], hist = True, kde = True,kde_kws = {'linewidth': 3}, label=\"HHHH\")\n",
    "# site_ap_stats_df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_ap = ap_stats_df.where(ap_stats_df.id == '00-78-88-e6-36-b7').select('radios')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_error_ap = error_ap.rdd.map(lambda x: x['radios'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_ap.filter(lambda x: x[0][1] == '00-78-88-e6-36-b7').take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers for use in the reduce/lambda functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg(items, keys):\n",
    "    features = []\n",
    "    for k in keys:\n",
    "        features.append(Row(mean=float(np.mean(list(map(lambda x: x[k], items)))),key=k))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_error_ap = _error_ap.map(lambda x: get_avg(x, ['utilization_non_wifi','chanutil_intf'])).toDF()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%local\n",
    "import numpy as np\n",
    "import seaborn as sea\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "sea.distplot(_pune_avg_util_non_wifi, hist = True, kde = True,kde_kws = {'linewidth': 3}, label=\"HHHH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rdd =spark.WholeTextFiles(\"s3n://ap-logs-staging/ap/*/2018/12/03/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
