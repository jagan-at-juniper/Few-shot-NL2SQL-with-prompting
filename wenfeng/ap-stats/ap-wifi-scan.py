
from pyspark.sql import SparkSession
import pyspark.sql.functions as F
import json
from datetime import datetime,timedelta
import os

app_name = "ap-wifi-scan"
spark = SparkSession \
    .builder \
    .appName(app_name) \
    .getOrCreate()

import re
from uuid import UUID
def convert_bytes_to_mac(mac_bytes, sep=''):
    '''
    Convert protobuf Mac bytearray generated by GO to mac string
    >>> convert_bytes_to_mac(bytearray(b'\x88\xc6&\xb2B\xad'))
    '88c626b242ad'
    >>> convert_bytes_to_mac(bytearray(b'\x88\xc6&\xb2B\xad'), sep='-')
    '88-c6-26-b2-42-ad'
    :param mac_bytes:
    :return:
    '''
    if mac_bytes is None or len(mac_bytes) != 6:
        return ""

    return sep.join(["{:02x}".format(b) for b in mac_bytes])


def convert_bytes_to_uuid(uuid_bytearray):
    '''
    Convert protobuf UUID bytearray generated by GO to uuid string
    >>> convert_bytes_to_uuid(bytearray(b'\xcd\xa2\x8b\x7fw\xc1El\xae/\x12\xb7\x03\xd8,\x16'))
    'cda28b7f-77c1-456c-ae2f-12b703d82c16'
    >>> convert_bytes_to_uuid(bytearray(b'v\xd6\x9c<\x05\xaeO\x8a\xae\x16\x0c\x08\x98\xc2\xf2\xf0'))
    '76d69c3c-05ae-4f8a-ae16-0c0898c2f2f0'
    :param uuid_bytearray:
    :return:
    '''
    int_val = int.from_bytes(uuid_bytearray, byteorder='big')
    return str(UUID(int=int_val))


from pyspark.sql.types import *
bytes_to_uuid = F.udf(convert_bytes_to_uuid, StringType())
bytes_to_mac = F.udf(convert_bytes_to_mac, StringType())


def save_df_to_fs(df, date_day, date_hour, repo_name="aps-no-client-all"):
    s3_path = "{fs}://mist-data-science-dev/wenfeng/{repo_name}/dt={dt}/hr={hr}" \
        .format(fs=fs, repo_name= repo_name, dt=date_day.replace("[", "").replace("]", ""), hr=date_hour)

    df.coalesce(1).write.save(s3_path,format='parquet',   mode='overwrite', header=True)

env = "production"

env = "production"
provider = os.environ.get("CLOUD_PROVIDER", "aws")
# provider = "aws"
# provider = "gcp"
fs = "gs" if provider == "gcp" else "s3"

# provider = "aws"
# provider =  "gcp"
# fs = "gs" if provider == "gcp" else "s3"




detect_time = datetime.now() - timedelta(hours=2)
date_day = detect_time.strftime("%Y-%m-%d")
date_hour = detect_time.strftime("%H")
# date_hour = "*"
# date_day = "2022-03-2[12]"

s3_bucket = "{fs}://mist-secorapp-{env}/ap-wifi-scan-results/ap-wifi-scan-results-{env}/".format(fs=fs, env=env)
s3_bucket += "dt={date}/hr={hr}/*.parquet".format(date=date_day, hr=date_hour)
print(s3_bucket)

df = spark.read.parquet(s3_bucket)
df.printSchema()


# df = df.withColumn("org_id", bytes_to_uuid(F.col("copied.org_id")))


df = df.withColumn("org_id", bytes_to_uuid(F.col("copied.org_id")) ) \
    .withColumn("site_id", bytes_to_uuid(F.col("copied.site_id")) ) \
    .withColumn("id", bytes_to_mac(F.col("id")) )


# scan_channel_stats


df_scan = df.withColumn("date_time", F.from_unixtime(F.col("info_from_terminator.timestamp.seconds"),"yyyy-MM-dd HH:mm:ss")) \
    .select("org_id", "site_id", "id", "date_time",  F.explode("copied.radios").alias("radio"), "scan5GHz", "scan_channel_stats")



###

df_scan_channel_stats = df_scan.select("org_id", "site_id", "id", "date_time",
                                       F.col("radio.band").alias("radio_band"),
                                       F.col("radio.channel").alias("radio_channel"),
                                       F.col("radio.bandwidth").alias("radio_bandwidth"),
                                       F.col("radio.noise_floor").alias("radio_noise_floor"),
                                       F.col("radio.chan_util.mean.rx_other_bss").alias("radio_chan_util_mean_rx_other_bss"),
                                       F.col("radio.chan_util.mean.unknown_wifi").alias("radio_chan_util_mean_unknown_wifi"),
                                       F.col("radio.chan_util.mean.non_wifi").alias("radio_chan_util_mean_non_wifi"),
                                       F.col("radio.chan_util.mean.all").alias("radio_chan_util_mean_all"),
                                       F.col("radio.chan_util.mean.rx").alias("radio_chan_util_mean_tx"),
                                       F.col("radio.chan_util.mean.rx_in_bss").alias("radio_chan_util_mean_rx_in_bss"),
                                       F.explode("scan_channel_stats").alias("scan_channel_stats")) \
    .select("*", "scan_channel_stats.*") \
    .drop("scan_channel_stats") \
    .withColumnRenamed("congest.rx_in_bss", "congest.rx_in_bss") \
    .withColumnRenamed("congest.rx_other_bss", "congest_rx_other_bss") \
    .withColumnRenamed("congest.non_wifi", "congest_non_wifi") \
    .withColumnRenamed("scan_channel", F.col("channel.primary"))

df_scan_channel_stats.printSchema()
df_scan_channel_stats.show()


##

df_scan_bssids = df_scan.select("org_id", "site_id", "id", "date_time",
                                "radio.band", "radio.channel", "radio.bandwidth", "radio.utilization.*",
                                F.explode("scan5GHz").alias("scan5GHz")) \
    .select("org_id", "site_id", "id", "date_time", "band", "channel", "bandwidth",  "scan5GHz.*")



df_scan.printSchema()